schema {
  query: query_root
  mutation: mutation_root
  subscription: subscription_root
}

"""whether this query should be cached (Hasura Cloud only)"""
directive @cached(
  """refresh the cache entry"""
  refresh: Boolean! = false

  """measured in seconds"""
  ttl: Int! = 60
) on QUERY

type Aggregate {
  count: Int!
}

"""Asset system model"""
type Asset implements Entity & Node {
  """The time the document was created"""
  createdAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [Asset!]!

  """The file name"""
  fileName: String!

  """The file handle"""
  handle: String!

  """The height of the file"""
  height: Float
  heroImageEvent(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `heroImageEvent` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `heroImageEvent` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: EventOrderByInput
    skip: Int
    where: EventWhereInput
  ): [Event!]!
  heroImageOrganizer(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `heroImageOrganizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `heroImageOrganizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: OrganizerOrderByInput
    skip: Int
    where: OrganizerWhereInput
  ): [Organizer!]!

  """List of Asset versions"""
  history(
    limit: Int! = 10
    skip: Int! = 0

    """
    This is optional and can be used to fetch the document version history for a specific stage instead of the current one
    """
    stageOverride: Stage
  ): [Version!]!

  """The unique identifier"""
  id: ID!
  imageOrganizer(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `imageOrganizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `imageOrganizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: OrganizerOrderByInput
    skip: Int
    where: OrganizerWhereInput
  ): [Organizer!]!

  """System Locale field"""
  locale: Locale!

  """Get the other localizations for this document"""
  localizations(
    """Decides if the current locale should be included or not"""
    includeCurrent: Boolean! = false

    """
    Potential locales that should be returned. 
    
    The order of locales will also override locale fall-backing behaviour in the query's subtree.
    
    Note any related model with localized fields in the query's subtree will be affected.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    
    Consider using this in conjunction with forceParentLocale on the children relation fields.
    """
    locales: [Locale!]! = [en, fr]
  ): [Asset!]!

  """The mime type of the file"""
  mimeType: String
  nftImageEventPass(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `nftImageEventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `nftImageEventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: EventPassOrderByInput
    skip: Int
    where: EventPassWhereInput
  ): [EventPass!]!
  nftImageEventPassDelayedRevealed(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `nftImageEventPassDelayedRevealed` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `nftImageEventPassDelayedRevealed` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: EventPassDelayedRevealedOrderByInput
    skip: Int
    where: EventPassDelayedRevealedWhereInput
  ): [EventPassDelayedRevealed!]!
  nftImagePack(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `nftImagePack` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `nftImagePack` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: PackOrderByInput
    skip: Int
    where: PackWhereInput
  ): [Pack!]!

  """The time the document was published. Null on documents in draft stage."""
  publishedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  scheduledIn(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """The file size"""
  size: Float

  """System stage field"""
  stage: Stage!

  """The time the document was updated"""
  updatedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """Get the url for the asset with provided transformations applied."""
  url(transformation: AssetTransformationInput): String!

  """The file width"""
  width: Float
}

"""A connection to a list of items."""
type AssetConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [AssetEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

input AssetCreateInput {
  createdAt: DateTime
  fileName: String!
  handle: String!
  height: Float
  heroImageEvent: EventCreateManyInlineInput
  heroImageOrganizer: OrganizerCreateManyInlineInput
  imageOrganizer: OrganizerCreateManyInlineInput

  """
  Inline mutations for managing document localizations excluding the default locale
  """
  localizations: AssetCreateLocalizationsInput
  mimeType: String
  nftImageEventPass: EventPassCreateManyInlineInput
  nftImageEventPassDelayedRevealed: EventPassDelayedRevealedCreateManyInlineInput
  nftImagePack: PackCreateManyInlineInput
  size: Float
  updatedAt: DateTime
  width: Float
}

input AssetCreateLocalizationDataInput {
  createdAt: DateTime
  fileName: String!
  handle: String!
  height: Float
  mimeType: String
  size: Float
  updatedAt: DateTime
  width: Float
}

input AssetCreateLocalizationInput {
  """Localization input"""
  data: AssetCreateLocalizationDataInput!
  locale: Locale!
}

input AssetCreateLocalizationsInput {
  """Create localizations for the newly-created document"""
  create: [AssetCreateLocalizationInput!]
}

input AssetCreateOneInlineInput {
  """Connect one existing Asset document"""
  connect: AssetWhereUniqueInput

  """Create and connect one Asset document"""
  create: AssetCreateInput
}

"""An edge in a connection."""
type AssetEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: Asset!
}

"""Identifies documents"""
input AssetManyWhereInput {
  """Logical AND on all given filters."""
  AND: [AssetWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [AssetWhereInput!]

  """Logical OR on all given filters."""
  OR: [AssetWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  documentInStages_every: AssetWhereStageInput
  documentInStages_none: AssetWhereStageInput
  documentInStages_some: AssetWhereStageInput
  heroImageEvent_every: EventWhereInput
  heroImageEvent_none: EventWhereInput
  heroImageEvent_some: EventWhereInput
  heroImageOrganizer_every: OrganizerWhereInput
  heroImageOrganizer_none: OrganizerWhereInput
  heroImageOrganizer_some: OrganizerWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  imageOrganizer_every: OrganizerWhereInput
  imageOrganizer_none: OrganizerWhereInput
  imageOrganizer_some: OrganizerWhereInput
  nftImageEventPassDelayedRevealed_every: EventPassDelayedRevealedWhereInput
  nftImageEventPassDelayedRevealed_none: EventPassDelayedRevealedWhereInput
  nftImageEventPassDelayedRevealed_some: EventPassDelayedRevealedWhereInput
  nftImageEventPass_every: EventPassWhereInput
  nftImageEventPass_none: EventPassWhereInput
  nftImageEventPass_some: EventPassWhereInput
  nftImagePack_every: PackWhereInput
  nftImagePack_none: PackWhereInput
  nftImagePack_some: PackWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

enum AssetOrderByInput {
  createdAt_ASC
  createdAt_DESC
  fileName_ASC
  fileName_DESC
  handle_ASC
  handle_DESC
  height_ASC
  height_DESC
  id_ASC
  id_DESC
  mimeType_ASC
  mimeType_DESC
  publishedAt_ASC
  publishedAt_DESC
  size_ASC
  size_DESC
  updatedAt_ASC
  updatedAt_DESC
  width_ASC
  width_DESC
}

"""Transformations for Assets"""
input AssetTransformationInput {
  document: DocumentTransformationInput
  image: ImageTransformationInput

  """Pass true if you want to validate the passed transformation parameters"""
  validateOptions: Boolean = false
}

input AssetUpdateInput {
  fileName: String
  handle: String
  height: Float
  heroImageEvent: EventUpdateManyInlineInput
  heroImageOrganizer: OrganizerUpdateManyInlineInput
  imageOrganizer: OrganizerUpdateManyInlineInput

  """Manage document localizations"""
  localizations: AssetUpdateLocalizationsInput
  mimeType: String
  nftImageEventPass: EventPassUpdateManyInlineInput
  nftImageEventPassDelayedRevealed: EventPassDelayedRevealedUpdateManyInlineInput
  nftImagePack: PackUpdateManyInlineInput
  size: Float
  width: Float
}

input AssetUpdateLocalizationDataInput {
  fileName: String
  handle: String
  height: Float
  mimeType: String
  size: Float
  width: Float
}

input AssetUpdateLocalizationInput {
  data: AssetUpdateLocalizationDataInput!
  locale: Locale!
}

input AssetUpdateLocalizationsInput {
  """Localizations to create"""
  create: [AssetCreateLocalizationInput!]

  """Localizations to delete"""
  delete: [Locale!]

  """Localizations to update"""
  update: [AssetUpdateLocalizationInput!]
  upsert: [AssetUpsertLocalizationInput!]
}

input AssetUpdateManyInput {
  fileName: String
  height: Float

  """Optional updates to localizations"""
  localizations: AssetUpdateManyLocalizationsInput
  mimeType: String
  size: Float
  width: Float
}

input AssetUpdateManyLocalizationDataInput {
  fileName: String
  height: Float
  mimeType: String
  size: Float
  width: Float
}

input AssetUpdateManyLocalizationInput {
  data: AssetUpdateManyLocalizationDataInput!
  locale: Locale!
}

input AssetUpdateManyLocalizationsInput {
  """Localizations to update"""
  update: [AssetUpdateManyLocalizationInput!]
}

input AssetUpdateOneInlineInput {
  """Connect existing Asset document"""
  connect: AssetWhereUniqueInput

  """Create and connect one Asset document"""
  create: AssetCreateInput

  """Delete currently connected Asset document"""
  delete: Boolean

  """Disconnect currently connected Asset document"""
  disconnect: Boolean

  """Update single Asset document"""
  update: AssetUpdateWithNestedWhereUniqueInput

  """Upsert single Asset document"""
  upsert: AssetUpsertWithNestedWhereUniqueInput
}

input AssetUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: AssetUpdateInput!

  """Unique document search"""
  where: AssetWhereUniqueInput!
}

input AssetUpsertInput {
  """Create document if it didn't exist"""
  create: AssetCreateInput!

  """Update document if it exists"""
  update: AssetUpdateInput!
}

input AssetUpsertLocalizationInput {
  create: AssetCreateLocalizationDataInput!
  locale: Locale!
  update: AssetUpdateLocalizationDataInput!
}

input AssetUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: AssetUpsertInput!

  """Unique document search"""
  where: AssetWhereUniqueInput!
}

"""
This contains a set of filters that can be used to compare values internally
"""
input AssetWhereComparatorInput {
  """
  This field can be used to request to check if the entry is outdated by internal comparison
  """
  outdated_to: Boolean
}

"""Identifies documents"""
input AssetWhereInput {
  """Logical AND on all given filters."""
  AND: [AssetWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [AssetWhereInput!]

  """Logical OR on all given filters."""
  OR: [AssetWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  documentInStages_every: AssetWhereStageInput
  documentInStages_none: AssetWhereStageInput
  documentInStages_some: AssetWhereStageInput
  fileName: String

  """All values containing the given string."""
  fileName_contains: String

  """All values ending with the given string."""
  fileName_ends_with: String

  """All values that are contained in given list."""
  fileName_in: [String]

  """Any other value that exists and is not equal to the given value."""
  fileName_not: String

  """All values not containing the given string."""
  fileName_not_contains: String

  """All values not ending with the given string"""
  fileName_not_ends_with: String

  """All values that are not contained in given list."""
  fileName_not_in: [String]

  """All values not starting with the given string."""
  fileName_not_starts_with: String

  """All values starting with the given string."""
  fileName_starts_with: String
  handle: String

  """All values containing the given string."""
  handle_contains: String

  """All values ending with the given string."""
  handle_ends_with: String

  """All values that are contained in given list."""
  handle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  handle_not: String

  """All values not containing the given string."""
  handle_not_contains: String

  """All values not ending with the given string"""
  handle_not_ends_with: String

  """All values that are not contained in given list."""
  handle_not_in: [String]

  """All values not starting with the given string."""
  handle_not_starts_with: String

  """All values starting with the given string."""
  handle_starts_with: String
  height: Float

  """All values greater than the given value."""
  height_gt: Float

  """All values greater than or equal the given value."""
  height_gte: Float

  """All values that are contained in given list."""
  height_in: [Float]

  """All values less than the given value."""
  height_lt: Float

  """All values less than or equal the given value."""
  height_lte: Float

  """Any other value that exists and is not equal to the given value."""
  height_not: Float

  """All values that are not contained in given list."""
  height_not_in: [Float]
  heroImageEvent_every: EventWhereInput
  heroImageEvent_none: EventWhereInput
  heroImageEvent_some: EventWhereInput
  heroImageOrganizer_every: OrganizerWhereInput
  heroImageOrganizer_none: OrganizerWhereInput
  heroImageOrganizer_some: OrganizerWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  imageOrganizer_every: OrganizerWhereInput
  imageOrganizer_none: OrganizerWhereInput
  imageOrganizer_some: OrganizerWhereInput
  mimeType: String

  """All values containing the given string."""
  mimeType_contains: String

  """All values ending with the given string."""
  mimeType_ends_with: String

  """All values that are contained in given list."""
  mimeType_in: [String]

  """Any other value that exists and is not equal to the given value."""
  mimeType_not: String

  """All values not containing the given string."""
  mimeType_not_contains: String

  """All values not ending with the given string"""
  mimeType_not_ends_with: String

  """All values that are not contained in given list."""
  mimeType_not_in: [String]

  """All values not starting with the given string."""
  mimeType_not_starts_with: String

  """All values starting with the given string."""
  mimeType_starts_with: String
  nftImageEventPassDelayedRevealed_every: EventPassDelayedRevealedWhereInput
  nftImageEventPassDelayedRevealed_none: EventPassDelayedRevealedWhereInput
  nftImageEventPassDelayedRevealed_some: EventPassDelayedRevealedWhereInput
  nftImageEventPass_every: EventPassWhereInput
  nftImageEventPass_none: EventPassWhereInput
  nftImageEventPass_some: EventPassWhereInput
  nftImagePack_every: PackWhereInput
  nftImagePack_none: PackWhereInput
  nftImagePack_some: PackWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  size: Float

  """All values greater than the given value."""
  size_gt: Float

  """All values greater than or equal the given value."""
  size_gte: Float

  """All values that are contained in given list."""
  size_in: [Float]

  """All values less than the given value."""
  size_lt: Float

  """All values less than or equal the given value."""
  size_lte: Float

  """Any other value that exists and is not equal to the given value."""
  size_not: Float

  """All values that are not contained in given list."""
  size_not_in: [Float]
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
  width: Float

  """All values greater than the given value."""
  width_gt: Float

  """All values greater than or equal the given value."""
  width_gte: Float

  """All values that are contained in given list."""
  width_in: [Float]

  """All values less than the given value."""
  width_lt: Float

  """All values less than or equal the given value."""
  width_lte: Float

  """Any other value that exists and is not equal to the given value."""
  width_not: Float

  """All values that are not contained in given list."""
  width_not_in: [Float]
}

"""
The document in stages filter allows specifying a stage entry to cross compare the same document between different stages
"""
input AssetWhereStageInput {
  """Logical AND on all given filters."""
  AND: [AssetWhereStageInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [AssetWhereStageInput!]

  """Logical OR on all given filters."""
  OR: [AssetWhereStageInput!]

  """
  This field contains fields which can be set as true or false to specify an internal comparison
  """
  compareWithParent: AssetWhereComparatorInput

  """Specify the stage to compare with"""
  stage: Stage
}

"""References Asset record uniquely"""
input AssetWhereUniqueInput {
  id: ID
}

type BatchPayload {
  """The number of nodes that have been affected by the Batch operation."""
  count: Long!
}

"""
Boolean expression to compare columns of type "Boolean". All fields are combined with logical 'AND'.
"""
input Boolean_comparison_exp {
  _eq: Boolean
  _gt: Boolean
  _gte: Boolean
  _in: [Boolean!]
  _is_null: Boolean
  _lt: Boolean
  _lte: Boolean
  _neq: Boolean
  _nin: [Boolean!]
}

input ConnectPositionInput {
  """Connect document after specified document"""
  after: ID

  """Connect document before specified document"""
  before: ID

  """Connect document at last position"""
  end: Boolean

  """Connect document at first position"""
  start: Boolean
}

"""
A date-time string at UTC, such as 2007-12-03T10:15:30Z, compliant with the date-timeformat outlined in section 5.6 of the RFC 3339 profile of the ISO 8601 standard for representationof dates and times using the Gregorian calendar.
"""
scalar DateTime

enum DocumentFileTypes {
  doc
  docx
  html
  jpg
  odp
  ods
  odt
  pdf
  png
  ppt
  pptx
  svg
  txt
  webp
  xls
  xlsx
}

input DocumentOutputInput {
  """
  Transforms a document into a desired file type.
  See this matrix for format support:
  
  PDF:	jpg, odp, ods, odt, png, svg, txt, and webp
  DOC:	docx, html, jpg, odt, pdf, png, svg, txt, and webp
  DOCX:	doc, html, jpg, odt, pdf, png, svg, txt, and webp
  ODT:	doc, docx, html, jpg, pdf, png, svg, txt, and webp
  XLS:	jpg, pdf, ods, png, svg, xlsx, and webp
  XLSX:	jpg, pdf, ods, png, svg, xls, and webp
  ODS:	jpg, pdf, png, xls, svg, xlsx, and webp
  PPT:	jpg, odp, pdf, png, svg, pptx, and webp
  PPTX:	jpg, odp, pdf, png, svg, ppt, and webp
  ODP:	jpg, pdf, png, ppt, svg, pptx, and webp
  BMP:	jpg, odp, ods, odt, pdf, png, svg, and webp
  GIF:	jpg, odp, ods, odt, pdf, png, svg, and webp
  JPG:	jpg, odp, ods, odt, pdf, png, svg, and webp
  PNG:	jpg, odp, ods, odt, pdf, png, svg, and webp
  WEBP:	jpg, odp, ods, odt, pdf, png, svg, and webp
  TIFF:	jpg, odp, ods, odt, pdf, png, svg, and webp
  AI:	    jpg, odp, ods, odt, pdf, png, svg, and webp
  PSD:	jpg, odp, ods, odt, pdf, png, svg, and webp
  SVG:	jpg, odp, ods, odt, pdf, png, and webp
  HTML:	jpg, odt, pdf, svg, txt, and webp
  TXT:	jpg, html, odt, pdf, svg, and webp
  """
  format: DocumentFileTypes
}

"""Transformations for Documents"""
input DocumentTransformationInput {
  """Changes the output for the file."""
  output: DocumentOutputInput
}

type DocumentVersion {
  createdAt: DateTime!
  data: Json
  id: ID!
  revision: Int!
  stage: Stage!
}

"""An object with an ID"""
interface Entity {
  """The id of the object."""
  id: ID!

  """The Stage of an object"""
  stage: Stage!
}

"""
This enumeration holds all typenames that implement the Entity interface. Components and models implement the Entity interface.
"""
enum EntityTypeName {
  """Asset system model"""
  Asset

  """Root event model"""
  Event

  """
  Model used to define the different locations and dates of an event. A festival or a tournament for instance could have several.
  """
  EventDateLocation

  """Define a pass for an event with different options"""
  EventPass

  """
  The EventPassDelayedReveal is a feature in our ticketing system that introduces a timed reveal of certain event pass details. It's designed for special events where additional information about the pass, such as its name, description, and image, is unveiled at a later stage, adding an element of anticipation and exclusivity for attendees. This feature is particularly useful for creating a unique and engaging experience for high-profile events.
  """
  EventPassDelayedRevealed

  """
  A model for location data (point on a map) + additional info such as street, venue etc.
  """
  LocationAddress

  """
  An organizer is an entity that launch events and handle the pass benefits.
  """
  Organizer

  "The 'Pack' model represents a collection of unique NFTs (eventPasses) bundled together. It serves as a loot system for users, offering them a chance to receive one or more NFTs related to specific events. Each pack contains details about its contents and the associated event, fostering a more engaging and rewarding experience for users.\n"
  Pack

  """
  Define the options of an 'Event Pass' on an 'Event Date Location'. You can define severals if the event have multiple locations.
  """
  PassOption

  """Scheduled Operation system model"""
  ScheduledOperation

  """Scheduled Release system model"""
  ScheduledRelease

  """User system model"""
  User
}

"""Allows to specify input to query models and components directly"""
input EntityWhereInput {
  """The ID of an object"""
  id: ID!
  locale: Locale
  stage: Stage!

  """The Type name of an object"""
  typename: EntityTypeName!
}

"""Root event model"""
type Event implements Entity & Node {
  """The time the document was created"""
  createdAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  description: EventDescriptionRichText!

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [Event!]!

  """
  Define the different locations and timeframe for your event.
  This is only for information purpose but it should match your 'Event Pass' locations and dates
  """
  eventDateLocations(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `eventDateLocations` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `eventDateLocations` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: EventDateLocationOrderByInput
    skip: Int
    where: EventDateLocationWhereInput
  ): [EventDateLocation!]!
  eventParameters: eventParameters
  eventPasses(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `eventPasses` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `eventPasses` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: EventPassOrderByInput
    skip: Int
    where: EventPassWhereInput
  ): [EventPass!]!

  """
  An hero image that will displayed on a rectangular format. The image need to be high quality in order to display well on every screen. Advised resolution is 1920 * 800 pixels
  """
  heroImage(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `heroImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `heroImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Asset!

  """
  Optional field used to style your hero image with classes. Every classes from tailwind are supported. This is typically useful to adapt your image with light and dark mode (for instance using filter contrast or invert, https://tailwindcss.com/docs/contrast)
  """
  heroImageClasses: String

  """List of Event versions"""
  history(
    limit: Int! = 10
    skip: Int! = 0

    """
    This is optional and can be used to fetch the document version history for a specific stage instead of the current one
    """
    stageOverride: Stage
  ): [Version!]!

  """The unique identifier"""
  id: ID!

  """System Locale field"""
  locale: Locale!

  """Get the other localizations for this document"""
  localizations(
    """Decides if the current locale should be included or not"""
    includeCurrent: Boolean! = false

    """
    Potential locales that should be returned. 
    
    The order of locales will also override locale fall-backing behaviour in the query's subtree.
    
    Note any related model with localized fields in the query's subtree will be affected.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    
    Consider using this in conjunction with forceParentLocale on the children relation fields.
    """
    locales: [Locale!]! = [en, fr]
  ): [Event!]!
  organizer(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Organizer

  """The time the document was published. Null on documents in draft stage."""
  publishedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  scheduledIn(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """Used in the URL"""
  slug: String!

  """System stage field"""
  stage: Stage!
  title: String!

  """The time the document was updated"""
  updatedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
}

input EventConnectInput {
  """
  Allow to specify document position in list of connected documents, will default to appending at end of list
  """
  position: ConnectPositionInput

  """Document to connect"""
  where: EventWhereUniqueInput!
}

"""A connection to a list of items."""
type EventConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [EventEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

input EventCreateInput {
  createdAt: DateTime

  """description input for default locale (en)"""
  description: RichTextAST!
  eventDateLocations: EventDateLocationCreateManyInlineInput
  eventPasses: EventPassCreateManyInlineInput
  heroImage: AssetCreateOneInlineInput!
  heroImageClasses: String

  """
  Inline mutations for managing document localizations excluding the default locale
  """
  localizations: EventCreateLocalizationsInput
  organizer: OrganizerCreateOneInlineInput
  slug: String!

  """title input for default locale (en)"""
  title: String!
  updatedAt: DateTime
}

input EventCreateLocalizationDataInput {
  createdAt: DateTime
  description: RichTextAST!
  title: String!
  updatedAt: DateTime
}

input EventCreateLocalizationInput {
  """Localization input"""
  data: EventCreateLocalizationDataInput!
  locale: Locale!
}

input EventCreateLocalizationsInput {
  """Create localizations for the newly-created document"""
  create: [EventCreateLocalizationInput!]
}

input EventCreateManyInlineInput {
  """Connect multiple existing Event documents"""
  connect: [EventWhereUniqueInput!]

  """Create and connect multiple existing Event documents"""
  create: [EventCreateInput!]
}

input EventCreateOneInlineInput {
  """Connect one existing Event document"""
  connect: EventWhereUniqueInput

  """Create and connect one Event document"""
  create: EventCreateInput
}

"""
Model used to define the different locations and dates of an event. A festival or a tournament for instance could have several.
"""
type EventDateLocation implements Entity {
  """The end date including time."""
  dateEnd: DateTime!

  """The start date including time."""
  dateStart: DateTime!

  """The unique identifier"""
  id: ID!

  """The location expressed in coordinates on a map and address"""
  locationAddress(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `locationAddress` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `locationAddress` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): LocationAddress!

  """System stage field"""
  stage: Stage!
}

input EventDateLocationCreateInput {
  dateEnd: DateTime!
  dateStart: DateTime!
  locationAddress: LocationAddressCreateOneInlineInput!
}

input EventDateLocationCreateManyInlineInput {
  """Create and connect multiple existing EventDateLocation documents"""
  create: [EventDateLocationCreateInput!]
}

input EventDateLocationCreateOneInlineInput {
  """Create and connect one EventDateLocation document"""
  create: EventDateLocationCreateInput
}

input EventDateLocationCreateWithPositionInput {
  """Document to create"""
  data: EventDateLocationCreateInput!

  """
  Position in the list of existing component instances, will default to appending at the end of list
  """
  position: ConnectPositionInput
}

enum EventDateLocationOrderByInput {
  dateEnd_ASC
  dateEnd_DESC
  dateStart_ASC
  dateStart_DESC
  id_ASC
  id_DESC
}

input EventDateLocationUpdateInput {
  dateEnd: DateTime
  dateStart: DateTime
  locationAddress: LocationAddressUpdateOneInlineInput
}

input EventDateLocationUpdateManyInlineInput {
  """Create and connect multiple EventDateLocation component instances"""
  create: [EventDateLocationCreateWithPositionInput!]

  """Delete multiple EventDateLocation documents"""
  delete: [EventDateLocationWhereUniqueInput!]

  """Update multiple EventDateLocation component instances"""
  update: [EventDateLocationUpdateWithNestedWhereUniqueAndPositionInput!]

  """Upsert multiple EventDateLocation component instances"""
  upsert: [EventDateLocationUpsertWithNestedWhereUniqueAndPositionInput!]
}

input EventDateLocationUpdateOneInlineInput {
  """Create and connect one EventDateLocation document"""
  create: EventDateLocationCreateInput

  """Delete currently connected EventDateLocation document"""
  delete: Boolean

  """Update single EventDateLocation document"""
  update: EventDateLocationUpdateWithNestedWhereUniqueInput

  """Upsert single EventDateLocation document"""
  upsert: EventDateLocationUpsertWithNestedWhereUniqueInput
}

input EventDateLocationUpdateWithNestedWhereUniqueAndPositionInput {
  """Document to update"""
  data: EventDateLocationUpdateInput

  """
  Position in the list of existing component instances, will default to appending at the end of list
  """
  position: ConnectPositionInput

  """Unique component instance search"""
  where: EventDateLocationWhereUniqueInput!
}

input EventDateLocationUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: EventDateLocationUpdateInput!

  """Unique document search"""
  where: EventDateLocationWhereUniqueInput!
}

input EventDateLocationUpsertInput {
  """Create document if it didn't exist"""
  create: EventDateLocationCreateInput!

  """Update document if it exists"""
  update: EventDateLocationUpdateInput!
}

input EventDateLocationUpsertWithNestedWhereUniqueAndPositionInput {
  """Document to upsert"""
  data: EventDateLocationUpsertInput

  """
  Position in the list of existing component instances, will default to appending at the end of list
  """
  position: ConnectPositionInput

  """Unique component instance search"""
  where: EventDateLocationWhereUniqueInput!
}

input EventDateLocationUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: EventDateLocationUpsertInput!

  """Unique document search"""
  where: EventDateLocationWhereUniqueInput!
}

"""Identifies documents"""
input EventDateLocationWhereInput {
  """Logical AND on all given filters."""
  AND: [EventDateLocationWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventDateLocationWhereInput!]

  """Logical OR on all given filters."""
  OR: [EventDateLocationWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  dateEnd: DateTime

  """All values greater than the given value."""
  dateEnd_gt: DateTime

  """All values greater than or equal the given value."""
  dateEnd_gte: DateTime

  """All values that are contained in given list."""
  dateEnd_in: [DateTime]

  """All values less than the given value."""
  dateEnd_lt: DateTime

  """All values less than or equal the given value."""
  dateEnd_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  dateEnd_not: DateTime

  """All values that are not contained in given list."""
  dateEnd_not_in: [DateTime]
  dateStart: DateTime

  """All values greater than the given value."""
  dateStart_gt: DateTime

  """All values greater than or equal the given value."""
  dateStart_gte: DateTime

  """All values that are contained in given list."""
  dateStart_in: [DateTime]

  """All values less than the given value."""
  dateStart_lt: DateTime

  """All values less than or equal the given value."""
  dateStart_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  dateStart_not: DateTime

  """All values that are not contained in given list."""
  dateStart_not_in: [DateTime]
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  locationAddress: LocationAddressWhereInput
}

"""References EventDateLocation record uniquely"""
input EventDateLocationWhereUniqueInput {
  id: ID
}

type EventDescriptionRichText {
  """Returns HTMl representation"""
  html: String!
  json: RichTextAST!

  """Returns Markdown representation"""
  markdown: String!
  raw: RichTextAST!
  references(after: String, before: String, first: Int, last: Int, skip: Int): [EventDescriptionRichTextEmbeddedTypes!]!

  """Returns plain-text contents of RichText"""
  text: String!
}

union EventDescriptionRichTextEmbeddedTypes = Asset

"""An edge in a connection."""
type EventEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: Event!
}

"""Identifies documents"""
input EventManyWhereInput {
  """Logical AND on all given filters."""
  AND: [EventWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventWhereInput!]

  """Logical OR on all given filters."""
  OR: [EventWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  documentInStages_every: EventWhereStageInput
  documentInStages_none: EventWhereStageInput
  documentInStages_some: EventWhereStageInput
  eventDateLocations_every: EventDateLocationWhereInput
  eventDateLocations_none: EventDateLocationWhereInput
  eventDateLocations_some: EventDateLocationWhereInput
  eventPasses_every: EventPassWhereInput
  eventPasses_none: EventPassWhereInput
  eventPasses_some: EventPassWhereInput
  heroImage: AssetWhereInput
  heroImageClasses: String

  """All values containing the given string."""
  heroImageClasses_contains: String

  """All values ending with the given string."""
  heroImageClasses_ends_with: String

  """All values that are contained in given list."""
  heroImageClasses_in: [String]

  """Any other value that exists and is not equal to the given value."""
  heroImageClasses_not: String

  """All values not containing the given string."""
  heroImageClasses_not_contains: String

  """All values not ending with the given string"""
  heroImageClasses_not_ends_with: String

  """All values that are not contained in given list."""
  heroImageClasses_not_in: [String]

  """All values not starting with the given string."""
  heroImageClasses_not_starts_with: String

  """All values starting with the given string."""
  heroImageClasses_starts_with: String
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  organizer: OrganizerWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  slug: String

  """All values containing the given string."""
  slug_contains: String

  """All values ending with the given string."""
  slug_ends_with: String

  """All values that are contained in given list."""
  slug_in: [String]

  """Any other value that exists and is not equal to the given value."""
  slug_not: String

  """All values not containing the given string."""
  slug_not_contains: String

  """All values not ending with the given string"""
  slug_not_ends_with: String

  """All values that are not contained in given list."""
  slug_not_in: [String]

  """All values not starting with the given string."""
  slug_not_starts_with: String

  """All values starting with the given string."""
  slug_starts_with: String
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

enum EventOrderByInput {
  createdAt_ASC
  createdAt_DESC
  heroImageClasses_ASC
  heroImageClasses_DESC
  id_ASC
  id_DESC
  publishedAt_ASC
  publishedAt_DESC
  slug_ASC
  slug_DESC
  title_ASC
  title_DESC
  updatedAt_ASC
  updatedAt_DESC
}

"""Define a pass for an event with different options"""
type EventPass implements Entity & Node {
  """The time the document was created"""
  createdAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """
  Description of the pass, like "Access to the event for 3 days"
  """
  description: String!

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [EventPass!]!
  event(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `event` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `event` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Event

  """
  This is a direct link from your `EventPass` to `EventPassDelayedReveal`, enabling access to additional, exclusive details that are revealed afterwards on the back-office.
  """
  eventPassDelayedRevealed(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `eventPassDelayedRevealed` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `eventPassDelayedRevealed` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): EventPassDelayedRevealed
  eventPassNftContract: eventPassNftContract

  """List of EventPass versions"""
  history(
    limit: Int! = 10
    skip: Int! = 0

    """
    This is optional and can be used to fetch the document version history for a specific stage instead of the current one
    """
    stageOverride: Stage
  ): [Version!]!

  """The unique identifier"""
  id: ID!

  """System Locale field"""
  locale: Locale!

  """Get the other localizations for this document"""
  localizations(
    """Decides if the current locale should be included or not"""
    includeCurrent: Boolean! = false

    """
    Potential locales that should be returned. 
    
    The order of locales will also override locale fall-backing behaviour in the query's subtree.
    
    Note any related model with localized fields in the query's subtree will be affected.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    
    Consider using this in conjunction with forceParentLocale on the children relation fields.
    """
    locales: [Locale!]! = [en, fr]
  ): [EventPass!]!

  """
  User-friendly name of the pass, like "VIP 3-Day Pass"
  """
  name: String!

  """
  Fixed description pertaining to the NFT. This content is static and non-localizable.
  """
  nftDescription: String!

  """
  Permanent image representing the NFT. Advised resolution is 800 x 800 pixels. Image content is non-changeable and cannot be localized.
  """
  nftImage(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `nftImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `nftImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Asset!

  """
  Permanent name associated with the NFT. Cannot be changed or localized.
  """
  nftName: String!
  pack(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `pack` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `pack` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Pack
  passAmount: passAmount

  """
  Define the different pass options. An option is defined for a specific location and timeframe
  """
  passOptions(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `passOptions` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `passOptions` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: PassOptionOrderByInput
    skip: Int
    where: PassOptionWhereInput
  ): [PassOption!]!
  passPricing: passPricing

  """The time the document was published. Null on documents in draft stage."""
  publishedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  scheduledIn(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """System stage field"""
  stage: Stage!

  """The time the document was updated"""
  updatedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
}

input EventPassConnectInput {
  """
  Allow to specify document position in list of connected documents, will default to appending at end of list
  """
  position: ConnectPositionInput

  """Document to connect"""
  where: EventPassWhereUniqueInput!
}

"""A connection to a list of items."""
type EventPassConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [EventPassEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

input EventPassCreateInput {
  clptwshsk4wx601usb3uggcu7: EventPassDelayedRevealedCreateManyInlineInput
  createdAt: DateTime

  """description input for default locale (en)"""
  description: String!
  event: EventCreateOneInlineInput
  eventPassDelayedRevealed: EventPassDelayedRevealedCreateOneInlineInput

  """
  Inline mutations for managing document localizations excluding the default locale
  """
  localizations: EventPassCreateLocalizationsInput

  """name input for default locale (en)"""
  name: String!
  nftDescription: String!
  nftImage: AssetCreateOneInlineInput!
  nftName: String!
  pack: PackCreateOneInlineInput
  passOptions: PassOptionCreateManyInlineInput
  updatedAt: DateTime
}

input EventPassCreateLocalizationDataInput {
  createdAt: DateTime
  description: String!
  name: String!
  updatedAt: DateTime
}

input EventPassCreateLocalizationInput {
  """Localization input"""
  data: EventPassCreateLocalizationDataInput!
  locale: Locale!
}

input EventPassCreateLocalizationsInput {
  """Create localizations for the newly-created document"""
  create: [EventPassCreateLocalizationInput!]
}

input EventPassCreateManyInlineInput {
  """Connect multiple existing EventPass documents"""
  connect: [EventPassWhereUniqueInput!]

  """Create and connect multiple existing EventPass documents"""
  create: [EventPassCreateInput!]
}

input EventPassCreateOneInlineInput {
  """Connect one existing EventPass document"""
  connect: EventPassWhereUniqueInput

  """Create and connect one EventPass document"""
  create: EventPassCreateInput
}

"""
The EventPassDelayedReveal is a feature in our ticketing system that introduces a timed reveal of certain event pass details. It's designed for special events where additional information about the pass, such as its name, description, and image, is unveiled at a later stage, adding an element of anticipation and exclusivity for attendees. This feature is particularly useful for creating a unique and engaging experience for high-profile events.
"""
type EventPassDelayedRevealed implements Entity & Node {
  """The time the document was created"""
  createdAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """A brief overview or summary of the event pass"""
  description: String!

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [EventPassDelayedRevealed!]!

  """
  Links directly to `EventPass`, providing initial, temporary details about the NFT until the full reveal occurs.
  """
  eventPass(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `eventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `eventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): EventPass

  """List of EventPassDelayedRevealed versions"""
  history(
    limit: Int! = 10
    skip: Int! = 0

    """
    This is optional and can be used to fetch the document version history for a specific stage instead of the current one
    """
    stageOverride: Stage
  ): [Version!]!

  """The unique identifier"""
  id: ID!

  """System Locale field"""
  locale: Locale!

  """Get the other localizations for this document"""
  localizations(
    """Decides if the current locale should be included or not"""
    includeCurrent: Boolean! = false

    """
    Potential locales that should be returned. 
    
    The order of locales will also override locale fall-backing behaviour in the query's subtree.
    
    Note any related model with localized fields in the query's subtree will be affected.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    
    Consider using this in conjunction with forceParentLocale on the children relation fields.
    """
    locales: [Locale!]! = [en, fr]
  ): [EventPassDelayedRevealed!]!

  """The official name of the event pass"""
  name: String!

  """
  Fixed description pertaining to the NFT. This content is static and non-localizable.
  """
  nftDescription: String!

  """
  Permanent image representing the NFT. Advised resolution is 800 x 800 pixels. Image content is non-changeable and cannot be localized.
  """
  nftImage(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `nftImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `nftImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Asset!

  """
  Permanent name associated with the NFT. Cannot be changed or localized.
  """
  nftName: String!

  """
  Define the different pass options. An option is defined for a specific location and timeframe
  """
  passOptions(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `passOptions` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `passOptions` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: PassOptionOrderByInput
    skip: Int
    where: PassOptionWhereInput
  ): [PassOption!]!

  """The time the document was published. Null on documents in draft stage."""
  publishedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  scheduledIn(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """System stage field"""
  stage: Stage!

  """The time the document was updated"""
  updatedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
}

input EventPassDelayedRevealedConnectInput {
  """
  Allow to specify document position in list of connected documents, will default to appending at end of list
  """
  position: ConnectPositionInput

  """Document to connect"""
  where: EventPassDelayedRevealedWhereUniqueInput!
}

"""A connection to a list of items."""
type EventPassDelayedRevealedConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [EventPassDelayedRevealedEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

input EventPassDelayedRevealedCreateInput {
  clptyt58r52j901t9gkjuht2t: EventPassCreateManyInlineInput
  createdAt: DateTime

  """description input for default locale (en)"""
  description: String!
  eventPass: EventPassCreateOneInlineInput

  """
  Inline mutations for managing document localizations excluding the default locale
  """
  localizations: EventPassDelayedRevealedCreateLocalizationsInput

  """name input for default locale (en)"""
  name: String!
  nftDescription: String!
  nftImage: AssetCreateOneInlineInput!
  nftName: String!
  passOptions: PassOptionCreateManyInlineInput
  updatedAt: DateTime
}

input EventPassDelayedRevealedCreateLocalizationDataInput {
  createdAt: DateTime
  description: String!
  name: String!
  updatedAt: DateTime
}

input EventPassDelayedRevealedCreateLocalizationInput {
  """Localization input"""
  data: EventPassDelayedRevealedCreateLocalizationDataInput!
  locale: Locale!
}

input EventPassDelayedRevealedCreateLocalizationsInput {
  """Create localizations for the newly-created document"""
  create: [EventPassDelayedRevealedCreateLocalizationInput!]
}

input EventPassDelayedRevealedCreateManyInlineInput {
  """Connect multiple existing EventPassDelayedRevealed documents"""
  connect: [EventPassDelayedRevealedWhereUniqueInput!]

  """
  Create and connect multiple existing EventPassDelayedRevealed documents
  """
  create: [EventPassDelayedRevealedCreateInput!]
}

input EventPassDelayedRevealedCreateOneInlineInput {
  """Connect one existing EventPassDelayedRevealed document"""
  connect: EventPassDelayedRevealedWhereUniqueInput

  """Create and connect one EventPassDelayedRevealed document"""
  create: EventPassDelayedRevealedCreateInput
}

"""An edge in a connection."""
type EventPassDelayedRevealedEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: EventPassDelayedRevealed!
}

"""Identifies documents"""
input EventPassDelayedRevealedManyWhereInput {
  """Logical AND on all given filters."""
  AND: [EventPassDelayedRevealedWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventPassDelayedRevealedWhereInput!]

  """Logical OR on all given filters."""
  OR: [EventPassDelayedRevealedWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  documentInStages_every: EventPassDelayedRevealedWhereStageInput
  documentInStages_none: EventPassDelayedRevealedWhereStageInput
  documentInStages_some: EventPassDelayedRevealedWhereStageInput
  eventPass: EventPassWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  nftDescription: String

  """All values containing the given string."""
  nftDescription_contains: String

  """All values ending with the given string."""
  nftDescription_ends_with: String

  """All values that are contained in given list."""
  nftDescription_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftDescription_not: String

  """All values not containing the given string."""
  nftDescription_not_contains: String

  """All values not ending with the given string"""
  nftDescription_not_ends_with: String

  """All values that are not contained in given list."""
  nftDescription_not_in: [String]

  """All values not starting with the given string."""
  nftDescription_not_starts_with: String

  """All values starting with the given string."""
  nftDescription_starts_with: String
  nftImage: AssetWhereInput
  nftName: String

  """All values containing the given string."""
  nftName_contains: String

  """All values ending with the given string."""
  nftName_ends_with: String

  """All values that are contained in given list."""
  nftName_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftName_not: String

  """All values not containing the given string."""
  nftName_not_contains: String

  """All values not ending with the given string"""
  nftName_not_ends_with: String

  """All values that are not contained in given list."""
  nftName_not_in: [String]

  """All values not starting with the given string."""
  nftName_not_starts_with: String

  """All values starting with the given string."""
  nftName_starts_with: String
  passOptions_every: PassOptionWhereInput
  passOptions_none: PassOptionWhereInput
  passOptions_some: PassOptionWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

enum EventPassDelayedRevealedOrderByInput {
  createdAt_ASC
  createdAt_DESC
  description_ASC
  description_DESC
  id_ASC
  id_DESC
  name_ASC
  name_DESC
  nftDescription_ASC
  nftDescription_DESC
  nftName_ASC
  nftName_DESC
  publishedAt_ASC
  publishedAt_DESC
  updatedAt_ASC
  updatedAt_DESC
}

input EventPassDelayedRevealedUpdateInput {
  clptyt58r52j901t9gkjuht2t: EventPassUpdateManyInlineInput

  """description input for default locale (en)"""
  description: String
  eventPass: EventPassUpdateOneInlineInput

  """Manage document localizations"""
  localizations: EventPassDelayedRevealedUpdateLocalizationsInput

  """name input for default locale (en)"""
  name: String
  nftDescription: String
  nftImage: AssetUpdateOneInlineInput
  nftName: String
  passOptions: PassOptionUpdateManyInlineInput
}

input EventPassDelayedRevealedUpdateLocalizationDataInput {
  description: String
  name: String
}

input EventPassDelayedRevealedUpdateLocalizationInput {
  data: EventPassDelayedRevealedUpdateLocalizationDataInput!
  locale: Locale!
}

input EventPassDelayedRevealedUpdateLocalizationsInput {
  """Localizations to create"""
  create: [EventPassDelayedRevealedCreateLocalizationInput!]

  """Localizations to delete"""
  delete: [Locale!]

  """Localizations to update"""
  update: [EventPassDelayedRevealedUpdateLocalizationInput!]
  upsert: [EventPassDelayedRevealedUpsertLocalizationInput!]
}

input EventPassDelayedRevealedUpdateManyInlineInput {
  """Connect multiple existing EventPassDelayedRevealed documents"""
  connect: [EventPassDelayedRevealedConnectInput!]

  """Create and connect multiple EventPassDelayedRevealed documents"""
  create: [EventPassDelayedRevealedCreateInput!]

  """Delete multiple EventPassDelayedRevealed documents"""
  delete: [EventPassDelayedRevealedWhereUniqueInput!]

  """Disconnect multiple EventPassDelayedRevealed documents"""
  disconnect: [EventPassDelayedRevealedWhereUniqueInput!]

  """
  Override currently-connected documents with multiple existing EventPassDelayedRevealed documents
  """
  set: [EventPassDelayedRevealedWhereUniqueInput!]

  """Update multiple EventPassDelayedRevealed documents"""
  update: [EventPassDelayedRevealedUpdateWithNestedWhereUniqueInput!]

  """Upsert multiple EventPassDelayedRevealed documents"""
  upsert: [EventPassDelayedRevealedUpsertWithNestedWhereUniqueInput!]
}

input EventPassDelayedRevealedUpdateManyInput {
  """description input for default locale (en)"""
  description: String

  """Optional updates to localizations"""
  localizations: EventPassDelayedRevealedUpdateManyLocalizationsInput

  """name input for default locale (en)"""
  name: String
  nftDescription: String
  nftName: String
}

input EventPassDelayedRevealedUpdateManyLocalizationDataInput {
  description: String
  name: String
}

input EventPassDelayedRevealedUpdateManyLocalizationInput {
  data: EventPassDelayedRevealedUpdateManyLocalizationDataInput!
  locale: Locale!
}

input EventPassDelayedRevealedUpdateManyLocalizationsInput {
  """Localizations to update"""
  update: [EventPassDelayedRevealedUpdateManyLocalizationInput!]
}

input EventPassDelayedRevealedUpdateOneInlineInput {
  """Connect existing EventPassDelayedRevealed document"""
  connect: EventPassDelayedRevealedWhereUniqueInput

  """Create and connect one EventPassDelayedRevealed document"""
  create: EventPassDelayedRevealedCreateInput

  """Delete currently connected EventPassDelayedRevealed document"""
  delete: Boolean

  """Disconnect currently connected EventPassDelayedRevealed document"""
  disconnect: Boolean

  """Update single EventPassDelayedRevealed document"""
  update: EventPassDelayedRevealedUpdateWithNestedWhereUniqueInput

  """Upsert single EventPassDelayedRevealed document"""
  upsert: EventPassDelayedRevealedUpsertWithNestedWhereUniqueInput
}

input EventPassDelayedRevealedUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: EventPassDelayedRevealedUpdateInput!

  """Unique document search"""
  where: EventPassDelayedRevealedWhereUniqueInput!
}

input EventPassDelayedRevealedUpsertInput {
  """Create document if it didn't exist"""
  create: EventPassDelayedRevealedCreateInput!

  """Update document if it exists"""
  update: EventPassDelayedRevealedUpdateInput!
}

input EventPassDelayedRevealedUpsertLocalizationInput {
  create: EventPassDelayedRevealedCreateLocalizationDataInput!
  locale: Locale!
  update: EventPassDelayedRevealedUpdateLocalizationDataInput!
}

input EventPassDelayedRevealedUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: EventPassDelayedRevealedUpsertInput!

  """Unique document search"""
  where: EventPassDelayedRevealedWhereUniqueInput!
}

"""
This contains a set of filters that can be used to compare values internally
"""
input EventPassDelayedRevealedWhereComparatorInput {
  """
  This field can be used to request to check if the entry is outdated by internal comparison
  """
  outdated_to: Boolean
}

"""Identifies documents"""
input EventPassDelayedRevealedWhereInput {
  """Logical AND on all given filters."""
  AND: [EventPassDelayedRevealedWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventPassDelayedRevealedWhereInput!]

  """Logical OR on all given filters."""
  OR: [EventPassDelayedRevealedWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  description: String

  """All values containing the given string."""
  description_contains: String

  """All values ending with the given string."""
  description_ends_with: String

  """All values that are contained in given list."""
  description_in: [String]

  """Any other value that exists and is not equal to the given value."""
  description_not: String

  """All values not containing the given string."""
  description_not_contains: String

  """All values not ending with the given string"""
  description_not_ends_with: String

  """All values that are not contained in given list."""
  description_not_in: [String]

  """All values not starting with the given string."""
  description_not_starts_with: String

  """All values starting with the given string."""
  description_starts_with: String
  documentInStages_every: EventPassDelayedRevealedWhereStageInput
  documentInStages_none: EventPassDelayedRevealedWhereStageInput
  documentInStages_some: EventPassDelayedRevealedWhereStageInput
  eventPass: EventPassWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  name: String

  """All values containing the given string."""
  name_contains: String

  """All values ending with the given string."""
  name_ends_with: String

  """All values that are contained in given list."""
  name_in: [String]

  """Any other value that exists and is not equal to the given value."""
  name_not: String

  """All values not containing the given string."""
  name_not_contains: String

  """All values not ending with the given string"""
  name_not_ends_with: String

  """All values that are not contained in given list."""
  name_not_in: [String]

  """All values not starting with the given string."""
  name_not_starts_with: String

  """All values starting with the given string."""
  name_starts_with: String
  nftDescription: String

  """All values containing the given string."""
  nftDescription_contains: String

  """All values ending with the given string."""
  nftDescription_ends_with: String

  """All values that are contained in given list."""
  nftDescription_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftDescription_not: String

  """All values not containing the given string."""
  nftDescription_not_contains: String

  """All values not ending with the given string"""
  nftDescription_not_ends_with: String

  """All values that are not contained in given list."""
  nftDescription_not_in: [String]

  """All values not starting with the given string."""
  nftDescription_not_starts_with: String

  """All values starting with the given string."""
  nftDescription_starts_with: String
  nftImage: AssetWhereInput
  nftName: String

  """All values containing the given string."""
  nftName_contains: String

  """All values ending with the given string."""
  nftName_ends_with: String

  """All values that are contained in given list."""
  nftName_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftName_not: String

  """All values not containing the given string."""
  nftName_not_contains: String

  """All values not ending with the given string"""
  nftName_not_ends_with: String

  """All values that are not contained in given list."""
  nftName_not_in: [String]

  """All values not starting with the given string."""
  nftName_not_starts_with: String

  """All values starting with the given string."""
  nftName_starts_with: String
  passOptions_every: PassOptionWhereInput
  passOptions_none: PassOptionWhereInput
  passOptions_some: PassOptionWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

"""
The document in stages filter allows specifying a stage entry to cross compare the same document between different stages
"""
input EventPassDelayedRevealedWhereStageInput {
  """Logical AND on all given filters."""
  AND: [EventPassDelayedRevealedWhereStageInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventPassDelayedRevealedWhereStageInput!]

  """Logical OR on all given filters."""
  OR: [EventPassDelayedRevealedWhereStageInput!]

  """
  This field contains fields which can be set as true or false to specify an internal comparison
  """
  compareWithParent: EventPassDelayedRevealedWhereComparatorInput

  """Specify the stage to compare with"""
  stage: Stage
}

"""References EventPassDelayedRevealed record uniquely"""
input EventPassDelayedRevealedWhereUniqueInput {
  id: ID
}

"""An edge in a connection."""
type EventPassEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: EventPass!
}

"""Identifies documents"""
input EventPassManyWhereInput {
  """Logical AND on all given filters."""
  AND: [EventPassWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventPassWhereInput!]

  """Logical OR on all given filters."""
  OR: [EventPassWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  documentInStages_every: EventPassWhereStageInput
  documentInStages_none: EventPassWhereStageInput
  documentInStages_some: EventPassWhereStageInput
  event: EventWhereInput
  eventPassDelayedRevealed: EventPassDelayedRevealedWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  nftDescription: String

  """All values containing the given string."""
  nftDescription_contains: String

  """All values ending with the given string."""
  nftDescription_ends_with: String

  """All values that are contained in given list."""
  nftDescription_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftDescription_not: String

  """All values not containing the given string."""
  nftDescription_not_contains: String

  """All values not ending with the given string"""
  nftDescription_not_ends_with: String

  """All values that are not contained in given list."""
  nftDescription_not_in: [String]

  """All values not starting with the given string."""
  nftDescription_not_starts_with: String

  """All values starting with the given string."""
  nftDescription_starts_with: String
  nftImage: AssetWhereInput
  nftName: String

  """All values containing the given string."""
  nftName_contains: String

  """All values ending with the given string."""
  nftName_ends_with: String

  """All values that are contained in given list."""
  nftName_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftName_not: String

  """All values not containing the given string."""
  nftName_not_contains: String

  """All values not ending with the given string"""
  nftName_not_ends_with: String

  """All values that are not contained in given list."""
  nftName_not_in: [String]

  """All values not starting with the given string."""
  nftName_not_starts_with: String

  """All values starting with the given string."""
  nftName_starts_with: String
  pack: PackWhereInput
  passOptions_every: PassOptionWhereInput
  passOptions_none: PassOptionWhereInput
  passOptions_some: PassOptionWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

enum EventPassOrderByInput {
  createdAt_ASC
  createdAt_DESC
  description_ASC
  description_DESC
  id_ASC
  id_DESC
  name_ASC
  name_DESC
  nftDescription_ASC
  nftDescription_DESC
  nftName_ASC
  nftName_DESC
  publishedAt_ASC
  publishedAt_DESC
  updatedAt_ASC
  updatedAt_DESC
}

input EventPassUpdateInput {
  clptwshsk4wx601usb3uggcu7: EventPassDelayedRevealedUpdateManyInlineInput

  """description input for default locale (en)"""
  description: String
  event: EventUpdateOneInlineInput
  eventPassDelayedRevealed: EventPassDelayedRevealedUpdateOneInlineInput

  """Manage document localizations"""
  localizations: EventPassUpdateLocalizationsInput

  """name input for default locale (en)"""
  name: String
  nftDescription: String
  nftImage: AssetUpdateOneInlineInput
  nftName: String
  pack: PackUpdateOneInlineInput
  passOptions: PassOptionUpdateManyInlineInput
}

input EventPassUpdateLocalizationDataInput {
  description: String
  name: String
}

input EventPassUpdateLocalizationInput {
  data: EventPassUpdateLocalizationDataInput!
  locale: Locale!
}

input EventPassUpdateLocalizationsInput {
  """Localizations to create"""
  create: [EventPassCreateLocalizationInput!]

  """Localizations to delete"""
  delete: [Locale!]

  """Localizations to update"""
  update: [EventPassUpdateLocalizationInput!]
  upsert: [EventPassUpsertLocalizationInput!]
}

input EventPassUpdateManyInlineInput {
  """Connect multiple existing EventPass documents"""
  connect: [EventPassConnectInput!]

  """Create and connect multiple EventPass documents"""
  create: [EventPassCreateInput!]

  """Delete multiple EventPass documents"""
  delete: [EventPassWhereUniqueInput!]

  """Disconnect multiple EventPass documents"""
  disconnect: [EventPassWhereUniqueInput!]

  """
  Override currently-connected documents with multiple existing EventPass documents
  """
  set: [EventPassWhereUniqueInput!]

  """Update multiple EventPass documents"""
  update: [EventPassUpdateWithNestedWhereUniqueInput!]

  """Upsert multiple EventPass documents"""
  upsert: [EventPassUpsertWithNestedWhereUniqueInput!]
}

input EventPassUpdateManyInput {
  """description input for default locale (en)"""
  description: String

  """Optional updates to localizations"""
  localizations: EventPassUpdateManyLocalizationsInput

  """name input for default locale (en)"""
  name: String
  nftDescription: String
  nftName: String
}

input EventPassUpdateManyLocalizationDataInput {
  description: String
  name: String
}

input EventPassUpdateManyLocalizationInput {
  data: EventPassUpdateManyLocalizationDataInput!
  locale: Locale!
}

input EventPassUpdateManyLocalizationsInput {
  """Localizations to update"""
  update: [EventPassUpdateManyLocalizationInput!]
}

input EventPassUpdateOneInlineInput {
  """Connect existing EventPass document"""
  connect: EventPassWhereUniqueInput

  """Create and connect one EventPass document"""
  create: EventPassCreateInput

  """Delete currently connected EventPass document"""
  delete: Boolean

  """Disconnect currently connected EventPass document"""
  disconnect: Boolean

  """Update single EventPass document"""
  update: EventPassUpdateWithNestedWhereUniqueInput

  """Upsert single EventPass document"""
  upsert: EventPassUpsertWithNestedWhereUniqueInput
}

input EventPassUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: EventPassUpdateInput!

  """Unique document search"""
  where: EventPassWhereUniqueInput!
}

input EventPassUpsertInput {
  """Create document if it didn't exist"""
  create: EventPassCreateInput!

  """Update document if it exists"""
  update: EventPassUpdateInput!
}

input EventPassUpsertLocalizationInput {
  create: EventPassCreateLocalizationDataInput!
  locale: Locale!
  update: EventPassUpdateLocalizationDataInput!
}

input EventPassUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: EventPassUpsertInput!

  """Unique document search"""
  where: EventPassWhereUniqueInput!
}

"""
This contains a set of filters that can be used to compare values internally
"""
input EventPassWhereComparatorInput {
  """
  This field can be used to request to check if the entry is outdated by internal comparison
  """
  outdated_to: Boolean
}

"""Identifies documents"""
input EventPassWhereInput {
  """Logical AND on all given filters."""
  AND: [EventPassWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventPassWhereInput!]

  """Logical OR on all given filters."""
  OR: [EventPassWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  description: String

  """All values containing the given string."""
  description_contains: String

  """All values ending with the given string."""
  description_ends_with: String

  """All values that are contained in given list."""
  description_in: [String]

  """Any other value that exists and is not equal to the given value."""
  description_not: String

  """All values not containing the given string."""
  description_not_contains: String

  """All values not ending with the given string"""
  description_not_ends_with: String

  """All values that are not contained in given list."""
  description_not_in: [String]

  """All values not starting with the given string."""
  description_not_starts_with: String

  """All values starting with the given string."""
  description_starts_with: String
  documentInStages_every: EventPassWhereStageInput
  documentInStages_none: EventPassWhereStageInput
  documentInStages_some: EventPassWhereStageInput
  event: EventWhereInput
  eventPassDelayedRevealed: EventPassDelayedRevealedWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  name: String

  """All values containing the given string."""
  name_contains: String

  """All values ending with the given string."""
  name_ends_with: String

  """All values that are contained in given list."""
  name_in: [String]

  """Any other value that exists and is not equal to the given value."""
  name_not: String

  """All values not containing the given string."""
  name_not_contains: String

  """All values not ending with the given string"""
  name_not_ends_with: String

  """All values that are not contained in given list."""
  name_not_in: [String]

  """All values not starting with the given string."""
  name_not_starts_with: String

  """All values starting with the given string."""
  name_starts_with: String
  nftDescription: String

  """All values containing the given string."""
  nftDescription_contains: String

  """All values ending with the given string."""
  nftDescription_ends_with: String

  """All values that are contained in given list."""
  nftDescription_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftDescription_not: String

  """All values not containing the given string."""
  nftDescription_not_contains: String

  """All values not ending with the given string"""
  nftDescription_not_ends_with: String

  """All values that are not contained in given list."""
  nftDescription_not_in: [String]

  """All values not starting with the given string."""
  nftDescription_not_starts_with: String

  """All values starting with the given string."""
  nftDescription_starts_with: String
  nftImage: AssetWhereInput
  nftName: String

  """All values containing the given string."""
  nftName_contains: String

  """All values ending with the given string."""
  nftName_ends_with: String

  """All values that are contained in given list."""
  nftName_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftName_not: String

  """All values not containing the given string."""
  nftName_not_contains: String

  """All values not ending with the given string"""
  nftName_not_ends_with: String

  """All values that are not contained in given list."""
  nftName_not_in: [String]

  """All values not starting with the given string."""
  nftName_not_starts_with: String

  """All values starting with the given string."""
  nftName_starts_with: String
  pack: PackWhereInput
  passOptions_every: PassOptionWhereInput
  passOptions_none: PassOptionWhereInput
  passOptions_some: PassOptionWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

"""
The document in stages filter allows specifying a stage entry to cross compare the same document between different stages
"""
input EventPassWhereStageInput {
  """Logical AND on all given filters."""
  AND: [EventPassWhereStageInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventPassWhereStageInput!]

  """Logical OR on all given filters."""
  OR: [EventPassWhereStageInput!]

  """
  This field contains fields which can be set as true or false to specify an internal comparison
  """
  compareWithParent: EventPassWhereComparatorInput

  """Specify the stage to compare with"""
  stage: Stage
}

"""References EventPass record uniquely"""
input EventPassWhereUniqueInput {
  id: ID
}

input EventUpdateInput {
  """description input for default locale (en)"""
  description: RichTextAST
  eventDateLocations: EventDateLocationUpdateManyInlineInput
  eventPasses: EventPassUpdateManyInlineInput
  heroImage: AssetUpdateOneInlineInput
  heroImageClasses: String

  """Manage document localizations"""
  localizations: EventUpdateLocalizationsInput
  organizer: OrganizerUpdateOneInlineInput
  slug: String

  """title input for default locale (en)"""
  title: String
}

input EventUpdateLocalizationDataInput {
  description: RichTextAST
  title: String
}

input EventUpdateLocalizationInput {
  data: EventUpdateLocalizationDataInput!
  locale: Locale!
}

input EventUpdateLocalizationsInput {
  """Localizations to create"""
  create: [EventCreateLocalizationInput!]

  """Localizations to delete"""
  delete: [Locale!]

  """Localizations to update"""
  update: [EventUpdateLocalizationInput!]
  upsert: [EventUpsertLocalizationInput!]
}

input EventUpdateManyInlineInput {
  """Connect multiple existing Event documents"""
  connect: [EventConnectInput!]

  """Create and connect multiple Event documents"""
  create: [EventCreateInput!]

  """Delete multiple Event documents"""
  delete: [EventWhereUniqueInput!]

  """Disconnect multiple Event documents"""
  disconnect: [EventWhereUniqueInput!]

  """
  Override currently-connected documents with multiple existing Event documents
  """
  set: [EventWhereUniqueInput!]

  """Update multiple Event documents"""
  update: [EventUpdateWithNestedWhereUniqueInput!]

  """Upsert multiple Event documents"""
  upsert: [EventUpsertWithNestedWhereUniqueInput!]
}

input EventUpdateManyInput {
  """description input for default locale (en)"""
  description: RichTextAST
  heroImageClasses: String

  """Optional updates to localizations"""
  localizations: EventUpdateManyLocalizationsInput

  """title input for default locale (en)"""
  title: String
}

input EventUpdateManyLocalizationDataInput {
  description: RichTextAST
  title: String
}

input EventUpdateManyLocalizationInput {
  data: EventUpdateManyLocalizationDataInput!
  locale: Locale!
}

input EventUpdateManyLocalizationsInput {
  """Localizations to update"""
  update: [EventUpdateManyLocalizationInput!]
}

input EventUpdateOneInlineInput {
  """Connect existing Event document"""
  connect: EventWhereUniqueInput

  """Create and connect one Event document"""
  create: EventCreateInput

  """Delete currently connected Event document"""
  delete: Boolean

  """Disconnect currently connected Event document"""
  disconnect: Boolean

  """Update single Event document"""
  update: EventUpdateWithNestedWhereUniqueInput

  """Upsert single Event document"""
  upsert: EventUpsertWithNestedWhereUniqueInput
}

input EventUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: EventUpdateInput!

  """Unique document search"""
  where: EventWhereUniqueInput!
}

input EventUpsertInput {
  """Create document if it didn't exist"""
  create: EventCreateInput!

  """Update document if it exists"""
  update: EventUpdateInput!
}

input EventUpsertLocalizationInput {
  create: EventCreateLocalizationDataInput!
  locale: Locale!
  update: EventUpdateLocalizationDataInput!
}

input EventUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: EventUpsertInput!

  """Unique document search"""
  where: EventWhereUniqueInput!
}

"""
This contains a set of filters that can be used to compare values internally
"""
input EventWhereComparatorInput {
  """
  This field can be used to request to check if the entry is outdated by internal comparison
  """
  outdated_to: Boolean
}

"""Identifies documents"""
input EventWhereInput {
  """Logical AND on all given filters."""
  AND: [EventWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventWhereInput!]

  """Logical OR on all given filters."""
  OR: [EventWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  documentInStages_every: EventWhereStageInput
  documentInStages_none: EventWhereStageInput
  documentInStages_some: EventWhereStageInput
  eventDateLocations_every: EventDateLocationWhereInput
  eventDateLocations_none: EventDateLocationWhereInput
  eventDateLocations_some: EventDateLocationWhereInput
  eventPasses_every: EventPassWhereInput
  eventPasses_none: EventPassWhereInput
  eventPasses_some: EventPassWhereInput
  heroImage: AssetWhereInput
  heroImageClasses: String

  """All values containing the given string."""
  heroImageClasses_contains: String

  """All values ending with the given string."""
  heroImageClasses_ends_with: String

  """All values that are contained in given list."""
  heroImageClasses_in: [String]

  """Any other value that exists and is not equal to the given value."""
  heroImageClasses_not: String

  """All values not containing the given string."""
  heroImageClasses_not_contains: String

  """All values not ending with the given string"""
  heroImageClasses_not_ends_with: String

  """All values that are not contained in given list."""
  heroImageClasses_not_in: [String]

  """All values not starting with the given string."""
  heroImageClasses_not_starts_with: String

  """All values starting with the given string."""
  heroImageClasses_starts_with: String
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  organizer: OrganizerWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  slug: String

  """All values containing the given string."""
  slug_contains: String

  """All values ending with the given string."""
  slug_ends_with: String

  """All values that are contained in given list."""
  slug_in: [String]

  """Any other value that exists and is not equal to the given value."""
  slug_not: String

  """All values not containing the given string."""
  slug_not_contains: String

  """All values not ending with the given string"""
  slug_not_ends_with: String

  """All values that are not contained in given list."""
  slug_not_in: [String]

  """All values not starting with the given string."""
  slug_not_starts_with: String

  """All values starting with the given string."""
  slug_starts_with: String
  title: String

  """All values containing the given string."""
  title_contains: String

  """All values ending with the given string."""
  title_ends_with: String

  """All values that are contained in given list."""
  title_in: [String]

  """Any other value that exists and is not equal to the given value."""
  title_not: String

  """All values not containing the given string."""
  title_not_contains: String

  """All values not ending with the given string"""
  title_not_ends_with: String

  """All values that are not contained in given list."""
  title_not_in: [String]

  """All values not starting with the given string."""
  title_not_starts_with: String

  """All values starting with the given string."""
  title_starts_with: String
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

"""
The document in stages filter allows specifying a stage entry to cross compare the same document between different stages
"""
input EventWhereStageInput {
  """Logical AND on all given filters."""
  AND: [EventWhereStageInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [EventWhereStageInput!]

  """Logical OR on all given filters."""
  OR: [EventWhereStageInput!]

  """
  This field contains fields which can be set as true or false to specify an internal comparison
  """
  compareWithParent: EventWhereComparatorInput

  """Specify the stage to compare with"""
  stage: Stage
}

"""References Event record uniquely"""
input EventWhereUniqueInput {
  id: ID
  slug: String
}

"""References Event record uniquely"""
input EventWhereUniqueInput_remote_rel_eventParametersevent {
  slug: String
}

"""References Event record uniquely"""
input EventWhereUniqueInput_remote_rel_eventPassNftevent {
  slug: String
}

enum ImageFit {
  """
  Resizes the image to fit within the specified parameters without distorting, cropping, or changing the aspect ratio.
  """
  clip

  """
  Resizes the image to fit the specified parameters exactly by removing any parts of the image that don't fit within the boundaries.
  """
  crop

  """
  Resizes the image to fit within the parameters, but as opposed to 'fit:clip' will not scale the image if the image is smaller than the output size.
  """
  max

  """
  Resizes the image to fit the specified parameters exactly by scaling the image to the desired size. The aspect ratio of the image is not respected and the image can be distorted using this method.
  """
  scale
}

input ImageResizeInput {
  """The default value for the fit parameter is fit:clip."""
  fit: ImageFit

  """
  The height in pixels to resize the image to. The value must be an integer from 1 to 10000.
  """
  height: Int

  """
  The width in pixels to resize the image to. The value must be an integer from 1 to 10000.
  """
  width: Int
}

"""Transformations for Images"""
input ImageTransformationInput {
  """Resizes the image"""
  resize: ImageResizeInput
}

"""
Boolean expression to compare columns of type "Int". All fields are combined with logical 'AND'.
"""
input Int_comparison_exp {
  _eq: Int
  _gt: Int
  _gte: Int
  _in: [Int!]
  _is_null: Boolean
  _lt: Int
  _lte: Int
  _neq: Int
  _nin: [Int!]
}

"""Raw JSON value"""
scalar Json

"""Locale system enumeration"""
enum Locale {
  """System locale"""
  en
  fr
}

"""Representing a geolocation point with latitude and longitude"""
type Location {
  distance(from: LocationInput!): Float!
  latitude: Float!
  longitude: Float!
}

"""
A model for location data (point on a map) + additional info such as street, venue etc.
"""
type LocationAddress implements Entity {
  """Name of the city"""
  city: String!

  """Point into the map where the event is happening"""
  coordinates: Location!

  """The name of the country"""
  country: String!

  """The unique identifier"""
  id: ID!

  """
  Place ID from google maps. Use this tool to retrieve the correct Place ID from the location you want to open on google maps while clicking on the address provided: https://developers.google.com/maps/documentation/places/web-service/place-id#find-id
  """
  placeId: String
  postalCode: String!

  """System stage field"""
  stage: Stage!

  """The name of the state if it exist"""
  state: String

  """Name of the street"""
  street: String

  """Name of the venue, useful if the address doesn't apply"""
  venue: String
}

input LocationAddressCreateInput {
  city: String!
  coordinates: LocationInput!
  country: String!
  placeId: String
  postalCode: String!
  state: String
  street: String
  venue: String
}

input LocationAddressCreateOneInlineInput {
  """Create and connect one LocationAddress document"""
  create: LocationAddressCreateInput
}

input LocationAddressUpdateInput {
  city: String
  coordinates: LocationInput
  country: String
  placeId: String
  postalCode: String
  state: String
  street: String
  venue: String
}

input LocationAddressUpdateOneInlineInput {
  """Create and connect one LocationAddress document"""
  create: LocationAddressCreateInput

  """Delete currently connected LocationAddress document"""
  delete: Boolean

  """Update single LocationAddress document"""
  update: LocationAddressUpdateWithNestedWhereUniqueInput

  """Upsert single LocationAddress document"""
  upsert: LocationAddressUpsertWithNestedWhereUniqueInput
}

input LocationAddressUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: LocationAddressUpdateInput!

  """Unique document search"""
  where: LocationAddressWhereUniqueInput!
}

input LocationAddressUpsertInput {
  """Create document if it didn't exist"""
  create: LocationAddressCreateInput!

  """Update document if it exists"""
  update: LocationAddressUpdateInput!
}

input LocationAddressUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: LocationAddressUpsertInput!

  """Unique document search"""
  where: LocationAddressWhereUniqueInput!
}

"""Identifies documents"""
input LocationAddressWhereInput {
  """Logical AND on all given filters."""
  AND: [LocationAddressWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [LocationAddressWhereInput!]

  """Logical OR on all given filters."""
  OR: [LocationAddressWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  city: String

  """All values containing the given string."""
  city_contains: String

  """All values ending with the given string."""
  city_ends_with: String

  """All values that are contained in given list."""
  city_in: [String]

  """Any other value that exists and is not equal to the given value."""
  city_not: String

  """All values not containing the given string."""
  city_not_contains: String

  """All values not ending with the given string"""
  city_not_ends_with: String

  """All values that are not contained in given list."""
  city_not_in: [String]

  """All values not starting with the given string."""
  city_not_starts_with: String

  """All values starting with the given string."""
  city_starts_with: String
  country: String

  """All values containing the given string."""
  country_contains: String

  """All values ending with the given string."""
  country_ends_with: String

  """All values that are contained in given list."""
  country_in: [String]

  """Any other value that exists and is not equal to the given value."""
  country_not: String

  """All values not containing the given string."""
  country_not_contains: String

  """All values not ending with the given string"""
  country_not_ends_with: String

  """All values that are not contained in given list."""
  country_not_in: [String]

  """All values not starting with the given string."""
  country_not_starts_with: String

  """All values starting with the given string."""
  country_starts_with: String
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  placeId: String

  """All values containing the given string."""
  placeId_contains: String

  """All values ending with the given string."""
  placeId_ends_with: String

  """All values that are contained in given list."""
  placeId_in: [String]

  """Any other value that exists and is not equal to the given value."""
  placeId_not: String

  """All values not containing the given string."""
  placeId_not_contains: String

  """All values not ending with the given string"""
  placeId_not_ends_with: String

  """All values that are not contained in given list."""
  placeId_not_in: [String]

  """All values not starting with the given string."""
  placeId_not_starts_with: String

  """All values starting with the given string."""
  placeId_starts_with: String
  postalCode: String

  """All values containing the given string."""
  postalCode_contains: String

  """All values ending with the given string."""
  postalCode_ends_with: String

  """All values that are contained in given list."""
  postalCode_in: [String]

  """Any other value that exists and is not equal to the given value."""
  postalCode_not: String

  """All values not containing the given string."""
  postalCode_not_contains: String

  """All values not ending with the given string"""
  postalCode_not_ends_with: String

  """All values that are not contained in given list."""
  postalCode_not_in: [String]

  """All values not starting with the given string."""
  postalCode_not_starts_with: String

  """All values starting with the given string."""
  postalCode_starts_with: String
  state: String

  """All values containing the given string."""
  state_contains: String

  """All values ending with the given string."""
  state_ends_with: String

  """All values that are contained in given list."""
  state_in: [String]

  """Any other value that exists and is not equal to the given value."""
  state_not: String

  """All values not containing the given string."""
  state_not_contains: String

  """All values not ending with the given string"""
  state_not_ends_with: String

  """All values that are not contained in given list."""
  state_not_in: [String]

  """All values not starting with the given string."""
  state_not_starts_with: String

  """All values starting with the given string."""
  state_starts_with: String
  street: String

  """All values containing the given string."""
  street_contains: String

  """All values ending with the given string."""
  street_ends_with: String

  """All values that are contained in given list."""
  street_in: [String]

  """Any other value that exists and is not equal to the given value."""
  street_not: String

  """All values not containing the given string."""
  street_not_contains: String

  """All values not ending with the given string"""
  street_not_ends_with: String

  """All values that are not contained in given list."""
  street_not_in: [String]

  """All values not starting with the given string."""
  street_not_starts_with: String

  """All values starting with the given string."""
  street_starts_with: String
  venue: String

  """All values containing the given string."""
  venue_contains: String

  """All values ending with the given string."""
  venue_ends_with: String

  """All values that are contained in given list."""
  venue_in: [String]

  """Any other value that exists and is not equal to the given value."""
  venue_not: String

  """All values not containing the given string."""
  venue_not_contains: String

  """All values not ending with the given string"""
  venue_not_ends_with: String

  """All values that are not contained in given list."""
  venue_not_in: [String]

  """All values not starting with the given string."""
  venue_not_starts_with: String

  """All values starting with the given string."""
  venue_starts_with: String
}

"""References LocationAddress record uniquely"""
input LocationAddressWhereUniqueInput {
  id: ID
}

"""Input for a geolocation point with latitude and longitude"""
input LocationInput {
  latitude: Float!
  longitude: Float!
}

"""
The Long scalar type represents non-fractional signed whole numeric values. Long can represent values between -(2^63) and 2^63 - 1.
"""
scalar Long

"""An object with an ID"""
interface Node {
  """The id of the object."""
  id: ID!

  """The Stage of an object"""
  stage: Stage!
}

"""
An organizer is an entity that launch events and handle the pass benefits.
"""
type Organizer implements Entity & Node {
  """The time the document was created"""
  createdAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  description: OrganizerDescriptionRichText

  """
  The discord widge id of the organizer. You need to enable the widget in your discord server and copy the value in `server id`. For details instruction of how to enable and find the id, refer to this section https://dev.fandom.com/wiki/DiscordIntegrator#Enabling_the_widget
  """
  discordWidgetId: String

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [Organizer!]!
  events(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `events` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `events` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: EventOrderByInput
    skip: Int
    where: EventWhereInput
  ): [Event!]!

  """
  The facebook handle (username) of the organizer. You can just copy the text on your facebook landing page on the URL, like 'johndoe' for 'https://www.facebook.com/johndoe'.
  """
  facebookHandle: String

  """
  An hero image that will displayed on a rectangular format. The image need to be high quality in order to display well on every screen. Advised resolution is 1920 * 800 pixels
  """
  heroImage(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `heroImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `heroImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Asset!

  """
  Optional field used to style your image with classes. Every classes from tailwind are supported. This is typically useful to adapt your image with light and dark mode (for instance using filter contrast or invert, https://tailwindcss.com/docs/contrast)
  """
  heroImageClasses: String

  """List of Organizer versions"""
  history(
    limit: Int! = 10
    skip: Int! = 0

    """
    This is optional and can be used to fetch the document version history for a specific stage instead of the current one
    """
    stageOverride: Stage
  ): [Version!]!

  """The unique identifier"""
  id: ID!

  """
  Image that represent the organizer, typically its logo. Advised resolution is 800 x 800 pixels, in square format with transparency (for ex: svg or png but not jpg) so that the image always look good either on light or dark mode.
  """
  image(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `image` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `image` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Asset!

  """
  Optional field used to style your image with classes. Every classes from tailwind are supported. This is typically useful to adapt your image with light and dark mode (for instance using filter contrast or invert, https://tailwindcss.com/docs/contrast)
  """
  imageClasses: String

  """
  The instagram handle (username) of the organizer. You can just copy the name on your instagram landing page next to the follow button.
  """
  instagramHandle: String

  """System Locale field"""
  locale: Locale!

  """Get the other localizations for this document"""
  localizations(
    """Decides if the current locale should be included or not"""
    includeCurrent: Boolean! = false

    """
    Potential locales that should be returned. 
    
    The order of locales will also override locale fall-backing behaviour in the query's subtree.
    
    Note any related model with localized fields in the query's subtree will be affected.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    
    Consider using this in conjunction with forceParentLocale on the children relation fields.
    """
    locales: [Locale!]! = [en, fr]
  ): [Organizer!]!

  """Name of the organizer"""
  name: String!

  """The time the document was published. Null on documents in draft stage."""
  publishedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  scheduledIn(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """Used in URL"""
  slug: String!

  """System stage field"""
  stage: Stage!

  """
  The telegram handle (username) of the organizer. You can just copy the text on your telegram profile page in parameters after the @, like 'johndoe' for '@johndoe'.
  """
  telegramHandle: String

  """
  The tiktok handle (username) of the organizer. You can just copy the name on your tiktok landing page.
  """
  tiktokHandle: String

  """
  The twitter (X) handle (username) of the organizer. You can just copy the text on your twitter landing page after the @, like 'johndoe' for '@johndoe'.
  """
  twitterHandle: String

  """The time the document was updated"""
  updatedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """
  The youtube handle (username) of the organizer. YYou can just copy the text on your youtube landing page after the @, like 'johndoe' for '@johndoe'.
  """
  youtubeHandle: String
}

input OrganizerConnectInput {
  """
  Allow to specify document position in list of connected documents, will default to appending at end of list
  """
  position: ConnectPositionInput

  """Document to connect"""
  where: OrganizerWhereUniqueInput!
}

"""A connection to a list of items."""
type OrganizerConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [OrganizerEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

input OrganizerCreateInput {
  clr7j9mmt0q2j01uo9zrs2fm7: PackCreateManyInlineInput
  createdAt: DateTime

  """description input for default locale (en)"""
  description: RichTextAST
  discordWidgetId: String
  events: EventCreateManyInlineInput
  facebookHandle: String
  heroImage: AssetCreateOneInlineInput!
  heroImageClasses: String
  image: AssetCreateOneInlineInput!
  imageClasses: String
  instagramHandle: String

  """
  Inline mutations for managing document localizations excluding the default locale
  """
  localizations: OrganizerCreateLocalizationsInput
  name: String!
  slug: String!
  telegramHandle: String
  tiktokHandle: String
  twitterHandle: String
  updatedAt: DateTime
  youtubeHandle: String
}

input OrganizerCreateLocalizationDataInput {
  createdAt: DateTime
  description: RichTextAST
  updatedAt: DateTime
}

input OrganizerCreateLocalizationInput {
  """Localization input"""
  data: OrganizerCreateLocalizationDataInput!
  locale: Locale!
}

input OrganizerCreateLocalizationsInput {
  """Create localizations for the newly-created document"""
  create: [OrganizerCreateLocalizationInput!]
}

input OrganizerCreateManyInlineInput {
  """Connect multiple existing Organizer documents"""
  connect: [OrganizerWhereUniqueInput!]

  """Create and connect multiple existing Organizer documents"""
  create: [OrganizerCreateInput!]
}

input OrganizerCreateOneInlineInput {
  """Connect one existing Organizer document"""
  connect: OrganizerWhereUniqueInput

  """Create and connect one Organizer document"""
  create: OrganizerCreateInput
}

type OrganizerDescriptionRichText {
  """Returns HTMl representation"""
  html: String!
  json: RichTextAST!

  """Returns Markdown representation"""
  markdown: String!
  raw: RichTextAST!
  references(after: String, before: String, first: Int, last: Int, skip: Int): [OrganizerDescriptionRichTextEmbeddedTypes!]!

  """Returns plain-text contents of RichText"""
  text: String!
}

union OrganizerDescriptionRichTextEmbeddedTypes = Asset

"""An edge in a connection."""
type OrganizerEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: Organizer!
}

"""Identifies documents"""
input OrganizerManyWhereInput {
  """Logical AND on all given filters."""
  AND: [OrganizerWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [OrganizerWhereInput!]

  """Logical OR on all given filters."""
  OR: [OrganizerWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  discordWidgetId: String

  """All values containing the given string."""
  discordWidgetId_contains: String

  """All values ending with the given string."""
  discordWidgetId_ends_with: String

  """All values that are contained in given list."""
  discordWidgetId_in: [String]

  """Any other value that exists and is not equal to the given value."""
  discordWidgetId_not: String

  """All values not containing the given string."""
  discordWidgetId_not_contains: String

  """All values not ending with the given string"""
  discordWidgetId_not_ends_with: String

  """All values that are not contained in given list."""
  discordWidgetId_not_in: [String]

  """All values not starting with the given string."""
  discordWidgetId_not_starts_with: String

  """All values starting with the given string."""
  discordWidgetId_starts_with: String
  documentInStages_every: OrganizerWhereStageInput
  documentInStages_none: OrganizerWhereStageInput
  documentInStages_some: OrganizerWhereStageInput
  events_every: EventWhereInput
  events_none: EventWhereInput
  events_some: EventWhereInput
  facebookHandle: String

  """All values containing the given string."""
  facebookHandle_contains: String

  """All values ending with the given string."""
  facebookHandle_ends_with: String

  """All values that are contained in given list."""
  facebookHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  facebookHandle_not: String

  """All values not containing the given string."""
  facebookHandle_not_contains: String

  """All values not ending with the given string"""
  facebookHandle_not_ends_with: String

  """All values that are not contained in given list."""
  facebookHandle_not_in: [String]

  """All values not starting with the given string."""
  facebookHandle_not_starts_with: String

  """All values starting with the given string."""
  facebookHandle_starts_with: String
  heroImage: AssetWhereInput
  heroImageClasses: String

  """All values containing the given string."""
  heroImageClasses_contains: String

  """All values ending with the given string."""
  heroImageClasses_ends_with: String

  """All values that are contained in given list."""
  heroImageClasses_in: [String]

  """Any other value that exists and is not equal to the given value."""
  heroImageClasses_not: String

  """All values not containing the given string."""
  heroImageClasses_not_contains: String

  """All values not ending with the given string"""
  heroImageClasses_not_ends_with: String

  """All values that are not contained in given list."""
  heroImageClasses_not_in: [String]

  """All values not starting with the given string."""
  heroImageClasses_not_starts_with: String

  """All values starting with the given string."""
  heroImageClasses_starts_with: String
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  image: AssetWhereInput
  imageClasses: String

  """All values containing the given string."""
  imageClasses_contains: String

  """All values ending with the given string."""
  imageClasses_ends_with: String

  """All values that are contained in given list."""
  imageClasses_in: [String]

  """Any other value that exists and is not equal to the given value."""
  imageClasses_not: String

  """All values not containing the given string."""
  imageClasses_not_contains: String

  """All values not ending with the given string"""
  imageClasses_not_ends_with: String

  """All values that are not contained in given list."""
  imageClasses_not_in: [String]

  """All values not starting with the given string."""
  imageClasses_not_starts_with: String

  """All values starting with the given string."""
  imageClasses_starts_with: String
  instagramHandle: String

  """All values containing the given string."""
  instagramHandle_contains: String

  """All values ending with the given string."""
  instagramHandle_ends_with: String

  """All values that are contained in given list."""
  instagramHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  instagramHandle_not: String

  """All values not containing the given string."""
  instagramHandle_not_contains: String

  """All values not ending with the given string"""
  instagramHandle_not_ends_with: String

  """All values that are not contained in given list."""
  instagramHandle_not_in: [String]

  """All values not starting with the given string."""
  instagramHandle_not_starts_with: String

  """All values starting with the given string."""
  instagramHandle_starts_with: String
  name: String

  """All values containing the given string."""
  name_contains: String

  """All values ending with the given string."""
  name_ends_with: String

  """All values that are contained in given list."""
  name_in: [String]

  """Any other value that exists and is not equal to the given value."""
  name_not: String

  """All values not containing the given string."""
  name_not_contains: String

  """All values not ending with the given string"""
  name_not_ends_with: String

  """All values that are not contained in given list."""
  name_not_in: [String]

  """All values not starting with the given string."""
  name_not_starts_with: String

  """All values starting with the given string."""
  name_starts_with: String
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  slug: String

  """All values containing the given string."""
  slug_contains: String

  """All values ending with the given string."""
  slug_ends_with: String

  """All values that are contained in given list."""
  slug_in: [String]

  """Any other value that exists and is not equal to the given value."""
  slug_not: String

  """All values not containing the given string."""
  slug_not_contains: String

  """All values not ending with the given string"""
  slug_not_ends_with: String

  """All values that are not contained in given list."""
  slug_not_in: [String]

  """All values not starting with the given string."""
  slug_not_starts_with: String

  """All values starting with the given string."""
  slug_starts_with: String
  telegramHandle: String

  """All values containing the given string."""
  telegramHandle_contains: String

  """All values ending with the given string."""
  telegramHandle_ends_with: String

  """All values that are contained in given list."""
  telegramHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  telegramHandle_not: String

  """All values not containing the given string."""
  telegramHandle_not_contains: String

  """All values not ending with the given string"""
  telegramHandle_not_ends_with: String

  """All values that are not contained in given list."""
  telegramHandle_not_in: [String]

  """All values not starting with the given string."""
  telegramHandle_not_starts_with: String

  """All values starting with the given string."""
  telegramHandle_starts_with: String
  tiktokHandle: String

  """All values containing the given string."""
  tiktokHandle_contains: String

  """All values ending with the given string."""
  tiktokHandle_ends_with: String

  """All values that are contained in given list."""
  tiktokHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  tiktokHandle_not: String

  """All values not containing the given string."""
  tiktokHandle_not_contains: String

  """All values not ending with the given string"""
  tiktokHandle_not_ends_with: String

  """All values that are not contained in given list."""
  tiktokHandle_not_in: [String]

  """All values not starting with the given string."""
  tiktokHandle_not_starts_with: String

  """All values starting with the given string."""
  tiktokHandle_starts_with: String
  twitterHandle: String

  """All values containing the given string."""
  twitterHandle_contains: String

  """All values ending with the given string."""
  twitterHandle_ends_with: String

  """All values that are contained in given list."""
  twitterHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  twitterHandle_not: String

  """All values not containing the given string."""
  twitterHandle_not_contains: String

  """All values not ending with the given string"""
  twitterHandle_not_ends_with: String

  """All values that are not contained in given list."""
  twitterHandle_not_in: [String]

  """All values not starting with the given string."""
  twitterHandle_not_starts_with: String

  """All values starting with the given string."""
  twitterHandle_starts_with: String
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
  youtubeHandle: String

  """All values containing the given string."""
  youtubeHandle_contains: String

  """All values ending with the given string."""
  youtubeHandle_ends_with: String

  """All values that are contained in given list."""
  youtubeHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  youtubeHandle_not: String

  """All values not containing the given string."""
  youtubeHandle_not_contains: String

  """All values not ending with the given string"""
  youtubeHandle_not_ends_with: String

  """All values that are not contained in given list."""
  youtubeHandle_not_in: [String]

  """All values not starting with the given string."""
  youtubeHandle_not_starts_with: String

  """All values starting with the given string."""
  youtubeHandle_starts_with: String
}

enum OrganizerOrderByInput {
  createdAt_ASC
  createdAt_DESC
  discordWidgetId_ASC
  discordWidgetId_DESC
  facebookHandle_ASC
  facebookHandle_DESC
  heroImageClasses_ASC
  heroImageClasses_DESC
  id_ASC
  id_DESC
  imageClasses_ASC
  imageClasses_DESC
  instagramHandle_ASC
  instagramHandle_DESC
  name_ASC
  name_DESC
  publishedAt_ASC
  publishedAt_DESC
  slug_ASC
  slug_DESC
  telegramHandle_ASC
  telegramHandle_DESC
  tiktokHandle_ASC
  tiktokHandle_DESC
  twitterHandle_ASC
  twitterHandle_DESC
  updatedAt_ASC
  updatedAt_DESC
  youtubeHandle_ASC
  youtubeHandle_DESC
}

input OrganizerUpdateInput {
  clr7j9mmt0q2j01uo9zrs2fm7: PackUpdateManyInlineInput

  """description input for default locale (en)"""
  description: RichTextAST
  discordWidgetId: String
  events: EventUpdateManyInlineInput
  facebookHandle: String
  heroImage: AssetUpdateOneInlineInput
  heroImageClasses: String
  image: AssetUpdateOneInlineInput
  imageClasses: String
  instagramHandle: String

  """Manage document localizations"""
  localizations: OrganizerUpdateLocalizationsInput
  name: String
  slug: String
  telegramHandle: String
  tiktokHandle: String
  twitterHandle: String
  youtubeHandle: String
}

input OrganizerUpdateLocalizationDataInput {
  description: RichTextAST
}

input OrganizerUpdateLocalizationInput {
  data: OrganizerUpdateLocalizationDataInput!
  locale: Locale!
}

input OrganizerUpdateLocalizationsInput {
  """Localizations to create"""
  create: [OrganizerCreateLocalizationInput!]

  """Localizations to delete"""
  delete: [Locale!]

  """Localizations to update"""
  update: [OrganizerUpdateLocalizationInput!]
  upsert: [OrganizerUpsertLocalizationInput!]
}

input OrganizerUpdateManyInlineInput {
  """Connect multiple existing Organizer documents"""
  connect: [OrganizerConnectInput!]

  """Create and connect multiple Organizer documents"""
  create: [OrganizerCreateInput!]

  """Delete multiple Organizer documents"""
  delete: [OrganizerWhereUniqueInput!]

  """Disconnect multiple Organizer documents"""
  disconnect: [OrganizerWhereUniqueInput!]

  """
  Override currently-connected documents with multiple existing Organizer documents
  """
  set: [OrganizerWhereUniqueInput!]

  """Update multiple Organizer documents"""
  update: [OrganizerUpdateWithNestedWhereUniqueInput!]

  """Upsert multiple Organizer documents"""
  upsert: [OrganizerUpsertWithNestedWhereUniqueInput!]
}

input OrganizerUpdateManyInput {
  """description input for default locale (en)"""
  description: RichTextAST
  discordWidgetId: String
  facebookHandle: String
  heroImageClasses: String
  imageClasses: String
  instagramHandle: String

  """Optional updates to localizations"""
  localizations: OrganizerUpdateManyLocalizationsInput
  telegramHandle: String
  tiktokHandle: String
  twitterHandle: String
  youtubeHandle: String
}

input OrganizerUpdateManyLocalizationDataInput {
  description: RichTextAST
}

input OrganizerUpdateManyLocalizationInput {
  data: OrganizerUpdateManyLocalizationDataInput!
  locale: Locale!
}

input OrganizerUpdateManyLocalizationsInput {
  """Localizations to update"""
  update: [OrganizerUpdateManyLocalizationInput!]
}

input OrganizerUpdateOneInlineInput {
  """Connect existing Organizer document"""
  connect: OrganizerWhereUniqueInput

  """Create and connect one Organizer document"""
  create: OrganizerCreateInput

  """Delete currently connected Organizer document"""
  delete: Boolean

  """Disconnect currently connected Organizer document"""
  disconnect: Boolean

  """Update single Organizer document"""
  update: OrganizerUpdateWithNestedWhereUniqueInput

  """Upsert single Organizer document"""
  upsert: OrganizerUpsertWithNestedWhereUniqueInput
}

input OrganizerUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: OrganizerUpdateInput!

  """Unique document search"""
  where: OrganizerWhereUniqueInput!
}

input OrganizerUpsertInput {
  """Create document if it didn't exist"""
  create: OrganizerCreateInput!

  """Update document if it exists"""
  update: OrganizerUpdateInput!
}

input OrganizerUpsertLocalizationInput {
  create: OrganizerCreateLocalizationDataInput!
  locale: Locale!
  update: OrganizerUpdateLocalizationDataInput!
}

input OrganizerUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: OrganizerUpsertInput!

  """Unique document search"""
  where: OrganizerWhereUniqueInput!
}

"""
This contains a set of filters that can be used to compare values internally
"""
input OrganizerWhereComparatorInput {
  """
  This field can be used to request to check if the entry is outdated by internal comparison
  """
  outdated_to: Boolean
}

"""Identifies documents"""
input OrganizerWhereInput {
  """Logical AND on all given filters."""
  AND: [OrganizerWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [OrganizerWhereInput!]

  """Logical OR on all given filters."""
  OR: [OrganizerWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  discordWidgetId: String

  """All values containing the given string."""
  discordWidgetId_contains: String

  """All values ending with the given string."""
  discordWidgetId_ends_with: String

  """All values that are contained in given list."""
  discordWidgetId_in: [String]

  """Any other value that exists and is not equal to the given value."""
  discordWidgetId_not: String

  """All values not containing the given string."""
  discordWidgetId_not_contains: String

  """All values not ending with the given string"""
  discordWidgetId_not_ends_with: String

  """All values that are not contained in given list."""
  discordWidgetId_not_in: [String]

  """All values not starting with the given string."""
  discordWidgetId_not_starts_with: String

  """All values starting with the given string."""
  discordWidgetId_starts_with: String
  documentInStages_every: OrganizerWhereStageInput
  documentInStages_none: OrganizerWhereStageInput
  documentInStages_some: OrganizerWhereStageInput
  events_every: EventWhereInput
  events_none: EventWhereInput
  events_some: EventWhereInput
  facebookHandle: String

  """All values containing the given string."""
  facebookHandle_contains: String

  """All values ending with the given string."""
  facebookHandle_ends_with: String

  """All values that are contained in given list."""
  facebookHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  facebookHandle_not: String

  """All values not containing the given string."""
  facebookHandle_not_contains: String

  """All values not ending with the given string"""
  facebookHandle_not_ends_with: String

  """All values that are not contained in given list."""
  facebookHandle_not_in: [String]

  """All values not starting with the given string."""
  facebookHandle_not_starts_with: String

  """All values starting with the given string."""
  facebookHandle_starts_with: String
  heroImage: AssetWhereInput
  heroImageClasses: String

  """All values containing the given string."""
  heroImageClasses_contains: String

  """All values ending with the given string."""
  heroImageClasses_ends_with: String

  """All values that are contained in given list."""
  heroImageClasses_in: [String]

  """Any other value that exists and is not equal to the given value."""
  heroImageClasses_not: String

  """All values not containing the given string."""
  heroImageClasses_not_contains: String

  """All values not ending with the given string"""
  heroImageClasses_not_ends_with: String

  """All values that are not contained in given list."""
  heroImageClasses_not_in: [String]

  """All values not starting with the given string."""
  heroImageClasses_not_starts_with: String

  """All values starting with the given string."""
  heroImageClasses_starts_with: String
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  image: AssetWhereInput
  imageClasses: String

  """All values containing the given string."""
  imageClasses_contains: String

  """All values ending with the given string."""
  imageClasses_ends_with: String

  """All values that are contained in given list."""
  imageClasses_in: [String]

  """Any other value that exists and is not equal to the given value."""
  imageClasses_not: String

  """All values not containing the given string."""
  imageClasses_not_contains: String

  """All values not ending with the given string"""
  imageClasses_not_ends_with: String

  """All values that are not contained in given list."""
  imageClasses_not_in: [String]

  """All values not starting with the given string."""
  imageClasses_not_starts_with: String

  """All values starting with the given string."""
  imageClasses_starts_with: String
  instagramHandle: String

  """All values containing the given string."""
  instagramHandle_contains: String

  """All values ending with the given string."""
  instagramHandle_ends_with: String

  """All values that are contained in given list."""
  instagramHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  instagramHandle_not: String

  """All values not containing the given string."""
  instagramHandle_not_contains: String

  """All values not ending with the given string"""
  instagramHandle_not_ends_with: String

  """All values that are not contained in given list."""
  instagramHandle_not_in: [String]

  """All values not starting with the given string."""
  instagramHandle_not_starts_with: String

  """All values starting with the given string."""
  instagramHandle_starts_with: String
  name: String

  """All values containing the given string."""
  name_contains: String

  """All values ending with the given string."""
  name_ends_with: String

  """All values that are contained in given list."""
  name_in: [String]

  """Any other value that exists and is not equal to the given value."""
  name_not: String

  """All values not containing the given string."""
  name_not_contains: String

  """All values not ending with the given string"""
  name_not_ends_with: String

  """All values that are not contained in given list."""
  name_not_in: [String]

  """All values not starting with the given string."""
  name_not_starts_with: String

  """All values starting with the given string."""
  name_starts_with: String
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  slug: String

  """All values containing the given string."""
  slug_contains: String

  """All values ending with the given string."""
  slug_ends_with: String

  """All values that are contained in given list."""
  slug_in: [String]

  """Any other value that exists and is not equal to the given value."""
  slug_not: String

  """All values not containing the given string."""
  slug_not_contains: String

  """All values not ending with the given string"""
  slug_not_ends_with: String

  """All values that are not contained in given list."""
  slug_not_in: [String]

  """All values not starting with the given string."""
  slug_not_starts_with: String

  """All values starting with the given string."""
  slug_starts_with: String
  telegramHandle: String

  """All values containing the given string."""
  telegramHandle_contains: String

  """All values ending with the given string."""
  telegramHandle_ends_with: String

  """All values that are contained in given list."""
  telegramHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  telegramHandle_not: String

  """All values not containing the given string."""
  telegramHandle_not_contains: String

  """All values not ending with the given string"""
  telegramHandle_not_ends_with: String

  """All values that are not contained in given list."""
  telegramHandle_not_in: [String]

  """All values not starting with the given string."""
  telegramHandle_not_starts_with: String

  """All values starting with the given string."""
  telegramHandle_starts_with: String
  tiktokHandle: String

  """All values containing the given string."""
  tiktokHandle_contains: String

  """All values ending with the given string."""
  tiktokHandle_ends_with: String

  """All values that are contained in given list."""
  tiktokHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  tiktokHandle_not: String

  """All values not containing the given string."""
  tiktokHandle_not_contains: String

  """All values not ending with the given string"""
  tiktokHandle_not_ends_with: String

  """All values that are not contained in given list."""
  tiktokHandle_not_in: [String]

  """All values not starting with the given string."""
  tiktokHandle_not_starts_with: String

  """All values starting with the given string."""
  tiktokHandle_starts_with: String
  twitterHandle: String

  """All values containing the given string."""
  twitterHandle_contains: String

  """All values ending with the given string."""
  twitterHandle_ends_with: String

  """All values that are contained in given list."""
  twitterHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  twitterHandle_not: String

  """All values not containing the given string."""
  twitterHandle_not_contains: String

  """All values not ending with the given string"""
  twitterHandle_not_ends_with: String

  """All values that are not contained in given list."""
  twitterHandle_not_in: [String]

  """All values not starting with the given string."""
  twitterHandle_not_starts_with: String

  """All values starting with the given string."""
  twitterHandle_starts_with: String
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
  youtubeHandle: String

  """All values containing the given string."""
  youtubeHandle_contains: String

  """All values ending with the given string."""
  youtubeHandle_ends_with: String

  """All values that are contained in given list."""
  youtubeHandle_in: [String]

  """Any other value that exists and is not equal to the given value."""
  youtubeHandle_not: String

  """All values not containing the given string."""
  youtubeHandle_not_contains: String

  """All values not ending with the given string"""
  youtubeHandle_not_ends_with: String

  """All values that are not contained in given list."""
  youtubeHandle_not_in: [String]

  """All values not starting with the given string."""
  youtubeHandle_not_starts_with: String

  """All values starting with the given string."""
  youtubeHandle_starts_with: String
}

"""
The document in stages filter allows specifying a stage entry to cross compare the same document between different stages
"""
input OrganizerWhereStageInput {
  """Logical AND on all given filters."""
  AND: [OrganizerWhereStageInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [OrganizerWhereStageInput!]

  """Logical OR on all given filters."""
  OR: [OrganizerWhereStageInput!]

  """
  This field contains fields which can be set as true or false to specify an internal comparison
  """
  compareWithParent: OrganizerWhereComparatorInput

  """Specify the stage to compare with"""
  stage: Stage
}

"""References Organizer record uniquely"""
input OrganizerWhereUniqueInput {
  id: ID
  name: String
  slug: String
}

"""References Organizer record uniquely"""
input OrganizerWhereUniqueInput_remote_rel_eventParametersorganizer {
  name: String
  slug: String
}

"""References Organizer record uniquely"""
input OrganizerWhereUniqueInput_remote_rel_eventPassNftorganizer {
  name: String
  slug: String
}

"""References Organizer record uniquely"""
input OrganizerWhereUniqueInput_remote_rel_roleAssignmentorganizer {
  name: String
  slug: String
}

"The 'Pack' model represents a collection of unique NFTs (eventPasses) bundled together. It serves as a loot system for users, offering them a chance to receive one or more NFTs related to specific events. Each pack contains details about its contents and the associated event, fostering a more engaging and rewarding experience for users.\n"
type Pack implements Entity & Node {
  """The time the document was created"""
  createdAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """A brief overview detailing the contents and purpose of the Pack."""
  description: String!

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [Pack!]!

  """
  This section allows you to select or create the event passes that will be included in your Pack. Think of it as curating a collection of exclusive access tickets, each offering unique experiences for the events. Here, you can assemble a variety of event passes that together form the enticing bundle that is your Pack.
  """
  eventPasses(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `eventPasses` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `eventPasses` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
  ): [PackEventPasses!]!

  """List of Pack versions"""
  history(
    limit: Int! = 10
    skip: Int! = 0

    """
    This is optional and can be used to fetch the document version history for a specific stage instead of the current one
    """
    stageOverride: Stage
  ): [Version!]!

  """The unique identifier"""
  id: ID!

  """System Locale field"""
  locale: Locale!

  """Get the other localizations for this document"""
  localizations(
    """Decides if the current locale should be included or not"""
    includeCurrent: Boolean! = false

    """
    Potential locales that should be returned. 
    
    The order of locales will also override locale fall-backing behaviour in the query's subtree.
    
    Note any related model with localized fields in the query's subtree will be affected.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    
    Consider using this in conjunction with forceParentLocale on the children relation fields.
    """
    locales: [Locale!]! = [en, fr]
  ): [Pack!]!

  """
  User-friendly name of the the Pack, like "Lottery for VIP 3-Day Pass"
  """
  name: String!

  """
  Fixed description pertaining to the NFT Pack. This content is static and non-localizable.
  """
  nftDescription: String!

  """
  Permanent image representing the NFT Pack. Advised resolution is 800 x 800 pixels. Image content is non-changeable and cannot be localized.
  """
  nftImage(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `nftImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `nftImage` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Asset!

  """
  Permanent name associated with the NFT. Cannot be changed or localized.
  """
  nftName: String!
  organizer(
    """
    Sets the locale of the parent document as the first locale in the fallback locales in the query's subtree.
    
    Note that `organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): Organizer

  """The time the document was published. Null on documents in draft stage."""
  publishedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
  scheduledIn(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `scheduledIn` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """System stage field"""
  stage: Stage!

  """The time the document was updated"""
  updatedAt(
    """
    Variation of DateTime field to return, allows value from base document, current localization, or combined by returning the newer value of both
    """
    variation: SystemDateTimeFieldVariation! = COMBINED
  ): DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
}

input PackConnectInput {
  """
  Allow to specify document position in list of connected documents, will default to appending at end of list
  """
  position: ConnectPositionInput

  """Document to connect"""
  where: PackWhereUniqueInput!
}

"""A connection to a list of items."""
type PackConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [PackEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

input PackCreateInput {
  createdAt: DateTime

  """description input for default locale (en)"""
  description: String!
  eventPasses: PackEventPassesCreateManyInlineInput

  """
  Inline mutations for managing document localizations excluding the default locale
  """
  localizations: PackCreateLocalizationsInput

  """name input for default locale (en)"""
  name: String!
  nftDescription: String!
  nftImage: AssetCreateOneInlineInput!
  nftName: String!
  organizer: OrganizerCreateOneInlineInput
  updatedAt: DateTime
}

input PackCreateLocalizationDataInput {
  createdAt: DateTime
  description: String!
  name: String!
  updatedAt: DateTime
}

input PackCreateLocalizationInput {
  """Localization input"""
  data: PackCreateLocalizationDataInput!
  locale: Locale!
}

input PackCreateLocalizationsInput {
  """Create localizations for the newly-created document"""
  create: [PackCreateLocalizationInput!]
}

input PackCreateManyInlineInput {
  """Connect multiple existing Pack documents"""
  connect: [PackWhereUniqueInput!]

  """Create and connect multiple existing Pack documents"""
  create: [PackCreateInput!]
}

input PackCreateOneInlineInput {
  """Connect one existing Pack document"""
  connect: PackWhereUniqueInput

  """Create and connect one Pack document"""
  create: PackCreateInput
}

"""An edge in a connection."""
type PackEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: Pack!
}

union PackEventPasses = EventPass

input PackEventPassesConnectInput {
  EventPass: EventPassConnectInput
}

input PackEventPassesCreateInput {
  EventPass: EventPassCreateInput
}

input PackEventPassesCreateManyInlineInput {
  """Connect multiple existing PackEventPasses documents"""
  connect: [PackEventPassesWhereUniqueInput!]

  """Create and connect multiple existing PackEventPasses documents"""
  create: [PackEventPassesCreateInput!]
}

input PackEventPassesUpdateManyInlineInput {
  """Connect multiple existing PackEventPasses documents"""
  connect: [PackEventPassesConnectInput!]

  """Create and connect multiple PackEventPasses documents"""
  create: [PackEventPassesCreateInput!]

  """Delete multiple PackEventPasses documents"""
  delete: [PackEventPassesWhereUniqueInput!]

  """Disconnect multiple PackEventPasses documents"""
  disconnect: [PackEventPassesWhereUniqueInput!]

  """
  Override currently-connected documents with multiple existing PackEventPasses documents
  """
  set: [PackEventPassesWhereUniqueInput!]

  """Update multiple PackEventPasses documents"""
  update: [PackEventPassesUpdateWithNestedWhereUniqueInput!]

  """Upsert multiple PackEventPasses documents"""
  upsert: [PackEventPassesUpsertWithNestedWhereUniqueInput!]
}

input PackEventPassesUpdateWithNestedWhereUniqueInput {
  EventPass: EventPassUpdateWithNestedWhereUniqueInput
}

input PackEventPassesUpsertWithNestedWhereUniqueInput {
  EventPass: EventPassUpsertWithNestedWhereUniqueInput
}

input PackEventPassesWhereInput {
  EventPass: EventPassWhereInput
}

input PackEventPassesWhereUniqueInput {
  EventPass: EventPassWhereUniqueInput
}

"""Identifies documents"""
input PackManyWhereInput {
  """Logical AND on all given filters."""
  AND: [PackWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [PackWhereInput!]

  """Logical OR on all given filters."""
  OR: [PackWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  documentInStages_every: PackWhereStageInput
  documentInStages_none: PackWhereStageInput
  documentInStages_some: PackWhereStageInput

  """All values in which the union is empty"""
  eventPasses_empty: Boolean

  """
  Matches if the union contains at least one connection to the provided item to the filter
  """
  eventPasses_some: PackEventPassesWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  nftDescription: String

  """All values containing the given string."""
  nftDescription_contains: String

  """All values ending with the given string."""
  nftDescription_ends_with: String

  """All values that are contained in given list."""
  nftDescription_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftDescription_not: String

  """All values not containing the given string."""
  nftDescription_not_contains: String

  """All values not ending with the given string"""
  nftDescription_not_ends_with: String

  """All values that are not contained in given list."""
  nftDescription_not_in: [String]

  """All values not starting with the given string."""
  nftDescription_not_starts_with: String

  """All values starting with the given string."""
  nftDescription_starts_with: String
  nftImage: AssetWhereInput
  nftName: String

  """All values containing the given string."""
  nftName_contains: String

  """All values ending with the given string."""
  nftName_ends_with: String

  """All values that are contained in given list."""
  nftName_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftName_not: String

  """All values not containing the given string."""
  nftName_not_contains: String

  """All values not ending with the given string"""
  nftName_not_ends_with: String

  """All values that are not contained in given list."""
  nftName_not_in: [String]

  """All values not starting with the given string."""
  nftName_not_starts_with: String

  """All values starting with the given string."""
  nftName_starts_with: String
  organizer: OrganizerWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

enum PackOrderByInput {
  createdAt_ASC
  createdAt_DESC
  description_ASC
  description_DESC
  id_ASC
  id_DESC
  name_ASC
  name_DESC
  nftDescription_ASC
  nftDescription_DESC
  nftName_ASC
  nftName_DESC
  publishedAt_ASC
  publishedAt_DESC
  updatedAt_ASC
  updatedAt_DESC
}

input PackUpdateInput {
  """description input for default locale (en)"""
  description: String
  eventPasses: PackEventPassesUpdateManyInlineInput

  """Manage document localizations"""
  localizations: PackUpdateLocalizationsInput

  """name input for default locale (en)"""
  name: String
  nftDescription: String
  nftImage: AssetUpdateOneInlineInput
  nftName: String
  organizer: OrganizerUpdateOneInlineInput
}

input PackUpdateLocalizationDataInput {
  description: String
  name: String
}

input PackUpdateLocalizationInput {
  data: PackUpdateLocalizationDataInput!
  locale: Locale!
}

input PackUpdateLocalizationsInput {
  """Localizations to create"""
  create: [PackCreateLocalizationInput!]

  """Localizations to delete"""
  delete: [Locale!]

  """Localizations to update"""
  update: [PackUpdateLocalizationInput!]
  upsert: [PackUpsertLocalizationInput!]
}

input PackUpdateManyInlineInput {
  """Connect multiple existing Pack documents"""
  connect: [PackConnectInput!]

  """Create and connect multiple Pack documents"""
  create: [PackCreateInput!]

  """Delete multiple Pack documents"""
  delete: [PackWhereUniqueInput!]

  """Disconnect multiple Pack documents"""
  disconnect: [PackWhereUniqueInput!]

  """
  Override currently-connected documents with multiple existing Pack documents
  """
  set: [PackWhereUniqueInput!]

  """Update multiple Pack documents"""
  update: [PackUpdateWithNestedWhereUniqueInput!]

  """Upsert multiple Pack documents"""
  upsert: [PackUpsertWithNestedWhereUniqueInput!]
}

input PackUpdateManyInput {
  """description input for default locale (en)"""
  description: String

  """Optional updates to localizations"""
  localizations: PackUpdateManyLocalizationsInput

  """name input for default locale (en)"""
  name: String
  nftDescription: String
  nftName: String
}

input PackUpdateManyLocalizationDataInput {
  description: String
  name: String
}

input PackUpdateManyLocalizationInput {
  data: PackUpdateManyLocalizationDataInput!
  locale: Locale!
}

input PackUpdateManyLocalizationsInput {
  """Localizations to update"""
  update: [PackUpdateManyLocalizationInput!]
}

input PackUpdateOneInlineInput {
  """Connect existing Pack document"""
  connect: PackWhereUniqueInput

  """Create and connect one Pack document"""
  create: PackCreateInput

  """Delete currently connected Pack document"""
  delete: Boolean

  """Disconnect currently connected Pack document"""
  disconnect: Boolean

  """Update single Pack document"""
  update: PackUpdateWithNestedWhereUniqueInput

  """Upsert single Pack document"""
  upsert: PackUpsertWithNestedWhereUniqueInput
}

input PackUpdateWithNestedWhereUniqueInput {
  """Document to update"""
  data: PackUpdateInput!

  """Unique document search"""
  where: PackWhereUniqueInput!
}

input PackUpsertInput {
  """Create document if it didn't exist"""
  create: PackCreateInput!

  """Update document if it exists"""
  update: PackUpdateInput!
}

input PackUpsertLocalizationInput {
  create: PackCreateLocalizationDataInput!
  locale: Locale!
  update: PackUpdateLocalizationDataInput!
}

input PackUpsertWithNestedWhereUniqueInput {
  """Upsert data"""
  data: PackUpsertInput!

  """Unique document search"""
  where: PackWhereUniqueInput!
}

"""
This contains a set of filters that can be used to compare values internally
"""
input PackWhereComparatorInput {
  """
  This field can be used to request to check if the entry is outdated by internal comparison
  """
  outdated_to: Boolean
}

"""Identifies documents"""
input PackWhereInput {
  """Logical AND on all given filters."""
  AND: [PackWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [PackWhereInput!]

  """Logical OR on all given filters."""
  OR: [PackWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  description: String

  """All values containing the given string."""
  description_contains: String

  """All values ending with the given string."""
  description_ends_with: String

  """All values that are contained in given list."""
  description_in: [String]

  """Any other value that exists and is not equal to the given value."""
  description_not: String

  """All values not containing the given string."""
  description_not_contains: String

  """All values not ending with the given string"""
  description_not_ends_with: String

  """All values that are not contained in given list."""
  description_not_in: [String]

  """All values not starting with the given string."""
  description_not_starts_with: String

  """All values starting with the given string."""
  description_starts_with: String
  documentInStages_every: PackWhereStageInput
  documentInStages_none: PackWhereStageInput
  documentInStages_some: PackWhereStageInput

  """All values in which the union is empty"""
  eventPasses_empty: Boolean

  """
  Matches if the union contains at least one connection to the provided item to the filter
  """
  eventPasses_some: PackEventPassesWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  name: String

  """All values containing the given string."""
  name_contains: String

  """All values ending with the given string."""
  name_ends_with: String

  """All values that are contained in given list."""
  name_in: [String]

  """Any other value that exists and is not equal to the given value."""
  name_not: String

  """All values not containing the given string."""
  name_not_contains: String

  """All values not ending with the given string"""
  name_not_ends_with: String

  """All values that are not contained in given list."""
  name_not_in: [String]

  """All values not starting with the given string."""
  name_not_starts_with: String

  """All values starting with the given string."""
  name_starts_with: String
  nftDescription: String

  """All values containing the given string."""
  nftDescription_contains: String

  """All values ending with the given string."""
  nftDescription_ends_with: String

  """All values that are contained in given list."""
  nftDescription_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftDescription_not: String

  """All values not containing the given string."""
  nftDescription_not_contains: String

  """All values not ending with the given string"""
  nftDescription_not_ends_with: String

  """All values that are not contained in given list."""
  nftDescription_not_in: [String]

  """All values not starting with the given string."""
  nftDescription_not_starts_with: String

  """All values starting with the given string."""
  nftDescription_starts_with: String
  nftImage: AssetWhereInput
  nftName: String

  """All values containing the given string."""
  nftName_contains: String

  """All values ending with the given string."""
  nftName_ends_with: String

  """All values that are contained in given list."""
  nftName_in: [String]

  """Any other value that exists and is not equal to the given value."""
  nftName_not: String

  """All values not containing the given string."""
  nftName_not_contains: String

  """All values not ending with the given string"""
  nftName_not_ends_with: String

  """All values that are not contained in given list."""
  nftName_not_in: [String]

  """All values not starting with the given string."""
  nftName_not_starts_with: String

  """All values starting with the given string."""
  nftName_starts_with: String
  organizer: OrganizerWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  scheduledIn_every: ScheduledOperationWhereInput
  scheduledIn_none: ScheduledOperationWhereInput
  scheduledIn_some: ScheduledOperationWhereInput
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

"""
The document in stages filter allows specifying a stage entry to cross compare the same document between different stages
"""
input PackWhereStageInput {
  """Logical AND on all given filters."""
  AND: [PackWhereStageInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [PackWhereStageInput!]

  """Logical OR on all given filters."""
  OR: [PackWhereStageInput!]

  """
  This field contains fields which can be set as true or false to specify an internal comparison
  """
  compareWithParent: PackWhereComparatorInput

  """Specify the stage to compare with"""
  stage: Stage
}

"""References Pack record uniquely"""
input PackWhereUniqueInput {
  id: ID
}

"""Information about pagination in a connection."""
type PageInfo {
  """When paginating forwards, the cursor to continue."""
  endCursor: String

  """When paginating forwards, are there more items?"""
  hasNextPage: Boolean!

  """When paginating backwards, are there more items?"""
  hasPreviousPage: Boolean!

  """Number of items in the current page."""
  pageSize: Int

  """When paginating backwards, the cursor to continue."""
  startCursor: String
}

"""
Define the options of an 'Event Pass' on an 'Event Date Location'. You can define severals if the event have multiple locations.
"""
type PassOption implements Entity {
  """
  Description of the option, like "Access to the event on Day 1"
  """
  description: String

  """Define the location and date for this option."""
  eventDateLocation(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `eventDateLocation` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `eventDateLocation` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): EventDateLocation

  """The unique identifier"""
  id: ID!

  """System Locale field"""
  locale: Locale!

  """Get the other localizations for this document"""
  localizations(
    """Decides if the current locale should be included or not"""
    includeCurrent: Boolean! = false

    """
    Potential locales that should be returned. 
    
    The order of locales will also override locale fall-backing behaviour in the query's subtree.
    
    Note any related model with localized fields in the query's subtree will be affected.
    The first locale matching the provided list will be returned, localized entries that do not have the provided locale defined will be filtered out.
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    
    Consider using this in conjunction with forceParentLocale on the children relation fields.
    """
    locales: [Locale!]! = [en, fr]
  ): [PassOption!]!

  """
  Name of the options, like "Day 1 Access" or "VIP Room Access"
  """
  name: String!

  """System stage field"""
  stage: Stage!
}

input PassOptionCreateInput {
  """description input for default locale (en)"""
  description: String
  eventDateLocation: EventDateLocationCreateOneInlineInput

  """
  Inline mutations for managing document localizations excluding the default locale
  """
  localizations: PassOptionCreateLocalizationsInput

  """name input for default locale (en)"""
  name: String!
}

input PassOptionCreateLocalizationDataInput {
  description: String
  name: String!
}

input PassOptionCreateLocalizationInput {
  """Localization input"""
  data: PassOptionCreateLocalizationDataInput!
  locale: Locale!
}

input PassOptionCreateLocalizationsInput {
  """Create localizations for the newly-created document"""
  create: [PassOptionCreateLocalizationInput!]
}

input PassOptionCreateManyInlineInput {
  """Create and connect multiple existing PassOption documents"""
  create: [PassOptionCreateInput!]
}

input PassOptionCreateWithPositionInput {
  """Document to create"""
  data: PassOptionCreateInput!

  """
  Position in the list of existing component instances, will default to appending at the end of list
  """
  position: ConnectPositionInput
}

enum PassOptionOrderByInput {
  description_ASC
  description_DESC
  id_ASC
  id_DESC
  name_ASC
  name_DESC
}

input PassOptionUpdateInput {
  """description input for default locale (en)"""
  description: String
  eventDateLocation: EventDateLocationUpdateOneInlineInput

  """Manage document localizations"""
  localizations: PassOptionUpdateLocalizationsInput

  """name input for default locale (en)"""
  name: String
}

input PassOptionUpdateLocalizationDataInput {
  description: String
  name: String
}

input PassOptionUpdateLocalizationInput {
  data: PassOptionUpdateLocalizationDataInput!
  locale: Locale!
}

input PassOptionUpdateLocalizationsInput {
  """Localizations to create"""
  create: [PassOptionCreateLocalizationInput!]

  """Localizations to delete"""
  delete: [Locale!]

  """Localizations to update"""
  update: [PassOptionUpdateLocalizationInput!]
  upsert: [PassOptionUpsertLocalizationInput!]
}

input PassOptionUpdateManyInlineInput {
  """Create and connect multiple PassOption component instances"""
  create: [PassOptionCreateWithPositionInput!]

  """Delete multiple PassOption documents"""
  delete: [PassOptionWhereUniqueInput!]

  """Update multiple PassOption component instances"""
  update: [PassOptionUpdateWithNestedWhereUniqueAndPositionInput!]

  """Upsert multiple PassOption component instances"""
  upsert: [PassOptionUpsertWithNestedWhereUniqueAndPositionInput!]
}

input PassOptionUpdateWithNestedWhereUniqueAndPositionInput {
  """Document to update"""
  data: PassOptionUpdateInput

  """
  Position in the list of existing component instances, will default to appending at the end of list
  """
  position: ConnectPositionInput

  """Unique component instance search"""
  where: PassOptionWhereUniqueInput!
}

input PassOptionUpsertInput {
  """Create document if it didn't exist"""
  create: PassOptionCreateInput!

  """Update document if it exists"""
  update: PassOptionUpdateInput!
}

input PassOptionUpsertLocalizationInput {
  create: PassOptionCreateLocalizationDataInput!
  locale: Locale!
  update: PassOptionUpdateLocalizationDataInput!
}

input PassOptionUpsertWithNestedWhereUniqueAndPositionInput {
  """Document to upsert"""
  data: PassOptionUpsertInput

  """
  Position in the list of existing component instances, will default to appending at the end of list
  """
  position: ConnectPositionInput

  """Unique component instance search"""
  where: PassOptionWhereUniqueInput!
}

"""Identifies documents"""
input PassOptionWhereInput {
  """Logical AND on all given filters."""
  AND: [PassOptionWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [PassOptionWhereInput!]

  """Logical OR on all given filters."""
  OR: [PassOptionWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  description: String

  """All values containing the given string."""
  description_contains: String

  """All values ending with the given string."""
  description_ends_with: String

  """All values that are contained in given list."""
  description_in: [String]

  """Any other value that exists and is not equal to the given value."""
  description_not: String

  """All values not containing the given string."""
  description_not_contains: String

  """All values not ending with the given string"""
  description_not_ends_with: String

  """All values that are not contained in given list."""
  description_not_in: [String]

  """All values not starting with the given string."""
  description_not_starts_with: String

  """All values starting with the given string."""
  description_starts_with: String
  eventDateLocation: EventDateLocationWhereInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  name: String

  """All values containing the given string."""
  name_contains: String

  """All values ending with the given string."""
  name_ends_with: String

  """All values that are contained in given list."""
  name_in: [String]

  """Any other value that exists and is not equal to the given value."""
  name_not: String

  """All values not containing the given string."""
  name_not_contains: String

  """All values not ending with the given string"""
  name_not_ends_with: String

  """All values that are not contained in given list."""
  name_not_in: [String]

  """All values not starting with the given string."""
  name_not_starts_with: String

  """All values starting with the given string."""
  name_starts_with: String
}

"""References PassOption record uniquely"""
input PassOptionWhereUniqueInput {
  id: ID
}

"""Slate-compatible RichText AST"""
scalar RichTextAST

"""Scheduled Operation system model"""
type ScheduledOperation implements Entity & Node {
  affectedDocuments(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `affectedDocuments` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `affectedDocuments` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    skip: Int
  ): [ScheduledOperationAffectedDocument!]!

  """The time the document was created"""
  createdAt: DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """Operation description"""
  description: String

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [ScheduledOperation!]!

  """Operation error message"""
  errorMessage: String

  """The unique identifier"""
  id: ID!

  """The time the document was published. Null on documents in draft stage."""
  publishedAt: DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """
  Raw operation payload including all details, this field is subject to change
  """
  rawPayload: Json!

  """The release this operation is scheduled for"""
  release(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `release` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `release` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): ScheduledRelease

  """System stage field"""
  stage: Stage!

  """operation Status"""
  status: ScheduledOperationStatus!

  """The time the document was updated"""
  updatedAt: DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
}

union ScheduledOperationAffectedDocument = Asset | Event | EventPass | EventPassDelayedRevealed | Organizer | Pack

"""A connection to a list of items."""
type ScheduledOperationConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [ScheduledOperationEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

"""An edge in a connection."""
type ScheduledOperationEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: ScheduledOperation!
}

enum ScheduledOperationOrderByInput {
  createdAt_ASC
  createdAt_DESC
  description_ASC
  description_DESC
  errorMessage_ASC
  errorMessage_DESC
  id_ASC
  id_DESC
  publishedAt_ASC
  publishedAt_DESC
  status_ASC
  status_DESC
  updatedAt_ASC
  updatedAt_DESC
}

"""System Scheduled Operation Status"""
enum ScheduledOperationStatus {
  CANCELED
  COMPLETED
  FAILED
  IN_PROGRESS
  PENDING
}

"""Identifies documents"""
input ScheduledOperationWhereInput {
  """Logical AND on all given filters."""
  AND: [ScheduledOperationWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [ScheduledOperationWhereInput!]

  """Logical OR on all given filters."""
  OR: [ScheduledOperationWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  description: String

  """All values containing the given string."""
  description_contains: String

  """All values ending with the given string."""
  description_ends_with: String

  """All values that are contained in given list."""
  description_in: [String]

  """Any other value that exists and is not equal to the given value."""
  description_not: String

  """All values not containing the given string."""
  description_not_contains: String

  """All values not ending with the given string"""
  description_not_ends_with: String

  """All values that are not contained in given list."""
  description_not_in: [String]

  """All values not starting with the given string."""
  description_not_starts_with: String

  """All values starting with the given string."""
  description_starts_with: String
  errorMessage: String

  """All values containing the given string."""
  errorMessage_contains: String

  """All values ending with the given string."""
  errorMessage_ends_with: String

  """All values that are contained in given list."""
  errorMessage_in: [String]

  """Any other value that exists and is not equal to the given value."""
  errorMessage_not: String

  """All values not containing the given string."""
  errorMessage_not_contains: String

  """All values not ending with the given string"""
  errorMessage_not_ends_with: String

  """All values that are not contained in given list."""
  errorMessage_not_in: [String]

  """All values not starting with the given string."""
  errorMessage_not_starts_with: String

  """All values starting with the given string."""
  errorMessage_starts_with: String
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput

  """All values containing the given json path."""
  rawPayload_json_path_exists: String

  """
  Recursively tries to find the provided JSON scalar value inside the field.
  It does use an exact match when comparing values.
  If you pass `null` as value the filter will be ignored. 
  Note: This filter fails if you try to look for a non scalar JSON value!
  """
  rawPayload_value_recursive: Json
  release: ScheduledReleaseWhereInput
  status: ScheduledOperationStatus

  """All values that are contained in given list."""
  status_in: [ScheduledOperationStatus]

  """Any other value that exists and is not equal to the given value."""
  status_not: ScheduledOperationStatus

  """All values that are not contained in given list."""
  status_not_in: [ScheduledOperationStatus]
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

"""References ScheduledOperation record uniquely"""
input ScheduledOperationWhereUniqueInput {
  id: ID
}

"""Scheduled Release system model"""
type ScheduledRelease implements Entity & Node {
  """The time the document was created"""
  createdAt: DateTime!

  """User that created this document"""
  createdBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `createdBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """Release description"""
  description: String

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [ScheduledRelease!]!

  """Release error message"""
  errorMessage: String

  """The unique identifier"""
  id: ID!

  """Whether scheduled release should be run"""
  isActive: Boolean!

  """Whether scheduled release is implicit"""
  isImplicit: Boolean!

  """Operations to run with this release"""
  operations(
    after: String
    before: String
    first: Int

    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `operations` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean
    last: Int

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `operations` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
    orderBy: ScheduledOperationOrderByInput
    skip: Int
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """The time the document was published. Null on documents in draft stage."""
  publishedAt: DateTime

  """User that last published this document"""
  publishedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `publishedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User

  """Release date and time"""
  releaseAt: DateTime

  """System stage field"""
  stage: Stage!

  """Release Status"""
  status: ScheduledReleaseStatus!

  """Release Title"""
  title: String

  """The time the document was updated"""
  updatedAt: DateTime!

  """User that last updated this document"""
  updatedBy(
    """
    Sets the locale of the resolved parent document as the only locale in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locale will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will affect any existing locale filtering defined in the query's tree for the subtree.
    """
    forceParentLocale: Boolean

    """
    Allows to optionally override locale filtering behaviour in the query's subtree.
    
    Note that `updatedBy` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument will overwrite any existing locale filtering defined in the query's tree for the subtree.
    """
    locales: [Locale!]
  ): User
}

"""A connection to a list of items."""
type ScheduledReleaseConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [ScheduledReleaseEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

input ScheduledReleaseCreateInput {
  createdAt: DateTime
  description: String
  errorMessage: String
  isActive: Boolean = true
  releaseAt: DateTime
  title: String
  updatedAt: DateTime
}

"""An edge in a connection."""
type ScheduledReleaseEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: ScheduledRelease!
}

enum ScheduledReleaseOrderByInput {
  createdAt_ASC
  createdAt_DESC
  description_ASC
  description_DESC
  errorMessage_ASC
  errorMessage_DESC
  id_ASC
  id_DESC
  isActive_ASC
  isActive_DESC
  isImplicit_ASC
  isImplicit_DESC
  publishedAt_ASC
  publishedAt_DESC
  releaseAt_ASC
  releaseAt_DESC
  status_ASC
  status_DESC
  title_ASC
  title_DESC
  updatedAt_ASC
  updatedAt_DESC
}

"""System Scheduled Release Status"""
enum ScheduledReleaseStatus {
  COMPLETED
  FAILED
  IN_PROGRESS
  PENDING
}

input ScheduledReleaseUpdateInput {
  description: String
  errorMessage: String
  isActive: Boolean
  releaseAt: DateTime
  title: String
}

"""Identifies documents"""
input ScheduledReleaseWhereInput {
  """Logical AND on all given filters."""
  AND: [ScheduledReleaseWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [ScheduledReleaseWhereInput!]

  """Logical OR on all given filters."""
  OR: [ScheduledReleaseWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  createdBy: UserWhereInput
  description: String

  """All values containing the given string."""
  description_contains: String

  """All values ending with the given string."""
  description_ends_with: String

  """All values that are contained in given list."""
  description_in: [String]

  """Any other value that exists and is not equal to the given value."""
  description_not: String

  """All values not containing the given string."""
  description_not_contains: String

  """All values not ending with the given string"""
  description_not_ends_with: String

  """All values that are not contained in given list."""
  description_not_in: [String]

  """All values not starting with the given string."""
  description_not_starts_with: String

  """All values starting with the given string."""
  description_starts_with: String
  errorMessage: String

  """All values containing the given string."""
  errorMessage_contains: String

  """All values ending with the given string."""
  errorMessage_ends_with: String

  """All values that are contained in given list."""
  errorMessage_in: [String]

  """Any other value that exists and is not equal to the given value."""
  errorMessage_not: String

  """All values not containing the given string."""
  errorMessage_not_contains: String

  """All values not ending with the given string"""
  errorMessage_not_ends_with: String

  """All values that are not contained in given list."""
  errorMessage_not_in: [String]

  """All values not starting with the given string."""
  errorMessage_not_starts_with: String

  """All values starting with the given string."""
  errorMessage_starts_with: String
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  isActive: Boolean

  """Any other value that exists and is not equal to the given value."""
  isActive_not: Boolean
  isImplicit: Boolean

  """Any other value that exists and is not equal to the given value."""
  isImplicit_not: Boolean
  operations_every: ScheduledOperationWhereInput
  operations_none: ScheduledOperationWhereInput
  operations_some: ScheduledOperationWhereInput
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  publishedBy: UserWhereInput
  releaseAt: DateTime

  """All values greater than the given value."""
  releaseAt_gt: DateTime

  """All values greater than or equal the given value."""
  releaseAt_gte: DateTime

  """All values that are contained in given list."""
  releaseAt_in: [DateTime]

  """All values less than the given value."""
  releaseAt_lt: DateTime

  """All values less than or equal the given value."""
  releaseAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  releaseAt_not: DateTime

  """All values that are not contained in given list."""
  releaseAt_not_in: [DateTime]
  status: ScheduledReleaseStatus

  """All values that are contained in given list."""
  status_in: [ScheduledReleaseStatus]

  """Any other value that exists and is not equal to the given value."""
  status_not: ScheduledReleaseStatus

  """All values that are not contained in given list."""
  status_not_in: [ScheduledReleaseStatus]
  title: String

  """All values containing the given string."""
  title_contains: String

  """All values ending with the given string."""
  title_ends_with: String

  """All values that are contained in given list."""
  title_in: [String]

  """Any other value that exists and is not equal to the given value."""
  title_not: String

  """All values not containing the given string."""
  title_not_contains: String

  """All values not ending with the given string"""
  title_not_ends_with: String

  """All values that are not contained in given list."""
  title_not_in: [String]

  """All values not starting with the given string."""
  title_not_starts_with: String

  """All values starting with the given string."""
  title_starts_with: String
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
  updatedBy: UserWhereInput
}

"""References ScheduledRelease record uniquely"""
input ScheduledReleaseWhereUniqueInput {
  id: ID
}

"""Stage system enumeration"""
enum Stage {
  """The Draft is the default stage for all your content."""
  DRAFT

  """The Published stage is where you can publish your content to."""
  PUBLISHED
}

"""
Boolean expression to compare columns of type "String". All fields are combined with logical 'AND'.
"""
input String_comparison_exp {
  _eq: String
  _gt: String
  _gte: String

  """does the column match the given case-insensitive pattern"""
  _ilike: String
  _in: [String!]

  """
  does the column match the given POSIX regular expression, case insensitive
  """
  _iregex: String
  _is_null: Boolean

  """does the column match the given pattern"""
  _like: String
  _lt: String
  _lte: String
  _neq: String

  """does the column NOT match the given case-insensitive pattern"""
  _nilike: String
  _nin: [String!]

  """
  does the column NOT match the given POSIX regular expression, case insensitive
  """
  _niregex: String

  """does the column NOT match the given pattern"""
  _nlike: String

  """
  does the column NOT match the given POSIX regular expression, case sensitive
  """
  _nregex: String

  """does the column NOT match the given SQL regular expression"""
  _nsimilar: String

  """
  does the column match the given POSIX regular expression, case sensitive
  """
  _regex: String

  """does the column match the given SQL regular expression"""
  _similar: String
}

enum SystemDateTimeFieldVariation {
  BASE
  COMBINED
  LOCALIZATION
}

"""User system model"""
type User implements Entity & Node {
  """The time the document was created"""
  createdAt: DateTime!

  """Get the document in other stages"""
  documentInStages(
    """Decides if the current stage should be included or not"""
    includeCurrent: Boolean! = false

    """
    Decides if the documents should match the parent documents locale or should use the fallback order defined in the tree
    """
    inheritLocale: Boolean! = false

    """Potential stages that should be returned"""
    stages: [Stage!]! = [DRAFT, PUBLISHED]
  ): [User!]!

  """The unique identifier"""
  id: ID!

  """Flag to determine if user is active or not"""
  isActive: Boolean!

  """User Kind. Can be either MEMBER, PAT or PUBLIC"""
  kind: UserKind!

  """The username"""
  name: String!

  """Profile Picture url"""
  picture: String

  """The time the document was published. Null on documents in draft stage."""
  publishedAt: DateTime

  """System stage field"""
  stage: Stage!

  """The time the document was updated"""
  updatedAt: DateTime!
}

"""A connection to a list of items."""
type UserConnection {
  aggregate: Aggregate!

  """A list of edges."""
  edges: [UserEdge!]!

  """Information to aid in pagination."""
  pageInfo: PageInfo!
}

"""An edge in a connection."""
type UserEdge {
  """A cursor for use in pagination."""
  cursor: String!

  """The item at the end of the edge."""
  node: User!
}

"""System User Kind"""
enum UserKind {
  APP_TOKEN
  MEMBER
  PAT
  PUBLIC
  WEBHOOK
}

enum UserOrderByInput {
  createdAt_ASC
  createdAt_DESC
  id_ASC
  id_DESC
  isActive_ASC
  isActive_DESC
  kind_ASC
  kind_DESC
  name_ASC
  name_DESC
  picture_ASC
  picture_DESC
  publishedAt_ASC
  publishedAt_DESC
  updatedAt_ASC
  updatedAt_DESC
}

"""
This contains a set of filters that can be used to compare values internally
"""
input UserWhereComparatorInput {
  """
  This field can be used to request to check if the entry is outdated by internal comparison
  """
  outdated_to: Boolean
}

"""Identifies documents"""
input UserWhereInput {
  """Logical AND on all given filters."""
  AND: [UserWhereInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [UserWhereInput!]

  """Logical OR on all given filters."""
  OR: [UserWhereInput!]

  """Contains search across all appropriate fields."""
  _search: String
  createdAt: DateTime

  """All values greater than the given value."""
  createdAt_gt: DateTime

  """All values greater than or equal the given value."""
  createdAt_gte: DateTime

  """All values that are contained in given list."""
  createdAt_in: [DateTime]

  """All values less than the given value."""
  createdAt_lt: DateTime

  """All values less than or equal the given value."""
  createdAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  createdAt_not: DateTime

  """All values that are not contained in given list."""
  createdAt_not_in: [DateTime]
  documentInStages_every: UserWhereStageInput
  documentInStages_none: UserWhereStageInput
  documentInStages_some: UserWhereStageInput
  id: ID

  """All values containing the given string."""
  id_contains: ID

  """All values ending with the given string."""
  id_ends_with: ID

  """All values that are contained in given list."""
  id_in: [ID]

  """Any other value that exists and is not equal to the given value."""
  id_not: ID

  """All values not containing the given string."""
  id_not_contains: ID

  """All values not ending with the given string"""
  id_not_ends_with: ID

  """All values that are not contained in given list."""
  id_not_in: [ID]

  """All values not starting with the given string."""
  id_not_starts_with: ID

  """All values starting with the given string."""
  id_starts_with: ID
  isActive: Boolean

  """Any other value that exists and is not equal to the given value."""
  isActive_not: Boolean
  kind: UserKind

  """All values that are contained in given list."""
  kind_in: [UserKind]

  """Any other value that exists and is not equal to the given value."""
  kind_not: UserKind

  """All values that are not contained in given list."""
  kind_not_in: [UserKind]
  name: String

  """All values containing the given string."""
  name_contains: String

  """All values ending with the given string."""
  name_ends_with: String

  """All values that are contained in given list."""
  name_in: [String]

  """Any other value that exists and is not equal to the given value."""
  name_not: String

  """All values not containing the given string."""
  name_not_contains: String

  """All values not ending with the given string"""
  name_not_ends_with: String

  """All values that are not contained in given list."""
  name_not_in: [String]

  """All values not starting with the given string."""
  name_not_starts_with: String

  """All values starting with the given string."""
  name_starts_with: String
  picture: String

  """All values containing the given string."""
  picture_contains: String

  """All values ending with the given string."""
  picture_ends_with: String

  """All values that are contained in given list."""
  picture_in: [String]

  """Any other value that exists and is not equal to the given value."""
  picture_not: String

  """All values not containing the given string."""
  picture_not_contains: String

  """All values not ending with the given string"""
  picture_not_ends_with: String

  """All values that are not contained in given list."""
  picture_not_in: [String]

  """All values not starting with the given string."""
  picture_not_starts_with: String

  """All values starting with the given string."""
  picture_starts_with: String
  publishedAt: DateTime

  """All values greater than the given value."""
  publishedAt_gt: DateTime

  """All values greater than or equal the given value."""
  publishedAt_gte: DateTime

  """All values that are contained in given list."""
  publishedAt_in: [DateTime]

  """All values less than the given value."""
  publishedAt_lt: DateTime

  """All values less than or equal the given value."""
  publishedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  publishedAt_not: DateTime

  """All values that are not contained in given list."""
  publishedAt_not_in: [DateTime]
  updatedAt: DateTime

  """All values greater than the given value."""
  updatedAt_gt: DateTime

  """All values greater than or equal the given value."""
  updatedAt_gte: DateTime

  """All values that are contained in given list."""
  updatedAt_in: [DateTime]

  """All values less than the given value."""
  updatedAt_lt: DateTime

  """All values less than or equal the given value."""
  updatedAt_lte: DateTime

  """Any other value that exists and is not equal to the given value."""
  updatedAt_not: DateTime

  """All values that are not contained in given list."""
  updatedAt_not_in: [DateTime]
}

"""
The document in stages filter allows specifying a stage entry to cross compare the same document between different stages
"""
input UserWhereStageInput {
  """Logical AND on all given filters."""
  AND: [UserWhereStageInput!]

  """Logical NOT on all given filters combined by AND."""
  NOT: [UserWhereStageInput!]

  """Logical OR on all given filters."""
  OR: [UserWhereStageInput!]

  """
  This field contains fields which can be set as true or false to specify an internal comparison
  """
  compareWithParent: UserWhereComparatorInput

  """Specify the stage to compare with"""
  stage: Stage
}

"""References User record uniquely"""
input UserWhereUniqueInput {
  id: ID
}

type Version {
  createdAt: DateTime!
  id: ID!
  revision: Int!
  stage: Stage!
}

input VersionWhereInput {
  id: ID!
  revision: Int!
  stage: Stage!
}

"""
An account can represent a user or a role on an organizer. It stores essential information and is used as the root class for relationships with other tables
"""
type account {
  address: String!
  created_at: timestamptz
  email: String
  id: uuid!

  """An object relationship"""
  kyc: kyc
  phone: String

  """An array relationship"""
  roles(
    """distinct select on columns"""
    distinct_on: [roleAssignment_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roleAssignment_order_by!]

    """filter the rows returned"""
    where: roleAssignment_bool_exp
  ): [roleAssignment!]!

  """An aggregate relationship"""
  roles_aggregate(
    """distinct select on columns"""
    distinct_on: [roleAssignment_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roleAssignment_order_by!]

    """filter the rows returned"""
    where: roleAssignment_bool_exp
  ): roleAssignment_aggregate!
  scwAddress: String

  """An object relationship"""
  stripeCustomer: stripeCustomer
  updated_at: timestamptz
}

"""
aggregated selection of "account"
"""
type account_aggregate {
  aggregate: account_aggregate_fields
  nodes: [account!]!
}

"""
aggregate fields of "account"
"""
type account_aggregate_fields {
  count(columns: [account_select_column!], distinct: Boolean): Int!
  max: account_max_fields
  min: account_min_fields
}

"""
Boolean expression to filter rows from the table "account". All fields are combined with a logical 'AND'.
"""
input account_bool_exp {
  _and: [account_bool_exp!]
  _not: account_bool_exp
  _or: [account_bool_exp!]
  address: String_comparison_exp
  created_at: timestamptz_comparison_exp
  email: String_comparison_exp
  id: uuid_comparison_exp
  kyc: kyc_bool_exp
  phone: String_comparison_exp
  roles: roleAssignment_bool_exp
  roles_aggregate: roleAssignment_aggregate_bool_exp
  scwAddress: String_comparison_exp
  stripeCustomer: stripeCustomer_bool_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "account"
"""
enum account_constraint {
  """
  unique or primary key constraint on columns "address"
  """
  account_address_key

  """
  unique or primary key constraint on columns "id"
  """
  account_pkey
}

"""
input type for inserting data into table "account"
"""
input account_insert_input {
  address: String
  created_at: timestamptz
  email: String
  id: uuid
  kyc: kyc_obj_rel_insert_input
  phone: String
  roles: roleAssignment_arr_rel_insert_input
  scwAddress: String
  stripeCustomer: stripeCustomer_obj_rel_insert_input
  updated_at: timestamptz
}

"""aggregate max on columns"""
type account_max_fields {
  address: String
  created_at: timestamptz
  email: String
  id: uuid
  phone: String
  scwAddress: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type account_min_fields {
  address: String
  created_at: timestamptz
  email: String
  id: uuid
  phone: String
  scwAddress: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "account"
"""
type account_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [account!]!
}

"""
input type for inserting object relation for remote table "account"
"""
input account_obj_rel_insert_input {
  data: account_insert_input!

  """upsert condition"""
  on_conflict: account_on_conflict
}

"""
on_conflict condition type for table "account"
"""
input account_on_conflict {
  constraint: account_constraint!
  update_columns: [account_update_column!]! = []
  where: account_bool_exp
}

"""Ordering options when selecting data from "account"."""
input account_order_by {
  address: order_by
  created_at: order_by
  email: order_by
  id: order_by
  kyc: kyc_order_by
  phone: order_by
  roles_aggregate: roleAssignment_aggregate_order_by
  scwAddress: order_by
  stripeCustomer: stripeCustomer_order_by
  updated_at: order_by
}

"""primary key columns input for table: account"""
input account_pk_columns_input {
  id: uuid!
}

"""
select columns of table "account"
"""
enum account_select_column {
  """column name"""
  address

  """column name"""
  created_at

  """column name"""
  email

  """column name"""
  id

  """column name"""
  phone

  """column name"""
  scwAddress

  """column name"""
  updated_at
}

"""
input type for updating data in table "account"
"""
input account_set_input {
  address: String
  created_at: timestamptz
  email: String
  id: uuid
  phone: String
  scwAddress: String
  updated_at: timestamptz
}

"""
Streaming cursor of the table "account"
"""
input account_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: account_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input account_stream_cursor_value_input {
  address: String
  created_at: timestamptz
  email: String
  id: uuid
  phone: String
  scwAddress: String
  updated_at: timestamptz
}

"""
update columns of table "account"
"""
enum account_update_column {
  """column name"""
  address

  """column name"""
  created_at

  """column name"""
  email

  """column name"""
  id

  """column name"""
  phone

  """column name"""
  scwAddress

  """column name"""
  updated_at
}

input account_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: account_set_input

  """filter the rows which have to be updated"""
  where: account_bool_exp!
}

scalar bigint

"""
Boolean expression to compare columns of type "bigint". All fields are combined with logical 'AND'.
"""
input bigint_comparison_exp {
  _eq: bigint
  _gt: bigint
  _gte: bigint
  _in: [bigint!]
  _is_null: Boolean
  _lt: bigint
  _lte: bigint
  _neq: bigint
  _nin: [bigint!]
}

"""
Currencies code following the standard ISO 4217 (https://en.wikipedia.org/wiki/ISO_4217)
"""
type currency {
  value: String!
}

"""
aggregated selection of "currency"
"""
type currency_aggregate {
  aggregate: currency_aggregate_fields
  nodes: [currency!]!
}

"""
aggregate fields of "currency"
"""
type currency_aggregate_fields {
  count(columns: [currency_select_column!], distinct: Boolean): Int!
  max: currency_max_fields
  min: currency_min_fields
}

"""
Boolean expression to filter rows from the table "currency". All fields are combined with a logical 'AND'.
"""
input currency_bool_exp {
  _and: [currency_bool_exp!]
  _not: currency_bool_exp
  _or: [currency_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "currency"
"""
enum currency_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  currency_pkey
}

enum currency_enum {
  AED
  CNY
  EUR
  GBP
  QAR
  SGD
  USD
}

"""
Boolean expression to compare columns of type "currency_enum". All fields are combined with logical 'AND'.
"""
input currency_enum_comparison_exp {
  _eq: currency_enum
  _in: [currency_enum!]
  _is_null: Boolean
  _neq: currency_enum
  _nin: [currency_enum!]
}

"""
input type for inserting data into table "currency"
"""
input currency_insert_input {
  value: String
}

"""aggregate max on columns"""
type currency_max_fields {
  value: String
}

"""aggregate min on columns"""
type currency_min_fields {
  value: String
}

"""
response of any mutation on the table "currency"
"""
type currency_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [currency!]!
}

"""
on_conflict condition type for table "currency"
"""
input currency_on_conflict {
  constraint: currency_constraint!
  update_columns: [currency_update_column!]! = []
  where: currency_bool_exp
}

"""Ordering options when selecting data from "currency"."""
input currency_order_by {
  value: order_by
}

"""primary key columns input for table: currency"""
input currency_pk_columns_input {
  value: String!
}

"""
select columns of table "currency"
"""
enum currency_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "currency"
"""
input currency_set_input {
  value: String
}

"""
Streaming cursor of the table "currency"
"""
input currency_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: currency_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input currency_stream_cursor_value_input {
  value: String
}

"""
update columns of table "currency"
"""
enum currency_update_column {
  """column name"""
  value
}

input currency_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: currency_set_input

  """filter the rows which have to be updated"""
  where: currency_bool_exp!
}

"""ordering argument of a cursor"""
enum cursor_ordering {
  """ascending ordering of the cursor"""
  ASC

  """descending ordering of the cursor"""
  DESC
}

"""
The eventParameters model is designed to define properties on an event involving all event passes. This table includes critical details like the eventId and activityWebhookId, which aids in monitoring and processing events or changes related to the event parameters. By centralizing this information, our system can effectively manage and control parameters tied to specific events, enhancing the overall functionality and flexibility of event handling.
"""
type eventParameters {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers. This webhook ID is essential for real-time monitoring and processing of NFT transactions related to the event, ensuring that the platform stays updated with the latest transfer activities.
  """
  activityWebhookId: String
  created_at: timestamptz!

  """
  The "dateEnd" column specifies the end date and time of the event. Similar to "dateStart", this timestamp is stored without a timezone, marking the official conclusion of the event. This information is vital for managing the overall duration and scheduling of the event.
  """
  dateEnd: timestamp

  """
  The "dateSaleEnd" column indicates the end date and time for the sale of event passes. By providing a clear cut-off point for sales, this timestamp aids in the strategic planning and closure of the pass sale period.
  """
  dateSaleEnd: timestamp

  """
  The "dateSaleStart" column denotes the start date and time for when the event passes become available for sale. This timestamp, free from timezone specifics, is critical for controlling the sales window, allowing for precise planning and marketing of the event passes.
  """
  dateSaleStart: timestamp

  """
  The "dateStart" column represents the start date and time of the event. This timestamp, set in a timezone-neutral format, indicates when the event officially begins. It is crucial for scheduling and coordinating event-related activities.
  """
  dateStart: timestamp
  event(
    """
    Defines which locales should be returned.
    
    Note that `Event` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: EventWhereUniqueInput_remote_rel_eventParametersevent!
  ): Event
  eventId: String!

  """An array relationship"""
  eventPassNftContracts(
    """distinct select on columns"""
    distinct_on: [eventPassNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContract_order_by!]

    """filter the rows returned"""
    where: eventPassNftContract_bool_exp
  ): [eventPassNftContract!]!

  """An aggregate relationship"""
  eventPassNftContracts_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContract_order_by!]

    """filter the rows returned"""
    where: eventPassNftContract_bool_exp
  ): eventPassNftContract_aggregate!

  """An array relationship"""
  eventPassNfts(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): [eventPassNft!]!

  """An aggregate relationship"""
  eventPassNfts_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): eventPassNft_aggregate!
  id: uuid!

  """
  A computed field, executes function "is_event_ongoing"
  """
  isOngoing: Boolean

  """
  A computed field, executes function "is_sale_ongoing"
  """
  isSaleOngoing: Boolean
  organizer(
    """
    Defines which locales should be returned.
    
    Note that `Organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: OrganizerWhereUniqueInput_remote_rel_eventParametersorganizer!
  ): Organizer
  organizerId: String!
  signingKey: String
  status: eventStatus_enum

  """
  The "timezone" column contains the timezone identifier for the event. All event-related timestamps, such as "dateStart", "dateEnd", "dateSaleStart", and "dateSaleEnd", are interpreted in this specified timezone. This column ensures consistency in timekeeping and scheduling across various geographic locations.
  """
  timezone: String!
  updated_at: timestamptz!
}

"""
aggregated selection of "eventParameters"
"""
type eventParameters_aggregate {
  aggregate: eventParameters_aggregate_fields
  nodes: [eventParameters!]!
}

"""
aggregate fields of "eventParameters"
"""
type eventParameters_aggregate_fields {
  count(columns: [eventParameters_select_column!], distinct: Boolean): Int!
  max: eventParameters_max_fields
  min: eventParameters_min_fields
}

"""
Boolean expression to filter rows from the table "eventParameters". All fields are combined with a logical 'AND'.
"""
input eventParameters_bool_exp {
  _and: [eventParameters_bool_exp!]
  _not: eventParameters_bool_exp
  _or: [eventParameters_bool_exp!]
  activityWebhookId: String_comparison_exp
  created_at: timestamptz_comparison_exp
  dateEnd: timestamp_comparison_exp
  dateSaleEnd: timestamp_comparison_exp
  dateSaleStart: timestamp_comparison_exp
  dateStart: timestamp_comparison_exp
  eventId: String_comparison_exp
  eventPassNftContracts: eventPassNftContract_bool_exp
  eventPassNftContracts_aggregate: eventPassNftContract_aggregate_bool_exp
  eventPassNfts: eventPassNft_bool_exp
  eventPassNfts_aggregate: eventPassNft_aggregate_bool_exp
  id: uuid_comparison_exp
  isOngoing: Boolean_comparison_exp
  isSaleOngoing: Boolean_comparison_exp
  organizerId: String_comparison_exp
  signingKey: String_comparison_exp
  status: eventStatus_enum_comparison_exp
  timezone: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "eventParameters"
"""
enum eventParameters_constraint {
  """
  unique or primary key constraint on columns "eventId"
  """
  eventParameters_eventId_key

  """
  unique or primary key constraint on columns "id"
  """
  eventParameters_pkey

  """
  unique or primary key constraint on columns "signingKey"
  """
  eventParameters_signingKey_key
}

"""
input type for inserting data into table "eventParameters"
"""
input eventParameters_insert_input {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers. This webhook ID is essential for real-time monitoring and processing of NFT transactions related to the event, ensuring that the platform stays updated with the latest transfer activities.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  The "dateEnd" column specifies the end date and time of the event. Similar to "dateStart", this timestamp is stored without a timezone, marking the official conclusion of the event. This information is vital for managing the overall duration and scheduling of the event.
  """
  dateEnd: timestamp

  """
  The "dateSaleEnd" column indicates the end date and time for the sale of event passes. By providing a clear cut-off point for sales, this timestamp aids in the strategic planning and closure of the pass sale period.
  """
  dateSaleEnd: timestamp

  """
  The "dateSaleStart" column denotes the start date and time for when the event passes become available for sale. This timestamp, free from timezone specifics, is critical for controlling the sales window, allowing for precise planning and marketing of the event passes.
  """
  dateSaleStart: timestamp

  """
  The "dateStart" column represents the start date and time of the event. This timestamp, set in a timezone-neutral format, indicates when the event officially begins. It is crucial for scheduling and coordinating event-related activities.
  """
  dateStart: timestamp
  eventId: String
  eventPassNftContracts: eventPassNftContract_arr_rel_insert_input
  eventPassNfts: eventPassNft_arr_rel_insert_input
  id: uuid
  organizerId: String
  signingKey: String
  status: eventStatus_enum

  """
  The "timezone" column contains the timezone identifier for the event. All event-related timestamps, such as "dateStart", "dateEnd", "dateSaleStart", and "dateSaleEnd", are interpreted in this specified timezone. This column ensures consistency in timekeeping and scheduling across various geographic locations.
  """
  timezone: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type eventParameters_max_fields {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers. This webhook ID is essential for real-time monitoring and processing of NFT transactions related to the event, ensuring that the platform stays updated with the latest transfer activities.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  The "dateEnd" column specifies the end date and time of the event. Similar to "dateStart", this timestamp is stored without a timezone, marking the official conclusion of the event. This information is vital for managing the overall duration and scheduling of the event.
  """
  dateEnd: timestamp

  """
  The "dateSaleEnd" column indicates the end date and time for the sale of event passes. By providing a clear cut-off point for sales, this timestamp aids in the strategic planning and closure of the pass sale period.
  """
  dateSaleEnd: timestamp

  """
  The "dateSaleStart" column denotes the start date and time for when the event passes become available for sale. This timestamp, free from timezone specifics, is critical for controlling the sales window, allowing for precise planning and marketing of the event passes.
  """
  dateSaleStart: timestamp

  """
  The "dateStart" column represents the start date and time of the event. This timestamp, set in a timezone-neutral format, indicates when the event officially begins. It is crucial for scheduling and coordinating event-related activities.
  """
  dateStart: timestamp
  eventId: String
  id: uuid
  organizerId: String
  signingKey: String

  """
  The "timezone" column contains the timezone identifier for the event. All event-related timestamps, such as "dateStart", "dateEnd", "dateSaleStart", and "dateSaleEnd", are interpreted in this specified timezone. This column ensures consistency in timekeeping and scheduling across various geographic locations.
  """
  timezone: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type eventParameters_min_fields {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers. This webhook ID is essential for real-time monitoring and processing of NFT transactions related to the event, ensuring that the platform stays updated with the latest transfer activities.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  The "dateEnd" column specifies the end date and time of the event. Similar to "dateStart", this timestamp is stored without a timezone, marking the official conclusion of the event. This information is vital for managing the overall duration and scheduling of the event.
  """
  dateEnd: timestamp

  """
  The "dateSaleEnd" column indicates the end date and time for the sale of event passes. By providing a clear cut-off point for sales, this timestamp aids in the strategic planning and closure of the pass sale period.
  """
  dateSaleEnd: timestamp

  """
  The "dateSaleStart" column denotes the start date and time for when the event passes become available for sale. This timestamp, free from timezone specifics, is critical for controlling the sales window, allowing for precise planning and marketing of the event passes.
  """
  dateSaleStart: timestamp

  """
  The "dateStart" column represents the start date and time of the event. This timestamp, set in a timezone-neutral format, indicates when the event officially begins. It is crucial for scheduling and coordinating event-related activities.
  """
  dateStart: timestamp
  eventId: String
  id: uuid
  organizerId: String
  signingKey: String

  """
  The "timezone" column contains the timezone identifier for the event. All event-related timestamps, such as "dateStart", "dateEnd", "dateSaleStart", and "dateSaleEnd", are interpreted in this specified timezone. This column ensures consistency in timekeeping and scheduling across various geographic locations.
  """
  timezone: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "eventParameters"
"""
type eventParameters_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventParameters!]!
}

"""
input type for inserting object relation for remote table "eventParameters"
"""
input eventParameters_obj_rel_insert_input {
  data: eventParameters_insert_input!

  """upsert condition"""
  on_conflict: eventParameters_on_conflict
}

"""
on_conflict condition type for table "eventParameters"
"""
input eventParameters_on_conflict {
  constraint: eventParameters_constraint!
  update_columns: [eventParameters_update_column!]! = []
  where: eventParameters_bool_exp
}

"""Ordering options when selecting data from "eventParameters"."""
input eventParameters_order_by {
  activityWebhookId: order_by
  created_at: order_by
  dateEnd: order_by
  dateSaleEnd: order_by
  dateSaleStart: order_by
  dateStart: order_by
  eventId: order_by
  eventPassNftContracts_aggregate: eventPassNftContract_aggregate_order_by
  eventPassNfts_aggregate: eventPassNft_aggregate_order_by
  id: order_by
  isOngoing: order_by
  isSaleOngoing: order_by
  organizerId: order_by
  signingKey: order_by
  status: order_by
  timezone: order_by
  updated_at: order_by
}

"""primary key columns input for table: eventParameters"""
input eventParameters_pk_columns_input {
  id: uuid!
}

"""
select columns of table "eventParameters"
"""
enum eventParameters_select_column {
  """column name"""
  activityWebhookId

  """column name"""
  created_at

  """column name"""
  dateEnd

  """column name"""
  dateSaleEnd

  """column name"""
  dateSaleStart

  """column name"""
  dateStart

  """column name"""
  eventId

  """column name"""
  id

  """column name"""
  organizerId

  """column name"""
  signingKey

  """column name"""
  status

  """column name"""
  timezone

  """column name"""
  updated_at
}

"""
input type for updating data in table "eventParameters"
"""
input eventParameters_set_input {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers. This webhook ID is essential for real-time monitoring and processing of NFT transactions related to the event, ensuring that the platform stays updated with the latest transfer activities.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  The "dateEnd" column specifies the end date and time of the event. Similar to "dateStart", this timestamp is stored without a timezone, marking the official conclusion of the event. This information is vital for managing the overall duration and scheduling of the event.
  """
  dateEnd: timestamp

  """
  The "dateSaleEnd" column indicates the end date and time for the sale of event passes. By providing a clear cut-off point for sales, this timestamp aids in the strategic planning and closure of the pass sale period.
  """
  dateSaleEnd: timestamp

  """
  The "dateSaleStart" column denotes the start date and time for when the event passes become available for sale. This timestamp, free from timezone specifics, is critical for controlling the sales window, allowing for precise planning and marketing of the event passes.
  """
  dateSaleStart: timestamp

  """
  The "dateStart" column represents the start date and time of the event. This timestamp, set in a timezone-neutral format, indicates when the event officially begins. It is crucial for scheduling and coordinating event-related activities.
  """
  dateStart: timestamp
  eventId: String
  id: uuid
  organizerId: String
  signingKey: String
  status: eventStatus_enum

  """
  The "timezone" column contains the timezone identifier for the event. All event-related timestamps, such as "dateStart", "dateEnd", "dateSaleStart", and "dateSaleEnd", are interpreted in this specified timezone. This column ensures consistency in timekeeping and scheduling across various geographic locations.
  """
  timezone: String
  updated_at: timestamptz
}

"""
Streaming cursor of the table "eventParameters"
"""
input eventParameters_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventParameters_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventParameters_stream_cursor_value_input {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers. This webhook ID is essential for real-time monitoring and processing of NFT transactions related to the event, ensuring that the platform stays updated with the latest transfer activities.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  The "dateEnd" column specifies the end date and time of the event. Similar to "dateStart", this timestamp is stored without a timezone, marking the official conclusion of the event. This information is vital for managing the overall duration and scheduling of the event.
  """
  dateEnd: timestamp

  """
  The "dateSaleEnd" column indicates the end date and time for the sale of event passes. By providing a clear cut-off point for sales, this timestamp aids in the strategic planning and closure of the pass sale period.
  """
  dateSaleEnd: timestamp

  """
  The "dateSaleStart" column denotes the start date and time for when the event passes become available for sale. This timestamp, free from timezone specifics, is critical for controlling the sales window, allowing for precise planning and marketing of the event passes.
  """
  dateSaleStart: timestamp

  """
  The "dateStart" column represents the start date and time of the event. This timestamp, set in a timezone-neutral format, indicates when the event officially begins. It is crucial for scheduling and coordinating event-related activities.
  """
  dateStart: timestamp
  eventId: String
  id: uuid
  organizerId: String
  signingKey: String
  status: eventStatus_enum

  """
  The "timezone" column contains the timezone identifier for the event. All event-related timestamps, such as "dateStart", "dateEnd", "dateSaleStart", and "dateSaleEnd", are interpreted in this specified timezone. This column ensures consistency in timekeeping and scheduling across various geographic locations.
  """
  timezone: String
  updated_at: timestamptz
}

"""
update columns of table "eventParameters"
"""
enum eventParameters_update_column {
  """column name"""
  activityWebhookId

  """column name"""
  created_at

  """column name"""
  dateEnd

  """column name"""
  dateSaleEnd

  """column name"""
  dateSaleStart

  """column name"""
  dateStart

  """column name"""
  eventId

  """column name"""
  id

  """column name"""
  organizerId

  """column name"""
  signingKey

  """column name"""
  status

  """column name"""
  timezone

  """column name"""
  updated_at
}

input eventParameters_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: eventParameters_set_input

  """filter the rows which have to be updated"""
  where: eventParameters_bool_exp!
}

"""
columns and relationships of "eventPassNft"
"""
type eventPassNft {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: String!

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String!
  created_at: timestamptz!

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: String

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: String
  event(
    """
    Defines which locales should be returned.
    
    Note that `Event` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: EventWhereUniqueInput_remote_rel_eventPassNftevent!
  ): Event

  """A reference to the event associated with the event pass NFT"""
  eventId: String!

  """An object relationship"""
  eventParameters: eventParameters
  eventPass(
    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
  ): EventPass

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: String!

  """An object relationship"""
  eventPassNftContract: eventPassNftContract
  id: uuid!

  """
  Indicates whether the QR code pass for the event pass NFT has been revealed by the owner. This field is essential for tracking and managing the reveal status within the platform.
  """
  isRevealed: Boolean!

  """An object relationship"""
  lastNftTransfer: nftTransfer

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: uuid

  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata(
    """JSON select path"""
    path: String
  ): jsonb!

  """An array relationship"""
  nftTransfers(
    """distinct select on columns"""
    distinct_on: [nftTransfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [nftTransfer_order_by!]

    """filter the rows returned"""
    where: nftTransfer_bool_exp
  ): [nftTransfer!]!

  """An aggregate relationship"""
  nftTransfers_aggregate(
    """distinct select on columns"""
    distinct_on: [nftTransfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [nftTransfer_order_by!]

    """filter the rows returned"""
    where: nftTransfer_bool_exp
  ): nftTransfer_aggregate!
  organizer(
    """
    Defines which locales should be returned.
    
    Note that `Organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: OrganizerWhereUniqueInput_remote_rel_eventPassNftorganizer!
  ): Organizer

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: String!

  """An object relationship"""
  packAmount: passAmount
  packId: String

  """An object relationship"""
  packPricing: passPricing

  """An object relationship"""
  passAmount: passAmount

  """An object relationship"""
  passPricing: passPricing

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint!

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: String
  updated_at: timestamptz!
}

"""
The eventPassNftContract model is designed to store metadata associated with NFT contracts linked to specific event passes. This table captures critical, immutable details from the ERC-721 standard, such as the chainId and contractAddress, ensuring accurate tracking and referencing of NFT contracts. Additionally, this table includes information specific to each event pass, like the eventPassId and organizerId, allowing for precise management and interaction with NFT contracts tied to individual event passes. By centralizing this information, our system can effectively manage, reference, and interact with NFT contracts related to particular event passes.
"""
type eventPassNftContract {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: String!

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: String!
  created_at: timestamptz!

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: String!
  eventPass(
    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
  ): EventPass
  eventPassId: String!

  """An array relationship"""
  eventPassNfts(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): [eventPassNft!]!

  """An aggregate relationship"""
  eventPassNfts_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): eventPassNft_aggregate!

  """An object relationship"""
  eventPassOrderSums: eventPassOrderSums
  id: uuid!

  """Flag indicating whether the event pass NFT is airdropped."""
  isAirdrop: Boolean!

  """
  Flag indicating whether the delayed reveal functionality is active. Can be set to true only if type is delayed_reveal.
  """
  isDelayedRevealed: Boolean!

  """An array relationship"""
  orders(
    """distinct select on columns"""
    distinct_on: [order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [order_order_by!]

    """filter the rows returned"""
    where: order_bool_exp
  ): [order!]!

  """An aggregate relationship"""
  orders_aggregate(
    """distinct select on columns"""
    distinct_on: [order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [order_order_by!]

    """filter the rows returned"""
    where: order_bool_exp
  ): order_aggregate!
  organizerId: String!

  """An object relationship"""
  passAmount: passAmount

  """An object relationship"""
  passPricing: passPricing

  """Type of the pass, referencing the eventPassType table."""
  passType: eventPassType_enum!

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: String

  """Type of the event pass NFT contract."""
  type: eventPassNftContractType_enum!
  updated_at: timestamptz!

  """
  The method of validation for the event pass, referencing the eventPassValidationType table.
  """
  validationType: eventPassValidationType_enum!
}

"""Contract types representing the nature of the event pass NFT contract."""
type eventPassNftContractType {
  """Type name for event pass NFT contract."""
  value: String!
}

"""
aggregated selection of "eventPassNftContractType"
"""
type eventPassNftContractType_aggregate {
  aggregate: eventPassNftContractType_aggregate_fields
  nodes: [eventPassNftContractType!]!
}

"""
aggregate fields of "eventPassNftContractType"
"""
type eventPassNftContractType_aggregate_fields {
  count(columns: [eventPassNftContractType_select_column!], distinct: Boolean): Int!
  max: eventPassNftContractType_max_fields
  min: eventPassNftContractType_min_fields
}

"""
Boolean expression to filter rows from the table "eventPassNftContractType". All fields are combined with a logical 'AND'.
"""
input eventPassNftContractType_bool_exp {
  _and: [eventPassNftContractType_bool_exp!]
  _not: eventPassNftContractType_bool_exp
  _or: [eventPassNftContractType_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "eventPassNftContractType"
"""
enum eventPassNftContractType_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  eventPassNftContractType_pkey
}

enum eventPassNftContractType_enum {
  delayed_reveal
  normal
}

"""
Boolean expression to compare columns of type "eventPassNftContractType_enum". All fields are combined with logical 'AND'.
"""
input eventPassNftContractType_enum_comparison_exp {
  _eq: eventPassNftContractType_enum
  _in: [eventPassNftContractType_enum!]
  _is_null: Boolean
  _neq: eventPassNftContractType_enum
  _nin: [eventPassNftContractType_enum!]
}

"""
input type for inserting data into table "eventPassNftContractType"
"""
input eventPassNftContractType_insert_input {
  """Type name for event pass NFT contract."""
  value: String
}

"""aggregate max on columns"""
type eventPassNftContractType_max_fields {
  """Type name for event pass NFT contract."""
  value: String
}

"""aggregate min on columns"""
type eventPassNftContractType_min_fields {
  """Type name for event pass NFT contract."""
  value: String
}

"""
response of any mutation on the table "eventPassNftContractType"
"""
type eventPassNftContractType_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventPassNftContractType!]!
}

"""
on_conflict condition type for table "eventPassNftContractType"
"""
input eventPassNftContractType_on_conflict {
  constraint: eventPassNftContractType_constraint!
  update_columns: [eventPassNftContractType_update_column!]! = []
  where: eventPassNftContractType_bool_exp
}

"""Ordering options when selecting data from "eventPassNftContractType"."""
input eventPassNftContractType_order_by {
  value: order_by
}

"""primary key columns input for table: eventPassNftContractType"""
input eventPassNftContractType_pk_columns_input {
  """Type name for event pass NFT contract."""
  value: String!
}

"""
select columns of table "eventPassNftContractType"
"""
enum eventPassNftContractType_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "eventPassNftContractType"
"""
input eventPassNftContractType_set_input {
  """Type name for event pass NFT contract."""
  value: String
}

"""
Streaming cursor of the table "eventPassNftContractType"
"""
input eventPassNftContractType_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventPassNftContractType_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventPassNftContractType_stream_cursor_value_input {
  """Type name for event pass NFT contract."""
  value: String
}

"""
update columns of table "eventPassNftContractType"
"""
enum eventPassNftContractType_update_column {
  """column name"""
  value
}

input eventPassNftContractType_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: eventPassNftContractType_set_input

  """filter the rows which have to be updated"""
  where: eventPassNftContractType_bool_exp!
}

"""
aggregated selection of "eventPassNftContract"
"""
type eventPassNftContract_aggregate {
  aggregate: eventPassNftContract_aggregate_fields
  nodes: [eventPassNftContract!]!
}

input eventPassNftContract_aggregate_bool_exp {
  bool_and: eventPassNftContract_aggregate_bool_exp_bool_and
  bool_or: eventPassNftContract_aggregate_bool_exp_bool_or
  count: eventPassNftContract_aggregate_bool_exp_count
}

input eventPassNftContract_aggregate_bool_exp_bool_and {
  arguments: eventPassNftContract_select_column_eventPassNftContract_aggregate_bool_exp_bool_and_arguments_columns!
  distinct: Boolean
  filter: eventPassNftContract_bool_exp
  predicate: Boolean_comparison_exp!
}

input eventPassNftContract_aggregate_bool_exp_bool_or {
  arguments: eventPassNftContract_select_column_eventPassNftContract_aggregate_bool_exp_bool_or_arguments_columns!
  distinct: Boolean
  filter: eventPassNftContract_bool_exp
  predicate: Boolean_comparison_exp!
}

input eventPassNftContract_aggregate_bool_exp_count {
  arguments: [eventPassNftContract_select_column!]
  distinct: Boolean
  filter: eventPassNftContract_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "eventPassNftContract"
"""
type eventPassNftContract_aggregate_fields {
  count(columns: [eventPassNftContract_select_column!], distinct: Boolean): Int!
  max: eventPassNftContract_max_fields
  min: eventPassNftContract_min_fields
}

"""
order by aggregate values of table "eventPassNftContract"
"""
input eventPassNftContract_aggregate_order_by {
  count: order_by
  max: eventPassNftContract_max_order_by
  min: eventPassNftContract_min_order_by
}

"""
input type for inserting array relation for remote table "eventPassNftContract"
"""
input eventPassNftContract_arr_rel_insert_input {
  data: [eventPassNftContract_insert_input!]!

  """upsert condition"""
  on_conflict: eventPassNftContract_on_conflict
}

"""
Boolean expression to filter rows from the table "eventPassNftContract". All fields are combined with a logical 'AND'.
"""
input eventPassNftContract_bool_exp {
  _and: [eventPassNftContract_bool_exp!]
  _not: eventPassNftContract_bool_exp
  _or: [eventPassNftContract_bool_exp!]
  chainId: String_comparison_exp
  contractAddress: String_comparison_exp
  created_at: timestamptz_comparison_exp
  eventId: String_comparison_exp
  eventPassId: String_comparison_exp
  eventPassNfts: eventPassNft_bool_exp
  eventPassNfts_aggregate: eventPassNft_aggregate_bool_exp
  eventPassOrderSums: eventPassOrderSums_bool_exp
  id: uuid_comparison_exp
  isAirdrop: Boolean_comparison_exp
  isDelayedRevealed: Boolean_comparison_exp
  orders: order_bool_exp
  orders_aggregate: order_aggregate_bool_exp
  organizerId: String_comparison_exp
  passAmount: passAmount_bool_exp
  passPricing: passPricing_bool_exp
  passType: eventPassType_enum_comparison_exp
  password: String_comparison_exp
  type: eventPassNftContractType_enum_comparison_exp
  updated_at: timestamptz_comparison_exp
  validationType: eventPassValidationType_enum_comparison_exp
}

"""
unique or primary key constraints on table "eventPassNftContract"
"""
enum eventPassNftContract_constraint {
  """
  unique or primary key constraint on columns "eventPassId"
  """
  eventPassId_unique

  """
  unique or primary key constraint on columns "chainId", "contractAddress"
  """
  eventPassNftContract_contractAddress_chainId_key

  """
  unique or primary key constraint on columns "id"
  """
  eventPassNftContract_pkey
}

"""
input type for inserting data into table "eventPassNftContract"
"""
input eventPassNftContract_insert_input {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: String

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: String
  eventPassId: String
  eventPassNfts: eventPassNft_arr_rel_insert_input
  eventPassOrderSums: eventPassOrderSums_obj_rel_insert_input
  id: uuid

  """Flag indicating whether the event pass NFT is airdropped."""
  isAirdrop: Boolean

  """
  Flag indicating whether the delayed reveal functionality is active. Can be set to true only if type is delayed_reveal.
  """
  isDelayedRevealed: Boolean
  orders: order_arr_rel_insert_input
  organizerId: String
  passAmount: passAmount_obj_rel_insert_input
  passPricing: passPricing_obj_rel_insert_input

  """Type of the pass, referencing the eventPassType table."""
  passType: eventPassType_enum

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: String

  """Type of the event pass NFT contract."""
  type: eventPassNftContractType_enum
  updated_at: timestamptz

  """
  The method of validation for the event pass, referencing the eventPassValidationType table.
  """
  validationType: eventPassValidationType_enum
}

"""aggregate max on columns"""
type eventPassNftContract_max_fields {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: String

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: String
  eventPassId: String
  id: uuid
  organizerId: String

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: String
  updated_at: timestamptz
}

"""
order by max() on columns of table "eventPassNftContract"
"""
input eventPassNftContract_max_order_by {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: order_by

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: order_by
  created_at: order_by

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: order_by
  eventPassId: order_by
  id: order_by
  organizerId: order_by

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: order_by
  updated_at: order_by
}

"""aggregate min on columns"""
type eventPassNftContract_min_fields {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: String

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: String
  eventPassId: String
  id: uuid
  organizerId: String

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: String
  updated_at: timestamptz
}

"""
order by min() on columns of table "eventPassNftContract"
"""
input eventPassNftContract_min_order_by {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: order_by

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: order_by
  created_at: order_by

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: order_by
  eventPassId: order_by
  id: order_by
  organizerId: order_by

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: order_by
  updated_at: order_by
}

"""
response of any mutation on the table "eventPassNftContract"
"""
type eventPassNftContract_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventPassNftContract!]!
}

"""
input type for inserting object relation for remote table "eventPassNftContract"
"""
input eventPassNftContract_obj_rel_insert_input {
  data: eventPassNftContract_insert_input!

  """upsert condition"""
  on_conflict: eventPassNftContract_on_conflict
}

"""
on_conflict condition type for table "eventPassNftContract"
"""
input eventPassNftContract_on_conflict {
  constraint: eventPassNftContract_constraint!
  update_columns: [eventPassNftContract_update_column!]! = []
  where: eventPassNftContract_bool_exp
}

"""Ordering options when selecting data from "eventPassNftContract"."""
input eventPassNftContract_order_by {
  chainId: order_by
  contractAddress: order_by
  created_at: order_by
  eventId: order_by
  eventPassId: order_by
  eventPassNfts_aggregate: eventPassNft_aggregate_order_by
  eventPassOrderSums: eventPassOrderSums_order_by
  id: order_by
  isAirdrop: order_by
  isDelayedRevealed: order_by
  orders_aggregate: order_aggregate_order_by
  organizerId: order_by
  passAmount: passAmount_order_by
  passPricing: passPricing_order_by
  passType: order_by
  password: order_by
  type: order_by
  updated_at: order_by
  validationType: order_by
}

"""primary key columns input for table: eventPassNftContract"""
input eventPassNftContract_pk_columns_input {
  id: uuid!
}

"""
select columns of table "eventPassNftContract"
"""
enum eventPassNftContract_select_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  eventId

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  isAirdrop

  """column name"""
  isDelayedRevealed

  """column name"""
  organizerId

  """column name"""
  passType

  """column name"""
  password

  """column name"""
  type

  """column name"""
  updated_at

  """column name"""
  validationType
}

"""
select "eventPassNftContract_aggregate_bool_exp_bool_and_arguments_columns" columns of table "eventPassNftContract"
"""
enum eventPassNftContract_select_column_eventPassNftContract_aggregate_bool_exp_bool_and_arguments_columns {
  """column name"""
  isAirdrop

  """column name"""
  isDelayedRevealed
}

"""
select "eventPassNftContract_aggregate_bool_exp_bool_or_arguments_columns" columns of table "eventPassNftContract"
"""
enum eventPassNftContract_select_column_eventPassNftContract_aggregate_bool_exp_bool_or_arguments_columns {
  """column name"""
  isAirdrop

  """column name"""
  isDelayedRevealed
}

"""
input type for updating data in table "eventPassNftContract"
"""
input eventPassNftContract_set_input {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: String

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: String
  eventPassId: String
  id: uuid

  """Flag indicating whether the event pass NFT is airdropped."""
  isAirdrop: Boolean

  """
  Flag indicating whether the delayed reveal functionality is active. Can be set to true only if type is delayed_reveal.
  """
  isDelayedRevealed: Boolean
  organizerId: String

  """Type of the pass, referencing the eventPassType table."""
  passType: eventPassType_enum

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: String

  """Type of the event pass NFT contract."""
  type: eventPassNftContractType_enum
  updated_at: timestamptz

  """
  The method of validation for the event pass, referencing the eventPassValidationType table.
  """
  validationType: eventPassValidationType_enum
}

"""
Streaming cursor of the table "eventPassNftContract"
"""
input eventPassNftContract_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventPassNftContract_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventPassNftContract_stream_cursor_value_input {
  """
  Specifies the particular blockchain or network on which the NFT collection resides. Essential for distinguishing between different blockchains in a multi-chain environment.
  """
  chainId: String

  """
  Represents the unique address of the smart contract that governs the NFT collection. It acts as the primary reference to the NFTs existence and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  A unique identifier for the event associated with the NFT collection. This ties each collection directly to a specific event within the platform.
  """
  eventId: String
  eventPassId: String
  id: uuid

  """Flag indicating whether the event pass NFT is airdropped."""
  isAirdrop: Boolean

  """
  Flag indicating whether the delayed reveal functionality is active. Can be set to true only if type is delayed_reveal.
  """
  isDelayedRevealed: Boolean
  organizerId: String

  """Type of the pass, referencing the eventPassType table."""
  passType: eventPassType_enum

  """
  Password for the delayed reveal functionality. Nullable and only applicable for delayed_reveal type.
  """
  password: String

  """Type of the event pass NFT contract."""
  type: eventPassNftContractType_enum
  updated_at: timestamptz

  """
  The method of validation for the event pass, referencing the eventPassValidationType table.
  """
  validationType: eventPassValidationType_enum
}

"""
update columns of table "eventPassNftContract"
"""
enum eventPassNftContract_update_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  eventId

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  isAirdrop

  """column name"""
  isDelayedRevealed

  """column name"""
  organizerId

  """column name"""
  passType

  """column name"""
  password

  """column name"""
  type

  """column name"""
  updated_at

  """column name"""
  validationType
}

input eventPassNftContract_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: eventPassNftContract_set_input

  """filter the rows which have to be updated"""
  where: eventPassNftContract_bool_exp!
}

"""
aggregated selection of "eventPassNft"
"""
type eventPassNft_aggregate {
  aggregate: eventPassNft_aggregate_fields
  nodes: [eventPassNft!]!
}

input eventPassNft_aggregate_bool_exp {
  bool_and: eventPassNft_aggregate_bool_exp_bool_and
  bool_or: eventPassNft_aggregate_bool_exp_bool_or
  count: eventPassNft_aggregate_bool_exp_count
}

input eventPassNft_aggregate_bool_exp_bool_and {
  arguments: eventPassNft_select_column_eventPassNft_aggregate_bool_exp_bool_and_arguments_columns!
  distinct: Boolean
  filter: eventPassNft_bool_exp
  predicate: Boolean_comparison_exp!
}

input eventPassNft_aggregate_bool_exp_bool_or {
  arguments: eventPassNft_select_column_eventPassNft_aggregate_bool_exp_bool_or_arguments_columns!
  distinct: Boolean
  filter: eventPassNft_bool_exp
  predicate: Boolean_comparison_exp!
}

input eventPassNft_aggregate_bool_exp_count {
  arguments: [eventPassNft_select_column!]
  distinct: Boolean
  filter: eventPassNft_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "eventPassNft"
"""
type eventPassNft_aggregate_fields {
  avg: eventPassNft_avg_fields
  count(columns: [eventPassNft_select_column!], distinct: Boolean): Int!
  max: eventPassNft_max_fields
  min: eventPassNft_min_fields
  stddev: eventPassNft_stddev_fields
  stddev_pop: eventPassNft_stddev_pop_fields
  stddev_samp: eventPassNft_stddev_samp_fields
  sum: eventPassNft_sum_fields
  var_pop: eventPassNft_var_pop_fields
  var_samp: eventPassNft_var_samp_fields
  variance: eventPassNft_variance_fields
}

"""
order by aggregate values of table "eventPassNft"
"""
input eventPassNft_aggregate_order_by {
  avg: eventPassNft_avg_order_by
  count: order_by
  max: eventPassNft_max_order_by
  min: eventPassNft_min_order_by
  stddev: eventPassNft_stddev_order_by
  stddev_pop: eventPassNft_stddev_pop_order_by
  stddev_samp: eventPassNft_stddev_samp_order_by
  sum: eventPassNft_sum_order_by
  var_pop: eventPassNft_var_pop_order_by
  var_samp: eventPassNft_var_samp_order_by
  variance: eventPassNft_variance_order_by
}

"""append existing jsonb value of filtered columns with new jsonb value"""
input eventPassNft_append_input {
  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: jsonb
}

"""
input type for inserting array relation for remote table "eventPassNft"
"""
input eventPassNft_arr_rel_insert_input {
  data: [eventPassNft_insert_input!]!

  """upsert condition"""
  on_conflict: eventPassNft_on_conflict
}

"""aggregate avg on columns"""
type eventPassNft_avg_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: Float
}

"""
order by avg() on columns of table "eventPassNft"
"""
input eventPassNft_avg_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""
Boolean expression to filter rows from the table "eventPassNft". All fields are combined with a logical 'AND'.
"""
input eventPassNft_bool_exp {
  _and: [eventPassNft_bool_exp!]
  _not: eventPassNft_bool_exp
  _or: [eventPassNft_bool_exp!]
  chainId: String_comparison_exp
  contractAddress: String_comparison_exp
  created_at: timestamptz_comparison_exp
  currentOwnerAddress: String_comparison_exp
  error: String_comparison_exp
  eventId: String_comparison_exp
  eventParameters: eventParameters_bool_exp
  eventPassId: String_comparison_exp
  eventPassNftContract: eventPassNftContract_bool_exp
  id: uuid_comparison_exp
  isRevealed: Boolean_comparison_exp
  lastNftTransfer: nftTransfer_bool_exp
  lastNftTransferId: uuid_comparison_exp
  metadata: jsonb_comparison_exp
  nftTransfers: nftTransfer_bool_exp
  nftTransfers_aggregate: nftTransfer_aggregate_bool_exp
  organizerId: String_comparison_exp
  packAmount: passAmount_bool_exp
  packId: String_comparison_exp
  packPricing: passPricing_bool_exp
  passAmount: passAmount_bool_exp
  passPricing: passPricing_bool_exp
  tokenId: bigint_comparison_exp
  tokenUri: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "eventPassNft"
"""
enum eventPassNft_constraint {
  """
  unique or primary key constraint on columns "chainId", "contractAddress", "tokenId"
  """
  eventPassNft_contractAddress_tokenId_chainId_key

  """
  unique or primary key constraint on columns "id"
  """
  eventPassNft_pkey
}

"""
delete the field or element with specified path (for JSON arrays, negative integers count from the end)
"""
input eventPassNft_delete_at_path_input {
  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: [String!]
}

"""
delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
"""
input eventPassNft_delete_elem_input {
  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: Int
}

"""
delete key/value pair or string element. key/value pairs are matched based on their key value
"""
input eventPassNft_delete_key_input {
  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: String
}

"""
input type for incrementing numeric columns in table "eventPassNft"
"""
input eventPassNft_inc_input {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint
}

"""
input type for inserting data into table "eventPassNft"
"""
input eventPassNft_insert_input {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: String

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: String

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: String

  """A reference to the event associated with the event pass NFT"""
  eventId: String
  eventParameters: eventParameters_obj_rel_insert_input

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: String
  eventPassNftContract: eventPassNftContract_obj_rel_insert_input
  id: uuid

  """
  Indicates whether the QR code pass for the event pass NFT has been revealed by the owner. This field is essential for tracking and managing the reveal status within the platform.
  """
  isRevealed: Boolean
  lastNftTransfer: nftTransfer_obj_rel_insert_input

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: uuid

  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: jsonb
  nftTransfers: nftTransfer_arr_rel_insert_input

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: String
  packAmount: passAmount_obj_rel_insert_input
  packId: String
  packPricing: passPricing_obj_rel_insert_input
  passAmount: passAmount_obj_rel_insert_input
  passPricing: passPricing_obj_rel_insert_input

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type eventPassNft_max_fields {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: String

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: String

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: String

  """A reference to the event associated with the event pass NFT"""
  eventId: String

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: String
  id: uuid

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: uuid

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: String
  packId: String

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: String
  updated_at: timestamptz
}

"""
order by max() on columns of table "eventPassNft"
"""
input eventPassNft_max_order_by {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: order_by

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: order_by
  created_at: order_by

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: order_by

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: order_by

  """A reference to the event associated with the event pass NFT"""
  eventId: order_by

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: order_by
  id: order_by

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: order_by

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: order_by
  packId: order_by

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: order_by
  updated_at: order_by
}

"""aggregate min on columns"""
type eventPassNft_min_fields {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: String

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: String

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: String

  """A reference to the event associated with the event pass NFT"""
  eventId: String

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: String
  id: uuid

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: uuid

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: String
  packId: String

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: String
  updated_at: timestamptz
}

"""
order by min() on columns of table "eventPassNft"
"""
input eventPassNft_min_order_by {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: order_by

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: order_by
  created_at: order_by

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: order_by

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: order_by

  """A reference to the event associated with the event pass NFT"""
  eventId: order_by

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: order_by
  id: order_by

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: order_by

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: order_by
  packId: order_by

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: order_by
  updated_at: order_by
}

"""
response of any mutation on the table "eventPassNft"
"""
type eventPassNft_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventPassNft!]!
}

"""
on_conflict condition type for table "eventPassNft"
"""
input eventPassNft_on_conflict {
  constraint: eventPassNft_constraint!
  update_columns: [eventPassNft_update_column!]! = []
  where: eventPassNft_bool_exp
}

"""Ordering options when selecting data from "eventPassNft"."""
input eventPassNft_order_by {
  chainId: order_by
  contractAddress: order_by
  created_at: order_by
  currentOwnerAddress: order_by
  error: order_by
  eventId: order_by
  eventParameters: eventParameters_order_by
  eventPassId: order_by
  eventPassNftContract: eventPassNftContract_order_by
  id: order_by
  isRevealed: order_by
  lastNftTransfer: nftTransfer_order_by
  lastNftTransferId: order_by
  metadata: order_by
  nftTransfers_aggregate: nftTransfer_aggregate_order_by
  organizerId: order_by
  packAmount: passAmount_order_by
  packId: order_by
  packPricing: passPricing_order_by
  passAmount: passAmount_order_by
  passPricing: passPricing_order_by
  tokenId: order_by
  tokenUri: order_by
  updated_at: order_by
}

"""primary key columns input for table: eventPassNft"""
input eventPassNft_pk_columns_input {
  id: uuid!
}

"""prepend existing jsonb value of filtered columns with new jsonb value"""
input eventPassNft_prepend_input {
  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: jsonb
}

"""
select columns of table "eventPassNft"
"""
enum eventPassNft_select_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  currentOwnerAddress

  """column name"""
  error

  """column name"""
  eventId

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  isRevealed

  """column name"""
  lastNftTransferId

  """column name"""
  metadata

  """column name"""
  organizerId

  """column name"""
  packId

  """column name"""
  tokenId

  """column name"""
  tokenUri

  """column name"""
  updated_at
}

"""
select "eventPassNft_aggregate_bool_exp_bool_and_arguments_columns" columns of table "eventPassNft"
"""
enum eventPassNft_select_column_eventPassNft_aggregate_bool_exp_bool_and_arguments_columns {
  """column name"""
  isRevealed
}

"""
select "eventPassNft_aggregate_bool_exp_bool_or_arguments_columns" columns of table "eventPassNft"
"""
enum eventPassNft_select_column_eventPassNft_aggregate_bool_exp_bool_or_arguments_columns {
  """column name"""
  isRevealed
}

"""
input type for updating data in table "eventPassNft"
"""
input eventPassNft_set_input {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: String

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: String

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: String

  """A reference to the event associated with the event pass NFT"""
  eventId: String

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: String
  id: uuid

  """
  Indicates whether the QR code pass for the event pass NFT has been revealed by the owner. This field is essential for tracking and managing the reveal status within the platform.
  """
  isRevealed: Boolean

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: uuid

  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: jsonb

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: String
  packId: String

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: String
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type eventPassNft_stddev_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: Float
}

"""
order by stddev() on columns of table "eventPassNft"
"""
input eventPassNft_stddev_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""aggregate stddev_pop on columns"""
type eventPassNft_stddev_pop_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: Float
}

"""
order by stddev_pop() on columns of table "eventPassNft"
"""
input eventPassNft_stddev_pop_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""aggregate stddev_samp on columns"""
type eventPassNft_stddev_samp_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: Float
}

"""
order by stddev_samp() on columns of table "eventPassNft"
"""
input eventPassNft_stddev_samp_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""
Streaming cursor of the table "eventPassNft"
"""
input eventPassNft_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventPassNft_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventPassNft_stream_cursor_value_input {
  """Denotes the specific blockchain or network of the event pass NFT"""
  chainId: String

  """
  Identifies the smart contract associated with the event pass NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  The address currently holding the event pass NFT, allowing tracking of ownership
  """
  currentOwnerAddress: String

  """
  Contains any error message related to metadata retrieval, ensuring transparency in the data extraction process.
  """
  error: String

  """A reference to the event associated with the event pass NFT"""
  eventId: String

  """Directly relates to a specific Event Pass within the system"""
  eventPassId: String
  id: uuid

  """
  Indicates whether the QR code pass for the event pass NFT has been revealed by the owner. This field is essential for tracking and managing the reveal status within the platform.
  """
  isRevealed: Boolean

  """
  Reference `id` to the latest `nftTransfer` entry, detailing the most recent transaction for this event pass NFT.
  """
  lastNftTransferId: uuid

  """
  The structured metadata parsed from the token URI. This contains a variety of details regarding the event pass NFT.
  """
  metadata: jsonb

  """Ties the event pass NFT to a specific organizer within the platform"""
  organizerId: String
  packId: String

  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint

  """
  The designated URI for the event pass NFTs metadata blob, providing a stable reference for data extraction.
  """
  tokenUri: String
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type eventPassNft_sum_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: bigint
}

"""
order by sum() on columns of table "eventPassNft"
"""
input eventPassNft_sum_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""
update columns of table "eventPassNft"
"""
enum eventPassNft_update_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  currentOwnerAddress

  """column name"""
  error

  """column name"""
  eventId

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  isRevealed

  """column name"""
  lastNftTransferId

  """column name"""
  metadata

  """column name"""
  organizerId

  """column name"""
  packId

  """column name"""
  tokenId

  """column name"""
  tokenUri

  """column name"""
  updated_at
}

input eventPassNft_updates {
  """append existing jsonb value of filtered columns with new jsonb value"""
  _append: eventPassNft_append_input

  """
  delete the field or element with specified path (for JSON arrays, negative integers count from the end)
  """
  _delete_at_path: eventPassNft_delete_at_path_input

  """
  delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
  """
  _delete_elem: eventPassNft_delete_elem_input

  """
  delete key/value pair or string element. key/value pairs are matched based on their key value
  """
  _delete_key: eventPassNft_delete_key_input

  """increments the numeric columns with given value of the filtered values"""
  _inc: eventPassNft_inc_input

  """prepend existing jsonb value of filtered columns with new jsonb value"""
  _prepend: eventPassNft_prepend_input

  """sets the columns of the filtered rows to the given values"""
  _set: eventPassNft_set_input

  """filter the rows which have to be updated"""
  where: eventPassNft_bool_exp!
}

"""aggregate var_pop on columns"""
type eventPassNft_var_pop_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: Float
}

"""
order by var_pop() on columns of table "eventPassNft"
"""
input eventPassNft_var_pop_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""aggregate var_samp on columns"""
type eventPassNft_var_samp_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: Float
}

"""
order by var_samp() on columns of table "eventPassNft"
"""
input eventPassNft_var_samp_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""aggregate variance on columns"""
type eventPassNft_variance_fields {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: Float
}

"""
order by variance() on columns of table "eventPassNft"
"""
input eventPassNft_variance_order_by {
  """
  The unique identifier of the event pass NFT within its specific collection or contract. This remains constant across various platforms.
  """
  tokenId: order_by
}

"""Hold the sums for the Event Pass Orders"""
type eventPassOrderSums {
  eventPassId: String!
  totalReserved: Int!
}

"""
aggregated selection of "eventPassOrderSums"
"""
type eventPassOrderSums_aggregate {
  aggregate: eventPassOrderSums_aggregate_fields
  nodes: [eventPassOrderSums!]!
}

"""
aggregate fields of "eventPassOrderSums"
"""
type eventPassOrderSums_aggregate_fields {
  avg: eventPassOrderSums_avg_fields
  count(columns: [eventPassOrderSums_select_column!], distinct: Boolean): Int!
  max: eventPassOrderSums_max_fields
  min: eventPassOrderSums_min_fields
  stddev: eventPassOrderSums_stddev_fields
  stddev_pop: eventPassOrderSums_stddev_pop_fields
  stddev_samp: eventPassOrderSums_stddev_samp_fields
  sum: eventPassOrderSums_sum_fields
  var_pop: eventPassOrderSums_var_pop_fields
  var_samp: eventPassOrderSums_var_samp_fields
  variance: eventPassOrderSums_variance_fields
}

"""aggregate avg on columns"""
type eventPassOrderSums_avg_fields {
  totalReserved: Float
}

"""
Boolean expression to filter rows from the table "eventPassOrderSums". All fields are combined with a logical 'AND'.
"""
input eventPassOrderSums_bool_exp {
  _and: [eventPassOrderSums_bool_exp!]
  _not: eventPassOrderSums_bool_exp
  _or: [eventPassOrderSums_bool_exp!]
  eventPassId: String_comparison_exp
  totalReserved: Int_comparison_exp
}

"""
unique or primary key constraints on table "eventPassOrderSums"
"""
enum eventPassOrderSums_constraint {
  """
  unique or primary key constraint on columns "eventPassId"
  """
  eventPassOrderSums_pkey
}

"""
input type for incrementing numeric columns in table "eventPassOrderSums"
"""
input eventPassOrderSums_inc_input {
  totalReserved: Int
}

"""
input type for inserting data into table "eventPassOrderSums"
"""
input eventPassOrderSums_insert_input {
  eventPassId: String
  totalReserved: Int
}

"""aggregate max on columns"""
type eventPassOrderSums_max_fields {
  eventPassId: String
  totalReserved: Int
}

"""aggregate min on columns"""
type eventPassOrderSums_min_fields {
  eventPassId: String
  totalReserved: Int
}

"""
response of any mutation on the table "eventPassOrderSums"
"""
type eventPassOrderSums_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventPassOrderSums!]!
}

"""
input type for inserting object relation for remote table "eventPassOrderSums"
"""
input eventPassOrderSums_obj_rel_insert_input {
  data: eventPassOrderSums_insert_input!

  """upsert condition"""
  on_conflict: eventPassOrderSums_on_conflict
}

"""
on_conflict condition type for table "eventPassOrderSums"
"""
input eventPassOrderSums_on_conflict {
  constraint: eventPassOrderSums_constraint!
  update_columns: [eventPassOrderSums_update_column!]! = []
  where: eventPassOrderSums_bool_exp
}

"""Ordering options when selecting data from "eventPassOrderSums"."""
input eventPassOrderSums_order_by {
  eventPassId: order_by
  totalReserved: order_by
}

"""primary key columns input for table: eventPassOrderSums"""
input eventPassOrderSums_pk_columns_input {
  eventPassId: String!
}

"""
select columns of table "eventPassOrderSums"
"""
enum eventPassOrderSums_select_column {
  """column name"""
  eventPassId

  """column name"""
  totalReserved
}

"""
input type for updating data in table "eventPassOrderSums"
"""
input eventPassOrderSums_set_input {
  eventPassId: String
  totalReserved: Int
}

"""aggregate stddev on columns"""
type eventPassOrderSums_stddev_fields {
  totalReserved: Float
}

"""aggregate stddev_pop on columns"""
type eventPassOrderSums_stddev_pop_fields {
  totalReserved: Float
}

"""aggregate stddev_samp on columns"""
type eventPassOrderSums_stddev_samp_fields {
  totalReserved: Float
}

"""
Streaming cursor of the table "eventPassOrderSums"
"""
input eventPassOrderSums_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventPassOrderSums_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventPassOrderSums_stream_cursor_value_input {
  eventPassId: String
  totalReserved: Int
}

"""aggregate sum on columns"""
type eventPassOrderSums_sum_fields {
  totalReserved: Int
}

"""
update columns of table "eventPassOrderSums"
"""
enum eventPassOrderSums_update_column {
  """column name"""
  eventPassId

  """column name"""
  totalReserved
}

input eventPassOrderSums_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: eventPassOrderSums_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: eventPassOrderSums_set_input

  """filter the rows which have to be updated"""
  where: eventPassOrderSums_bool_exp!
}

"""aggregate var_pop on columns"""
type eventPassOrderSums_var_pop_fields {
  totalReserved: Float
}

"""aggregate var_samp on columns"""
type eventPassOrderSums_var_samp_fields {
  totalReserved: Float
}

"""aggregate variance on columns"""
type eventPassOrderSums_variance_fields {
  totalReserved: Float
}

"""Defines the types of event passes."""
type eventPassType {
  """Type name for event pass."""
  value: String!
}

"""
aggregated selection of "eventPassType"
"""
type eventPassType_aggregate {
  aggregate: eventPassType_aggregate_fields
  nodes: [eventPassType!]!
}

"""
aggregate fields of "eventPassType"
"""
type eventPassType_aggregate_fields {
  count(columns: [eventPassType_select_column!], distinct: Boolean): Int!
  max: eventPassType_max_fields
  min: eventPassType_min_fields
}

"""
Boolean expression to filter rows from the table "eventPassType". All fields are combined with a logical 'AND'.
"""
input eventPassType_bool_exp {
  _and: [eventPassType_bool_exp!]
  _not: eventPassType_bool_exp
  _or: [eventPassType_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "eventPassType"
"""
enum eventPassType_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  eventPassType_pkey
}

enum eventPassType_enum {
  event_access
  redeemable
}

"""
Boolean expression to compare columns of type "eventPassType_enum". All fields are combined with logical 'AND'.
"""
input eventPassType_enum_comparison_exp {
  _eq: eventPassType_enum
  _in: [eventPassType_enum!]
  _is_null: Boolean
  _neq: eventPassType_enum
  _nin: [eventPassType_enum!]
}

"""
input type for inserting data into table "eventPassType"
"""
input eventPassType_insert_input {
  """Type name for event pass."""
  value: String
}

"""aggregate max on columns"""
type eventPassType_max_fields {
  """Type name for event pass."""
  value: String
}

"""aggregate min on columns"""
type eventPassType_min_fields {
  """Type name for event pass."""
  value: String
}

"""
response of any mutation on the table "eventPassType"
"""
type eventPassType_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventPassType!]!
}

"""
on_conflict condition type for table "eventPassType"
"""
input eventPassType_on_conflict {
  constraint: eventPassType_constraint!
  update_columns: [eventPassType_update_column!]! = []
  where: eventPassType_bool_exp
}

"""Ordering options when selecting data from "eventPassType"."""
input eventPassType_order_by {
  value: order_by
}

"""primary key columns input for table: eventPassType"""
input eventPassType_pk_columns_input {
  """Type name for event pass."""
  value: String!
}

"""
select columns of table "eventPassType"
"""
enum eventPassType_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "eventPassType"
"""
input eventPassType_set_input {
  """Type name for event pass."""
  value: String
}

"""
Streaming cursor of the table "eventPassType"
"""
input eventPassType_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventPassType_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventPassType_stream_cursor_value_input {
  """Type name for event pass."""
  value: String
}

"""
update columns of table "eventPassType"
"""
enum eventPassType_update_column {
  """column name"""
  value
}

input eventPassType_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: eventPassType_set_input

  """filter the rows which have to be updated"""
  where: eventPassType_bool_exp!
}

"""Defines the types of validation for event passes."""
type eventPassValidationType {
  """Type name for event pass validation."""
  value: String!
}

"""
aggregated selection of "eventPassValidationType"
"""
type eventPassValidationType_aggregate {
  aggregate: eventPassValidationType_aggregate_fields
  nodes: [eventPassValidationType!]!
}

"""
aggregate fields of "eventPassValidationType"
"""
type eventPassValidationType_aggregate_fields {
  count(columns: [eventPassValidationType_select_column!], distinct: Boolean): Int!
  max: eventPassValidationType_max_fields
  min: eventPassValidationType_min_fields
}

"""
Boolean expression to filter rows from the table "eventPassValidationType". All fields are combined with a logical 'AND'.
"""
input eventPassValidationType_bool_exp {
  _and: [eventPassValidationType_bool_exp!]
  _not: eventPassValidationType_bool_exp
  _or: [eventPassValidationType_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "eventPassValidationType"
"""
enum eventPassValidationType_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  eventPassValidationType_pkey
}

enum eventPassValidationType_enum {
  external
  manual
  nft
}

"""
Boolean expression to compare columns of type "eventPassValidationType_enum". All fields are combined with logical 'AND'.
"""
input eventPassValidationType_enum_comparison_exp {
  _eq: eventPassValidationType_enum
  _in: [eventPassValidationType_enum!]
  _is_null: Boolean
  _neq: eventPassValidationType_enum
  _nin: [eventPassValidationType_enum!]
}

"""
input type for inserting data into table "eventPassValidationType"
"""
input eventPassValidationType_insert_input {
  """Type name for event pass validation."""
  value: String
}

"""aggregate max on columns"""
type eventPassValidationType_max_fields {
  """Type name for event pass validation."""
  value: String
}

"""aggregate min on columns"""
type eventPassValidationType_min_fields {
  """Type name for event pass validation."""
  value: String
}

"""
response of any mutation on the table "eventPassValidationType"
"""
type eventPassValidationType_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventPassValidationType!]!
}

"""
on_conflict condition type for table "eventPassValidationType"
"""
input eventPassValidationType_on_conflict {
  constraint: eventPassValidationType_constraint!
  update_columns: [eventPassValidationType_update_column!]! = []
  where: eventPassValidationType_bool_exp
}

"""Ordering options when selecting data from "eventPassValidationType"."""
input eventPassValidationType_order_by {
  value: order_by
}

"""primary key columns input for table: eventPassValidationType"""
input eventPassValidationType_pk_columns_input {
  """Type name for event pass validation."""
  value: String!
}

"""
select columns of table "eventPassValidationType"
"""
enum eventPassValidationType_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "eventPassValidationType"
"""
input eventPassValidationType_set_input {
  """Type name for event pass validation."""
  value: String
}

"""
Streaming cursor of the table "eventPassValidationType"
"""
input eventPassValidationType_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventPassValidationType_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventPassValidationType_stream_cursor_value_input {
  """Type name for event pass validation."""
  value: String
}

"""
update columns of table "eventPassValidationType"
"""
enum eventPassValidationType_update_column {
  """column name"""
  value
}

input eventPassValidationType_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: eventPassValidationType_set_input

  """filter the rows which have to be updated"""
  where: eventPassValidationType_bool_exp!
}

"""
columns and relationships of "eventStatus"
"""
type eventStatus {
  value: String!
}

"""
aggregated selection of "eventStatus"
"""
type eventStatus_aggregate {
  aggregate: eventStatus_aggregate_fields
  nodes: [eventStatus!]!
}

"""
aggregate fields of "eventStatus"
"""
type eventStatus_aggregate_fields {
  count(columns: [eventStatus_select_column!], distinct: Boolean): Int!
  max: eventStatus_max_fields
  min: eventStatus_min_fields
}

"""
Boolean expression to filter rows from the table "eventStatus". All fields are combined with a logical 'AND'.
"""
input eventStatus_bool_exp {
  _and: [eventStatus_bool_exp!]
  _not: eventStatus_bool_exp
  _or: [eventStatus_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "eventStatus"
"""
enum eventStatus_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  eventStatus_pkey
}

enum eventStatus_enum {
  DRAFT
  PUBLISHED
}

"""
Boolean expression to compare columns of type "eventStatus_enum". All fields are combined with logical 'AND'.
"""
input eventStatus_enum_comparison_exp {
  _eq: eventStatus_enum
  _in: [eventStatus_enum!]
  _is_null: Boolean
  _neq: eventStatus_enum
  _nin: [eventStatus_enum!]
}

"""
input type for inserting data into table "eventStatus"
"""
input eventStatus_insert_input {
  value: String
}

"""aggregate max on columns"""
type eventStatus_max_fields {
  value: String
}

"""aggregate min on columns"""
type eventStatus_min_fields {
  value: String
}

"""
response of any mutation on the table "eventStatus"
"""
type eventStatus_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [eventStatus!]!
}

"""
on_conflict condition type for table "eventStatus"
"""
input eventStatus_on_conflict {
  constraint: eventStatus_constraint!
  update_columns: [eventStatus_update_column!]! = []
  where: eventStatus_bool_exp
}

"""Ordering options when selecting data from "eventStatus"."""
input eventStatus_order_by {
  value: order_by
}

"""primary key columns input for table: eventStatus"""
input eventStatus_pk_columns_input {
  value: String!
}

"""
select columns of table "eventStatus"
"""
enum eventStatus_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "eventStatus"
"""
input eventStatus_set_input {
  value: String
}

"""
Streaming cursor of the table "eventStatus"
"""
input eventStatus_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: eventStatus_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input eventStatus_stream_cursor_value_input {
  value: String
}

"""
update columns of table "eventStatus"
"""
enum eventStatus_update_column {
  """column name"""
  value
}

input eventStatus_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: eventStatus_set_input

  """filter the rows which have to be updated"""
  where: eventStatus_bool_exp!
}

"""
Stores follow relationships. Each row represents an account following an organizer.
"""
type follow {
  """
  References the unique identifier of the account that is following an organizer.
  """
  accountId: uuid!
  created_at: timestamptz!

  """
  Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
  """
  organizerSlug: String!
}

"""
aggregated selection of "follow"
"""
type follow_aggregate {
  aggregate: follow_aggregate_fields
  nodes: [follow!]!
}

"""
aggregate fields of "follow"
"""
type follow_aggregate_fields {
  count(columns: [follow_select_column!], distinct: Boolean): Int!
  max: follow_max_fields
  min: follow_min_fields
}

"""
Boolean expression to filter rows from the table "follow". All fields are combined with a logical 'AND'.
"""
input follow_bool_exp {
  _and: [follow_bool_exp!]
  _not: follow_bool_exp
  _or: [follow_bool_exp!]
  accountId: uuid_comparison_exp
  created_at: timestamptz_comparison_exp
  organizerSlug: String_comparison_exp
}

"""
unique or primary key constraints on table "follow"
"""
enum follow_constraint {
  """
  unique or primary key constraint on columns "accountId", "organizerSlug"
  """
  follow_pkey
}

"""
input type for inserting data into table "follow"
"""
input follow_insert_input {
  """
  References the unique identifier of the account that is following an organizer.
  """
  accountId: uuid
  created_at: timestamptz

  """
  Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
  """
  organizerSlug: String
}

"""aggregate max on columns"""
type follow_max_fields {
  """
  References the unique identifier of the account that is following an organizer.
  """
  accountId: uuid
  created_at: timestamptz

  """
  Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
  """
  organizerSlug: String
}

"""aggregate min on columns"""
type follow_min_fields {
  """
  References the unique identifier of the account that is following an organizer.
  """
  accountId: uuid
  created_at: timestamptz

  """
  Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
  """
  organizerSlug: String
}

"""
response of any mutation on the table "follow"
"""
type follow_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [follow!]!
}

"""
on_conflict condition type for table "follow"
"""
input follow_on_conflict {
  constraint: follow_constraint!
  update_columns: [follow_update_column!]! = []
  where: follow_bool_exp
}

"""Ordering options when selecting data from "follow"."""
input follow_order_by {
  accountId: order_by
  created_at: order_by
  organizerSlug: order_by
}

"""primary key columns input for table: follow"""
input follow_pk_columns_input {
  """
  References the unique identifier of the account that is following an organizer.
  """
  accountId: uuid!

  """
  Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
  """
  organizerSlug: String!
}

"""
select columns of table "follow"
"""
enum follow_select_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  organizerSlug
}

"""
input type for updating data in table "follow"
"""
input follow_set_input {
  """
  References the unique identifier of the account that is following an organizer.
  """
  accountId: uuid
  created_at: timestamptz

  """
  Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
  """
  organizerSlug: String
}

"""
Streaming cursor of the table "follow"
"""
input follow_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: follow_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input follow_stream_cursor_value_input {
  """
  References the unique identifier of the account that is following an organizer.
  """
  accountId: uuid
  created_at: timestamptz

  """
  Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
  """
  organizerSlug: String
}

"""
update columns of table "follow"
"""
enum follow_update_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  organizerSlug
}

input follow_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: follow_set_input

  """filter the rows which have to be updated"""
  where: follow_bool_exp!
}

scalar jsonb

input jsonb_cast_exp {
  String: String_comparison_exp
}

"""
Boolean expression to compare columns of type "jsonb". All fields are combined with logical 'AND'.
"""
input jsonb_comparison_exp {
  _cast: jsonb_cast_exp

  """is the column contained in the given json value"""
  _contained_in: jsonb

  """does the column contain the given json value at the top level"""
  _contains: jsonb
  _eq: jsonb
  _gt: jsonb
  _gte: jsonb

  """does the string exist as a top-level key in the column"""
  _has_key: String

  """do all of these strings exist as top-level keys in the column"""
  _has_keys_all: [String!]

  """do any of these strings exist as top-level keys in the column"""
  _has_keys_any: [String!]
  _in: [jsonb!]
  _is_null: Boolean
  _lt: jsonb
  _lte: jsonb
  _neq: jsonb
  _nin: [jsonb!]
}

"""
columns and relationships of "kyc"
"""
type kyc {
  """Unique identifier for the applicant provided by Sumsub."""
  applicantId: String

  """
  The date and time when the applicant was created in Sumsub. Stored in UTC timestamp.
  """
  createDate: timestamptz!

  """UUID referencing the user ID in the existing accounts table."""
  externalUserId: uuid!

  """Level of KYC verification, referring to kycLevelName."""
  levelName: kycLevelName_enum

  """Status of the applicants review in Sumsub, referring to kycStatus."""
  reviewStatus: kycStatus_enum

  """Timestamp automatically updated whenever the kyc row changes."""
  updated_at: timestamptz
}

"""KYC levels representing the level of verification for the applicant."""
type kycLevelName {
  """
  basic_kyc_level: Basic level of KYC verification.
  advanced_kyc_level: Advanced level of KYC verification.
  """
  value: String!
}

"""
aggregated selection of "kycLevelName"
"""
type kycLevelName_aggregate {
  aggregate: kycLevelName_aggregate_fields
  nodes: [kycLevelName!]!
}

"""
aggregate fields of "kycLevelName"
"""
type kycLevelName_aggregate_fields {
  count(columns: [kycLevelName_select_column!], distinct: Boolean): Int!
  max: kycLevelName_max_fields
  min: kycLevelName_min_fields
}

"""
Boolean expression to filter rows from the table "kycLevelName". All fields are combined with a logical 'AND'.
"""
input kycLevelName_bool_exp {
  _and: [kycLevelName_bool_exp!]
  _not: kycLevelName_bool_exp
  _or: [kycLevelName_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "kycLevelName"
"""
enum kycLevelName_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  kycLevelName_pkey
}

enum kycLevelName_enum {
  advanced_kyc_level
  basic_kyc_level
}

"""
Boolean expression to compare columns of type "kycLevelName_enum". All fields are combined with logical 'AND'.
"""
input kycLevelName_enum_comparison_exp {
  _eq: kycLevelName_enum
  _in: [kycLevelName_enum!]
  _is_null: Boolean
  _neq: kycLevelName_enum
  _nin: [kycLevelName_enum!]
}

"""
input type for inserting data into table "kycLevelName"
"""
input kycLevelName_insert_input {
  """
  basic_kyc_level: Basic level of KYC verification.
  advanced_kyc_level: Advanced level of KYC verification.
  """
  value: String
}

"""aggregate max on columns"""
type kycLevelName_max_fields {
  """
  basic_kyc_level: Basic level of KYC verification.
  advanced_kyc_level: Advanced level of KYC verification.
  """
  value: String
}

"""aggregate min on columns"""
type kycLevelName_min_fields {
  """
  basic_kyc_level: Basic level of KYC verification.
  advanced_kyc_level: Advanced level of KYC verification.
  """
  value: String
}

"""
response of any mutation on the table "kycLevelName"
"""
type kycLevelName_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [kycLevelName!]!
}

"""
on_conflict condition type for table "kycLevelName"
"""
input kycLevelName_on_conflict {
  constraint: kycLevelName_constraint!
  update_columns: [kycLevelName_update_column!]! = []
  where: kycLevelName_bool_exp
}

"""Ordering options when selecting data from "kycLevelName"."""
input kycLevelName_order_by {
  value: order_by
}

"""primary key columns input for table: kycLevelName"""
input kycLevelName_pk_columns_input {
  """
  basic_kyc_level: Basic level of KYC verification.
  advanced_kyc_level: Advanced level of KYC verification.
  """
  value: String!
}

"""
select columns of table "kycLevelName"
"""
enum kycLevelName_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "kycLevelName"
"""
input kycLevelName_set_input {
  """
  basic_kyc_level: Basic level of KYC verification.
  advanced_kyc_level: Advanced level of KYC verification.
  """
  value: String
}

"""
Streaming cursor of the table "kycLevelName"
"""
input kycLevelName_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: kycLevelName_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input kycLevelName_stream_cursor_value_input {
  """
  basic_kyc_level: Basic level of KYC verification.
  advanced_kyc_level: Advanced level of KYC verification.
  """
  value: String
}

"""
update columns of table "kycLevelName"
"""
enum kycLevelName_update_column {
  """column name"""
  value
}

input kycLevelName_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: kycLevelName_set_input

  """filter the rows which have to be updated"""
  where: kycLevelName_bool_exp!
}

"""Statuses of Know Your Customer (KYC) processes."""
type kycStatus {
  """
  init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
  pending: An applicant is ready to be processed.
  prechecked: The check is in a half way of being finished.
  queued: The checks have been started for the applicant.
  completed: The check has been completed.
  onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
  """
  value: String!
}

"""
aggregated selection of "kycStatus"
"""
type kycStatus_aggregate {
  aggregate: kycStatus_aggregate_fields
  nodes: [kycStatus!]!
}

"""
aggregate fields of "kycStatus"
"""
type kycStatus_aggregate_fields {
  count(columns: [kycStatus_select_column!], distinct: Boolean): Int!
  max: kycStatus_max_fields
  min: kycStatus_min_fields
}

"""
Boolean expression to filter rows from the table "kycStatus". All fields are combined with a logical 'AND'.
"""
input kycStatus_bool_exp {
  _and: [kycStatus_bool_exp!]
  _not: kycStatus_bool_exp
  _or: [kycStatus_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "kycStatus"
"""
enum kycStatus_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  kycStatus_pkey
}

enum kycStatus_enum {
  completed
  init
  onHold
  pending
  prechecked
  queued
}

"""
Boolean expression to compare columns of type "kycStatus_enum". All fields are combined with logical 'AND'.
"""
input kycStatus_enum_comparison_exp {
  _eq: kycStatus_enum
  _in: [kycStatus_enum!]
  _is_null: Boolean
  _neq: kycStatus_enum
  _nin: [kycStatus_enum!]
}

"""
input type for inserting data into table "kycStatus"
"""
input kycStatus_insert_input {
  """
  init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
  pending: An applicant is ready to be processed.
  prechecked: The check is in a half way of being finished.
  queued: The checks have been started for the applicant.
  completed: The check has been completed.
  onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
  """
  value: String
}

"""aggregate max on columns"""
type kycStatus_max_fields {
  """
  init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
  pending: An applicant is ready to be processed.
  prechecked: The check is in a half way of being finished.
  queued: The checks have been started for the applicant.
  completed: The check has been completed.
  onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
  """
  value: String
}

"""aggregate min on columns"""
type kycStatus_min_fields {
  """
  init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
  pending: An applicant is ready to be processed.
  prechecked: The check is in a half way of being finished.
  queued: The checks have been started for the applicant.
  completed: The check has been completed.
  onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
  """
  value: String
}

"""
response of any mutation on the table "kycStatus"
"""
type kycStatus_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [kycStatus!]!
}

"""
on_conflict condition type for table "kycStatus"
"""
input kycStatus_on_conflict {
  constraint: kycStatus_constraint!
  update_columns: [kycStatus_update_column!]! = []
  where: kycStatus_bool_exp
}

"""Ordering options when selecting data from "kycStatus"."""
input kycStatus_order_by {
  value: order_by
}

"""primary key columns input for table: kycStatus"""
input kycStatus_pk_columns_input {
  """
  init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
  pending: An applicant is ready to be processed.
  prechecked: The check is in a half way of being finished.
  queued: The checks have been started for the applicant.
  completed: The check has been completed.
  onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
  """
  value: String!
}

"""
select columns of table "kycStatus"
"""
enum kycStatus_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "kycStatus"
"""
input kycStatus_set_input {
  """
  init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
  pending: An applicant is ready to be processed.
  prechecked: The check is in a half way of being finished.
  queued: The checks have been started for the applicant.
  completed: The check has been completed.
  onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
  """
  value: String
}

"""
Streaming cursor of the table "kycStatus"
"""
input kycStatus_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: kycStatus_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input kycStatus_stream_cursor_value_input {
  """
  init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
  pending: An applicant is ready to be processed.
  prechecked: The check is in a half way of being finished.
  queued: The checks have been started for the applicant.
  completed: The check has been completed.
  onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
  """
  value: String
}

"""
update columns of table "kycStatus"
"""
enum kycStatus_update_column {
  """column name"""
  value
}

input kycStatus_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: kycStatus_set_input

  """filter the rows which have to be updated"""
  where: kycStatus_bool_exp!
}

"""
aggregated selection of "kyc"
"""
type kyc_aggregate {
  aggregate: kyc_aggregate_fields
  nodes: [kyc!]!
}

"""
aggregate fields of "kyc"
"""
type kyc_aggregate_fields {
  count(columns: [kyc_select_column!], distinct: Boolean): Int!
  max: kyc_max_fields
  min: kyc_min_fields
}

"""
Boolean expression to filter rows from the table "kyc". All fields are combined with a logical 'AND'.
"""
input kyc_bool_exp {
  _and: [kyc_bool_exp!]
  _not: kyc_bool_exp
  _or: [kyc_bool_exp!]
  applicantId: String_comparison_exp
  createDate: timestamptz_comparison_exp
  externalUserId: uuid_comparison_exp
  levelName: kycLevelName_enum_comparison_exp
  reviewStatus: kycStatus_enum_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "kyc"
"""
enum kyc_constraint {
  """
  unique or primary key constraint on columns "externalUserId"
  """
  kyc_externalUserId_key

  """
  unique or primary key constraint on columns "externalUserId"
  """
  kyc_pkey
}

"""
input type for inserting data into table "kyc"
"""
input kyc_insert_input {
  """Unique identifier for the applicant provided by Sumsub."""
  applicantId: String

  """
  The date and time when the applicant was created in Sumsub. Stored in UTC timestamp.
  """
  createDate: timestamptz

  """UUID referencing the user ID in the existing accounts table."""
  externalUserId: uuid

  """Level of KYC verification, referring to kycLevelName."""
  levelName: kycLevelName_enum

  """Status of the applicants review in Sumsub, referring to kycStatus."""
  reviewStatus: kycStatus_enum

  """Timestamp automatically updated whenever the kyc row changes."""
  updated_at: timestamptz
}

"""aggregate max on columns"""
type kyc_max_fields {
  """Unique identifier for the applicant provided by Sumsub."""
  applicantId: String

  """
  The date and time when the applicant was created in Sumsub. Stored in UTC timestamp.
  """
  createDate: timestamptz

  """UUID referencing the user ID in the existing accounts table."""
  externalUserId: uuid

  """Timestamp automatically updated whenever the kyc row changes."""
  updated_at: timestamptz
}

"""aggregate min on columns"""
type kyc_min_fields {
  """Unique identifier for the applicant provided by Sumsub."""
  applicantId: String

  """
  The date and time when the applicant was created in Sumsub. Stored in UTC timestamp.
  """
  createDate: timestamptz

  """UUID referencing the user ID in the existing accounts table."""
  externalUserId: uuid

  """Timestamp automatically updated whenever the kyc row changes."""
  updated_at: timestamptz
}

"""
response of any mutation on the table "kyc"
"""
type kyc_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [kyc!]!
}

"""
input type for inserting object relation for remote table "kyc"
"""
input kyc_obj_rel_insert_input {
  data: kyc_insert_input!

  """upsert condition"""
  on_conflict: kyc_on_conflict
}

"""
on_conflict condition type for table "kyc"
"""
input kyc_on_conflict {
  constraint: kyc_constraint!
  update_columns: [kyc_update_column!]! = []
  where: kyc_bool_exp
}

"""Ordering options when selecting data from "kyc"."""
input kyc_order_by {
  applicantId: order_by
  createDate: order_by
  externalUserId: order_by
  levelName: order_by
  reviewStatus: order_by
  updated_at: order_by
}

"""primary key columns input for table: kyc"""
input kyc_pk_columns_input {
  """UUID referencing the user ID in the existing accounts table."""
  externalUserId: uuid!
}

"""
select columns of table "kyc"
"""
enum kyc_select_column {
  """column name"""
  applicantId

  """column name"""
  createDate

  """column name"""
  externalUserId

  """column name"""
  levelName

  """column name"""
  reviewStatus

  """column name"""
  updated_at
}

"""
input type for updating data in table "kyc"
"""
input kyc_set_input {
  """Unique identifier for the applicant provided by Sumsub."""
  applicantId: String

  """
  The date and time when the applicant was created in Sumsub. Stored in UTC timestamp.
  """
  createDate: timestamptz

  """UUID referencing the user ID in the existing accounts table."""
  externalUserId: uuid

  """Level of KYC verification, referring to kycLevelName."""
  levelName: kycLevelName_enum

  """Status of the applicants review in Sumsub, referring to kycStatus."""
  reviewStatus: kycStatus_enum

  """Timestamp automatically updated whenever the kyc row changes."""
  updated_at: timestamptz
}

"""
Streaming cursor of the table "kyc"
"""
input kyc_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: kyc_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input kyc_stream_cursor_value_input {
  """Unique identifier for the applicant provided by Sumsub."""
  applicantId: String

  """
  The date and time when the applicant was created in Sumsub. Stored in UTC timestamp.
  """
  createDate: timestamptz

  """UUID referencing the user ID in the existing accounts table."""
  externalUserId: uuid

  """Level of KYC verification, referring to kycLevelName."""
  levelName: kycLevelName_enum

  """Status of the applicants review in Sumsub, referring to kycStatus."""
  reviewStatus: kycStatus_enum

  """Timestamp automatically updated whenever the kyc row changes."""
  updated_at: timestamptz
}

"""
update columns of table "kyc"
"""
enum kyc_update_column {
  """column name"""
  applicantId

  """column name"""
  createDate

  """column name"""
  externalUserId

  """column name"""
  levelName

  """column name"""
  reviewStatus

  """column name"""
  updated_at
}

input kyc_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: kyc_set_input

  """filter the rows which have to be updated"""
  where: kyc_bool_exp!
}

"""
The lotteryParameters model is designed to define properties on a lottery, including details like the lotteryId and activityWebhookId. It manages various timestamps and settings related to the lottery, ensuring efficient and accurate management of lottery events.
"""
type lotteryParameters {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers related to the lottery.
  """
  activityWebhookId: String
  created_at: timestamptz!

  """
  Optional column
  for the end date and time for the lottery ticket sales, used when there is a defined sales period for the lottery.
  """
  dateSaleEnd: timestamp

  """
  Optional column for the start date and time for the lottery ticket sales, applicable if the lottery involves a sale.
  """
  dateSaleStart: timestamp
  id: uuid!
  lotteryId: String!
  organizerId: String!
  signingKey: String
  status: lotteryStatus_enum

  """
  The "timezone" column contains the timezone identifier for the lottery, ensuring accurate timing for events and sales across different regions.
  """
  timezone: String!
  updated_at: timestamptz!
}

"""
aggregated selection of "lotteryParameters"
"""
type lotteryParameters_aggregate {
  aggregate: lotteryParameters_aggregate_fields
  nodes: [lotteryParameters!]!
}

"""
aggregate fields of "lotteryParameters"
"""
type lotteryParameters_aggregate_fields {
  count(columns: [lotteryParameters_select_column!], distinct: Boolean): Int!
  max: lotteryParameters_max_fields
  min: lotteryParameters_min_fields
}

"""
Boolean expression to filter rows from the table "lotteryParameters". All fields are combined with a logical 'AND'.
"""
input lotteryParameters_bool_exp {
  _and: [lotteryParameters_bool_exp!]
  _not: lotteryParameters_bool_exp
  _or: [lotteryParameters_bool_exp!]
  activityWebhookId: String_comparison_exp
  created_at: timestamptz_comparison_exp
  dateSaleEnd: timestamp_comparison_exp
  dateSaleStart: timestamp_comparison_exp
  id: uuid_comparison_exp
  lotteryId: String_comparison_exp
  organizerId: String_comparison_exp
  signingKey: String_comparison_exp
  status: lotteryStatus_enum_comparison_exp
  timezone: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "lotteryParameters"
"""
enum lotteryParameters_constraint {
  """
  unique or primary key constraint on columns "lotteryId"
  """
  lotteryParameters_lotteryId_key

  """
  unique or primary key constraint on columns "id"
  """
  lotteryParameters_pkey

  """
  unique or primary key constraint on columns "signingKey"
  """
  lotteryParameters_signingKey_key
}

"""
input type for inserting data into table "lotteryParameters"
"""
input lotteryParameters_insert_input {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers related to the lottery.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  Optional column
  for the end date and time for the lottery ticket sales, used when there is a defined sales period for the lottery.
  """
  dateSaleEnd: timestamp

  """
  Optional column for the start date and time for the lottery ticket sales, applicable if the lottery involves a sale.
  """
  dateSaleStart: timestamp
  id: uuid
  lotteryId: String
  organizerId: String
  signingKey: String
  status: lotteryStatus_enum

  """
  The "timezone" column contains the timezone identifier for the lottery, ensuring accurate timing for events and sales across different regions.
  """
  timezone: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type lotteryParameters_max_fields {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers related to the lottery.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  Optional column
  for the end date and time for the lottery ticket sales, used when there is a defined sales period for the lottery.
  """
  dateSaleEnd: timestamp

  """
  Optional column for the start date and time for the lottery ticket sales, applicable if the lottery involves a sale.
  """
  dateSaleStart: timestamp
  id: uuid
  lotteryId: String
  organizerId: String
  signingKey: String

  """
  The "timezone" column contains the timezone identifier for the lottery, ensuring accurate timing for events and sales across different regions.
  """
  timezone: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type lotteryParameters_min_fields {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers related to the lottery.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  Optional column
  for the end date and time for the lottery ticket sales, used when there is a defined sales period for the lottery.
  """
  dateSaleEnd: timestamp

  """
  Optional column for the start date and time for the lottery ticket sales, applicable if the lottery involves a sale.
  """
  dateSaleStart: timestamp
  id: uuid
  lotteryId: String
  organizerId: String
  signingKey: String

  """
  The "timezone" column contains the timezone identifier for the lottery, ensuring accurate timing for events and sales across different regions.
  """
  timezone: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "lotteryParameters"
"""
type lotteryParameters_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [lotteryParameters!]!
}

"""
on_conflict condition type for table "lotteryParameters"
"""
input lotteryParameters_on_conflict {
  constraint: lotteryParameters_constraint!
  update_columns: [lotteryParameters_update_column!]! = []
  where: lotteryParameters_bool_exp
}

"""Ordering options when selecting data from "lotteryParameters"."""
input lotteryParameters_order_by {
  activityWebhookId: order_by
  created_at: order_by
  dateSaleEnd: order_by
  dateSaleStart: order_by
  id: order_by
  lotteryId: order_by
  organizerId: order_by
  signingKey: order_by
  status: order_by
  timezone: order_by
  updated_at: order_by
}

"""primary key columns input for table: lotteryParameters"""
input lotteryParameters_pk_columns_input {
  id: uuid!
}

"""
select columns of table "lotteryParameters"
"""
enum lotteryParameters_select_column {
  """column name"""
  activityWebhookId

  """column name"""
  created_at

  """column name"""
  dateSaleEnd

  """column name"""
  dateSaleStart

  """column name"""
  id

  """column name"""
  lotteryId

  """column name"""
  organizerId

  """column name"""
  signingKey

  """column name"""
  status

  """column name"""
  timezone

  """column name"""
  updated_at
}

"""
input type for updating data in table "lotteryParameters"
"""
input lotteryParameters_set_input {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers related to the lottery.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  Optional column
  for the end date and time for the lottery ticket sales, used when there is a defined sales period for the lottery.
  """
  dateSaleEnd: timestamp

  """
  Optional column for the start date and time for the lottery ticket sales, applicable if the lottery involves a sale.
  """
  dateSaleStart: timestamp
  id: uuid
  lotteryId: String
  organizerId: String
  signingKey: String
  status: lotteryStatus_enum

  """
  The "timezone" column contains the timezone identifier for the lottery, ensuring accurate timing for events and sales across different regions.
  """
  timezone: String
  updated_at: timestamptz
}

"""
Streaming cursor of the table "lotteryParameters"
"""
input lotteryParameters_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: lotteryParameters_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input lotteryParameters_stream_cursor_value_input {
  """
  The "activityWebhookId" column stores the identifier for the Alchemy webhook that tracks NFT transfers related to the lottery.
  """
  activityWebhookId: String
  created_at: timestamptz

  """
  Optional column
  for the end date and time for the lottery ticket sales, used when there is a defined sales period for the lottery.
  """
  dateSaleEnd: timestamp

  """
  Optional column for the start date and time for the lottery ticket sales, applicable if the lottery involves a sale.
  """
  dateSaleStart: timestamp
  id: uuid
  lotteryId: String
  organizerId: String
  signingKey: String
  status: lotteryStatus_enum

  """
  The "timezone" column contains the timezone identifier for the lottery, ensuring accurate timing for events and sales across different regions.
  """
  timezone: String
  updated_at: timestamptz
}

"""
update columns of table "lotteryParameters"
"""
enum lotteryParameters_update_column {
  """column name"""
  activityWebhookId

  """column name"""
  created_at

  """column name"""
  dateSaleEnd

  """column name"""
  dateSaleStart

  """column name"""
  id

  """column name"""
  lotteryId

  """column name"""
  organizerId

  """column name"""
  signingKey

  """column name"""
  status

  """column name"""
  timezone

  """column name"""
  updated_at
}

input lotteryParameters_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: lotteryParameters_set_input

  """filter the rows which have to be updated"""
  where: lotteryParameters_bool_exp!
}

"""
columns and relationships of "lotteryStatus"
"""
type lotteryStatus {
  value: String!
}

"""
aggregated selection of "lotteryStatus"
"""
type lotteryStatus_aggregate {
  aggregate: lotteryStatus_aggregate_fields
  nodes: [lotteryStatus!]!
}

"""
aggregate fields of "lotteryStatus"
"""
type lotteryStatus_aggregate_fields {
  count(columns: [lotteryStatus_select_column!], distinct: Boolean): Int!
  max: lotteryStatus_max_fields
  min: lotteryStatus_min_fields
}

"""
Boolean expression to filter rows from the table "lotteryStatus". All fields are combined with a logical 'AND'.
"""
input lotteryStatus_bool_exp {
  _and: [lotteryStatus_bool_exp!]
  _not: lotteryStatus_bool_exp
  _or: [lotteryStatus_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "lotteryStatus"
"""
enum lotteryStatus_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  lotteryStatus_pkey
}

enum lotteryStatus_enum {
  DRAFT
  PUBLISHED
}

"""
Boolean expression to compare columns of type "lotteryStatus_enum". All fields are combined with logical 'AND'.
"""
input lotteryStatus_enum_comparison_exp {
  _eq: lotteryStatus_enum
  _in: [lotteryStatus_enum!]
  _is_null: Boolean
  _neq: lotteryStatus_enum
  _nin: [lotteryStatus_enum!]
}

"""
input type for inserting data into table "lotteryStatus"
"""
input lotteryStatus_insert_input {
  value: String
}

"""aggregate max on columns"""
type lotteryStatus_max_fields {
  value: String
}

"""aggregate min on columns"""
type lotteryStatus_min_fields {
  value: String
}

"""
response of any mutation on the table "lotteryStatus"
"""
type lotteryStatus_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [lotteryStatus!]!
}

"""
on_conflict condition type for table "lotteryStatus"
"""
input lotteryStatus_on_conflict {
  constraint: lotteryStatus_constraint!
  update_columns: [lotteryStatus_update_column!]! = []
  where: lotteryStatus_bool_exp
}

"""Ordering options when selecting data from "lotteryStatus"."""
input lotteryStatus_order_by {
  value: order_by
}

"""primary key columns input for table: lotteryStatus"""
input lotteryStatus_pk_columns_input {
  value: String!
}

"""
select columns of table "lotteryStatus"
"""
enum lotteryStatus_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "lotteryStatus"
"""
input lotteryStatus_set_input {
  value: String
}

"""
Streaming cursor of the table "lotteryStatus"
"""
input lotteryStatus_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: lotteryStatus_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input lotteryStatus_stream_cursor_value_input {
  value: String
}

"""
update columns of table "lotteryStatus"
"""
enum lotteryStatus_update_column {
  """column name"""
  value
}

input lotteryStatus_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: lotteryStatus_set_input

  """filter the rows which have to be updated"""
  where: lotteryStatus_bool_exp!
}

"""mutation root"""
type mutation_root {
  """Create one asset"""
  createAsset(data: AssetCreateInput!): Asset

  """Create one event"""
  createEvent(data: EventCreateInput!): Event

  """Create one eventPass"""
  createEventPass(data: EventPassCreateInput!): EventPass

  """Create one eventPassDelayedRevealed"""
  createEventPassDelayedRevealed(data: EventPassDelayedRevealedCreateInput!): EventPassDelayedRevealed

  """Create one organizer"""
  createOrganizer(data: OrganizerCreateInput!): Organizer

  """Create one pack"""
  createPack(data: PackCreateInput!): Pack

  """Create one scheduledRelease"""
  createScheduledRelease(data: ScheduledReleaseCreateInput!): ScheduledRelease

  """Delete one asset from _all_ existing stages. Returns deleted document."""
  deleteAsset(
    """Document to delete"""
    where: AssetWhereUniqueInput!
  ): Asset

  """Delete one event from _all_ existing stages. Returns deleted document."""
  deleteEvent(
    """Document to delete"""
    where: EventWhereUniqueInput!
  ): Event

  """
  Delete one eventPass from _all_ existing stages. Returns deleted document.
  """
  deleteEventPass(
    """Document to delete"""
    where: EventPassWhereUniqueInput!
  ): EventPass

  """
  Delete one eventPassDelayedRevealed from _all_ existing stages. Returns deleted document.
  """
  deleteEventPassDelayedRevealed(
    """Document to delete"""
    where: EventPassDelayedRevealedWhereUniqueInput!
  ): EventPassDelayedRevealed

  """Delete many Asset documents"""
  deleteManyAssets(
    """Documents to delete"""
    where: AssetManyWhereInput
  ): BatchPayload!

  """Delete many Asset documents, return deleted documents"""
  deleteManyAssetsConnection(
    after: ID
    before: ID
    first: Int
    last: Int
    skip: Int

    """Documents to delete"""
    where: AssetManyWhereInput
  ): AssetConnection!

  """Delete many EventPass documents"""
  deleteManyEventPasses(
    """Documents to delete"""
    where: EventPassManyWhereInput
  ): BatchPayload!

  """Delete many EventPass documents, return deleted documents"""
  deleteManyEventPassesConnection(
    after: ID
    before: ID
    first: Int
    last: Int
    skip: Int

    """Documents to delete"""
    where: EventPassManyWhereInput
  ): EventPassConnection!

  """Delete many EventPassDelayedRevealed documents"""
  deleteManyEventPassesDelayedRevealed(
    """Documents to delete"""
    where: EventPassDelayedRevealedManyWhereInput
  ): BatchPayload!

  """
  Delete many EventPassDelayedRevealed documents, return deleted documents
  """
  deleteManyEventPassesDelayedRevealedConnection(
    after: ID
    before: ID
    first: Int
    last: Int
    skip: Int

    """Documents to delete"""
    where: EventPassDelayedRevealedManyWhereInput
  ): EventPassDelayedRevealedConnection!

  """Delete many Event documents"""
  deleteManyEvents(
    """Documents to delete"""
    where: EventManyWhereInput
  ): BatchPayload!

  """Delete many Event documents, return deleted documents"""
  deleteManyEventsConnection(
    after: ID
    before: ID
    first: Int
    last: Int
    skip: Int

    """Documents to delete"""
    where: EventManyWhereInput
  ): EventConnection!

  """Delete many Organizer documents"""
  deleteManyOrganizers(
    """Documents to delete"""
    where: OrganizerManyWhereInput
  ): BatchPayload!

  """Delete many Organizer documents, return deleted documents"""
  deleteManyOrganizersConnection(
    after: ID
    before: ID
    first: Int
    last: Int
    skip: Int

    """Documents to delete"""
    where: OrganizerManyWhereInput
  ): OrganizerConnection!

  """Delete many Pack documents"""
  deleteManyPacks(
    """Documents to delete"""
    where: PackManyWhereInput
  ): BatchPayload!

  """Delete many Pack documents, return deleted documents"""
  deleteManyPacksConnection(
    after: ID
    before: ID
    first: Int
    last: Int
    skip: Int

    """Documents to delete"""
    where: PackManyWhereInput
  ): PackConnection!

  """
  Delete one organizer from _all_ existing stages. Returns deleted document.
  """
  deleteOrganizer(
    """Document to delete"""
    where: OrganizerWhereUniqueInput!
  ): Organizer

  """Delete one pack from _all_ existing stages. Returns deleted document."""
  deletePack(
    """Document to delete"""
    where: PackWhereUniqueInput!
  ): Pack

  """Delete and return scheduled operation"""
  deleteScheduledOperation(
    """Document to delete"""
    where: ScheduledOperationWhereUniqueInput!
  ): ScheduledOperation

  """
  Delete one scheduledRelease from _all_ existing stages. Returns deleted document.
  """
  deleteScheduledRelease(
    """Document to delete"""
    where: ScheduledReleaseWhereUniqueInput!
  ): ScheduledRelease

  """
  delete data from the table: "account"
  """
  delete_account(
    """filter the rows which have to be deleted"""
    where: account_bool_exp!
  ): account_mutation_response

  """
  delete single row from the table: "account"
  """
  delete_account_by_pk(id: uuid!): account

  """
  delete data from the table: "currency"
  """
  delete_currency(
    """filter the rows which have to be deleted"""
    where: currency_bool_exp!
  ): currency_mutation_response

  """
  delete single row from the table: "currency"
  """
  delete_currency_by_pk(value: String!): currency

  """
  delete data from the table: "eventParameters"
  """
  delete_eventParameters(
    """filter the rows which have to be deleted"""
    where: eventParameters_bool_exp!
  ): eventParameters_mutation_response

  """
  delete single row from the table: "eventParameters"
  """
  delete_eventParameters_by_pk(id: uuid!): eventParameters

  """
  delete data from the table: "eventPassNft"
  """
  delete_eventPassNft(
    """filter the rows which have to be deleted"""
    where: eventPassNft_bool_exp!
  ): eventPassNft_mutation_response

  """
  delete data from the table: "eventPassNftContract"
  """
  delete_eventPassNftContract(
    """filter the rows which have to be deleted"""
    where: eventPassNftContract_bool_exp!
  ): eventPassNftContract_mutation_response

  """
  delete data from the table: "eventPassNftContractType"
  """
  delete_eventPassNftContractType(
    """filter the rows which have to be deleted"""
    where: eventPassNftContractType_bool_exp!
  ): eventPassNftContractType_mutation_response

  """
  delete single row from the table: "eventPassNftContractType"
  """
  delete_eventPassNftContractType_by_pk(
    """Type name for event pass NFT contract."""
    value: String!
  ): eventPassNftContractType

  """
  delete single row from the table: "eventPassNftContract"
  """
  delete_eventPassNftContract_by_pk(id: uuid!): eventPassNftContract

  """
  delete single row from the table: "eventPassNft"
  """
  delete_eventPassNft_by_pk(id: uuid!): eventPassNft

  """
  delete data from the table: "eventPassOrderSums"
  """
  delete_eventPassOrderSums(
    """filter the rows which have to be deleted"""
    where: eventPassOrderSums_bool_exp!
  ): eventPassOrderSums_mutation_response

  """
  delete single row from the table: "eventPassOrderSums"
  """
  delete_eventPassOrderSums_by_pk(eventPassId: String!): eventPassOrderSums

  """
  delete data from the table: "eventPassType"
  """
  delete_eventPassType(
    """filter the rows which have to be deleted"""
    where: eventPassType_bool_exp!
  ): eventPassType_mutation_response

  """
  delete single row from the table: "eventPassType"
  """
  delete_eventPassType_by_pk(
    """Type name for event pass."""
    value: String!
  ): eventPassType

  """
  delete data from the table: "eventPassValidationType"
  """
  delete_eventPassValidationType(
    """filter the rows which have to be deleted"""
    where: eventPassValidationType_bool_exp!
  ): eventPassValidationType_mutation_response

  """
  delete single row from the table: "eventPassValidationType"
  """
  delete_eventPassValidationType_by_pk(
    """Type name for event pass validation."""
    value: String!
  ): eventPassValidationType

  """
  delete data from the table: "eventStatus"
  """
  delete_eventStatus(
    """filter the rows which have to be deleted"""
    where: eventStatus_bool_exp!
  ): eventStatus_mutation_response

  """
  delete single row from the table: "eventStatus"
  """
  delete_eventStatus_by_pk(value: String!): eventStatus

  """
  delete data from the table: "follow"
  """
  delete_follow(
    """filter the rows which have to be deleted"""
    where: follow_bool_exp!
  ): follow_mutation_response

  """
  delete single row from the table: "follow"
  """
  delete_follow_by_pk(
    """
    References the unique identifier of the account that is following an organizer.
    """
    accountId: uuid!

    """
    Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
    """
    organizerSlug: String!
  ): follow

  """
  delete data from the table: "kyc"
  """
  delete_kyc(
    """filter the rows which have to be deleted"""
    where: kyc_bool_exp!
  ): kyc_mutation_response

  """
  delete data from the table: "kycLevelName"
  """
  delete_kycLevelName(
    """filter the rows which have to be deleted"""
    where: kycLevelName_bool_exp!
  ): kycLevelName_mutation_response

  """
  delete single row from the table: "kycLevelName"
  """
  delete_kycLevelName_by_pk(
    """
    basic_kyc_level: Basic level of KYC verification.
    advanced_kyc_level: Advanced level of KYC verification.
    """
    value: String!
  ): kycLevelName

  """
  delete data from the table: "kycStatus"
  """
  delete_kycStatus(
    """filter the rows which have to be deleted"""
    where: kycStatus_bool_exp!
  ): kycStatus_mutation_response

  """
  delete single row from the table: "kycStatus"
  """
  delete_kycStatus_by_pk(
    """
    init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
    pending: An applicant is ready to be processed.
    prechecked: The check is in a half way of being finished.
    queued: The checks have been started for the applicant.
    completed: The check has been completed.
    onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
    """
    value: String!
  ): kycStatus

  """
  delete single row from the table: "kyc"
  """
  delete_kyc_by_pk(
    """UUID referencing the user ID in the existing accounts table."""
    externalUserId: uuid!
  ): kyc

  """
  delete data from the table: "lotteryParameters"
  """
  delete_lotteryParameters(
    """filter the rows which have to be deleted"""
    where: lotteryParameters_bool_exp!
  ): lotteryParameters_mutation_response

  """
  delete single row from the table: "lotteryParameters"
  """
  delete_lotteryParameters_by_pk(id: uuid!): lotteryParameters

  """
  delete data from the table: "lotteryStatus"
  """
  delete_lotteryStatus(
    """filter the rows which have to be deleted"""
    where: lotteryStatus_bool_exp!
  ): lotteryStatus_mutation_response

  """
  delete single row from the table: "lotteryStatus"
  """
  delete_lotteryStatus_by_pk(value: String!): lotteryStatus

  """
  delete data from the table: "nftTransfer"
  """
  delete_nftTransfer(
    """filter the rows which have to be deleted"""
    where: nftTransfer_bool_exp!
  ): nftTransfer_mutation_response

  """
  delete single row from the table: "nftTransfer"
  """
  delete_nftTransfer_by_pk(id: uuid!): nftTransfer

  """
  delete data from the table: "order"
  """
  delete_order(
    """filter the rows which have to be deleted"""
    where: order_bool_exp!
  ): order_mutation_response

  """
  delete data from the table: "orderStatus"
  """
  delete_orderStatus(
    """filter the rows which have to be deleted"""
    where: orderStatus_bool_exp!
  ): orderStatus_mutation_response

  """
  delete single row from the table: "orderStatus"
  """
  delete_orderStatus_by_pk(value: String!): orderStatus

  """
  delete single row from the table: "order"
  """
  delete_order_by_pk(id: uuid!): order

  """
  delete data from the table: "packEventPassNft"
  """
  delete_packEventPassNft(
    """filter the rows which have to be deleted"""
    where: packEventPassNft_bool_exp!
  ): packEventPassNft_mutation_response

  """
  delete single row from the table: "packEventPassNft"
  """
  delete_packEventPassNft_by_pk(
    """Identifier for the event pass NFT."""
    eventPassNftId: uuid!

    """Identifier for the pack NFT supply."""
    packNftSupplyId: uuid!
  ): packEventPassNft

  """
  delete data from the table: "packNftContract"
  """
  delete_packNftContract(
    """filter the rows which have to be deleted"""
    where: packNftContract_bool_exp!
  ): packNftContract_mutation_response

  """
  delete data from the table: "packNftContractEventPass"
  """
  delete_packNftContractEventPass(
    """filter the rows which have to be deleted"""
    where: packNftContractEventPass_bool_exp!
  ): packNftContractEventPass_mutation_response

  """
  delete single row from the table: "packNftContractEventPass"
  """
  delete_packNftContractEventPass_by_pk(
    """
    Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
    """
    eventPassId: String!

    """
    Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
    """
    packNftContractId: uuid!
  ): packNftContractEventPass

  """
  delete single row from the table: "packNftContract"
  """
  delete_packNftContract_by_pk(
    """Unique identifier for each pack NFT contract."""
    id: uuid!
  ): packNftContract

  """
  delete data from the table: "packNftSupply"
  """
  delete_packNftSupply(
    """filter the rows which have to be deleted"""
    where: packNftSupply_bool_exp!
  ): packNftSupply_mutation_response

  """
  delete single row from the table: "packNftSupply"
  """
  delete_packNftSupply_by_pk(id: uuid!): packNftSupply

  """
  delete data from the table: "packOrderSums"
  """
  delete_packOrderSums(
    """filter the rows which have to be deleted"""
    where: packOrderSums_bool_exp!
  ): packOrderSums_mutation_response

  """
  delete single row from the table: "packOrderSums"
  """
  delete_packOrderSums_by_pk(packId: String!): packOrderSums

  """
  delete data from the table: "passAmount"
  """
  delete_passAmount(
    """filter the rows which have to be deleted"""
    where: passAmount_bool_exp!
  ): passAmount_mutation_response

  """
  delete single row from the table: "passAmount"
  """
  delete_passAmount_by_pk(id: uuid!): passAmount

  """
  delete data from the table: "passPricing"
  """
  delete_passPricing(
    """filter the rows which have to be deleted"""
    where: passPricing_bool_exp!
  ): passPricing_mutation_response

  """
  delete single row from the table: "passPricing"
  """
  delete_passPricing_by_pk(id: uuid!): passPricing

  """
  delete data from the table: "pendingOrder"
  """
  delete_pendingOrder(
    """filter the rows which have to be deleted"""
    where: pendingOrder_bool_exp!
  ): pendingOrder_mutation_response

  """
  delete single row from the table: "pendingOrder"
  """
  delete_pendingOrder_by_pk(id: uuid!): pendingOrder

  """
  delete data from the table: "roleAssignment"
  """
  delete_roleAssignment(
    """filter the rows which have to be deleted"""
    where: roleAssignment_bool_exp!
  ): roleAssignment_mutation_response

  """
  delete data from the table: "roles"
  """
  delete_roles(
    """filter the rows which have to be deleted"""
    where: roles_bool_exp!
  ): roles_mutation_response

  """
  delete single row from the table: "roles"
  """
  delete_roles_by_pk(
    "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
    value: String!
  ): roles

  """
  delete data from the table: "stripeCheckoutSession"
  """
  delete_stripeCheckoutSession(
    """filter the rows which have to be deleted"""
    where: stripeCheckoutSession_bool_exp!
  ): stripeCheckoutSession_mutation_response

  """
  delete data from the table: "stripeCheckoutSessionType"
  """
  delete_stripeCheckoutSessionType(
    """filter the rows which have to be deleted"""
    where: stripeCheckoutSessionType_bool_exp!
  ): stripeCheckoutSessionType_mutation_response

  """
  delete single row from the table: "stripeCheckoutSessionType"
  """
  delete_stripeCheckoutSessionType_by_pk(
    """Type value."""
    value: String!
  ): stripeCheckoutSessionType

  """
  delete single row from the table: "stripeCheckoutSession"
  """
  delete_stripeCheckoutSession_by_pk(
    """Unique identifier for the Stripe Checkout Session."""
    stripeSessionId: String!
  ): stripeCheckoutSession

  """
  delete data from the table: "stripeCustomer"
  """
  delete_stripeCustomer(
    """filter the rows which have to be deleted"""
    where: stripeCustomer_bool_exp!
  ): stripeCustomer_mutation_response

  """
  delete single row from the table: "stripeCustomer"
  """
  delete_stripeCustomer_by_pk(
    """Unique identifier for the Stripe Customer."""
    stripeCustomerId: String!
  ): stripeCustomer

  """
  delete data from the table: "timezone"
  """
  delete_timezone(
    """filter the rows which have to be deleted"""
    where: timezone_bool_exp!
  ): timezone_mutation_response

  """
  delete single row from the table: "timezone"
  """
  delete_timezone_by_pk(value: String!): timezone

  """
  insert data into the table: "account"
  """
  insert_account(
    """the rows to be inserted"""
    objects: [account_insert_input!]!

    """upsert condition"""
    on_conflict: account_on_conflict
  ): account_mutation_response

  """
  insert a single row into the table: "account"
  """
  insert_account_one(
    """the row to be inserted"""
    object: account_insert_input!

    """upsert condition"""
    on_conflict: account_on_conflict
  ): account

  """
  insert data into the table: "currency"
  """
  insert_currency(
    """the rows to be inserted"""
    objects: [currency_insert_input!]!

    """upsert condition"""
    on_conflict: currency_on_conflict
  ): currency_mutation_response

  """
  insert a single row into the table: "currency"
  """
  insert_currency_one(
    """the row to be inserted"""
    object: currency_insert_input!

    """upsert condition"""
    on_conflict: currency_on_conflict
  ): currency

  """
  insert data into the table: "eventParameters"
  """
  insert_eventParameters(
    """the rows to be inserted"""
    objects: [eventParameters_insert_input!]!

    """upsert condition"""
    on_conflict: eventParameters_on_conflict
  ): eventParameters_mutation_response

  """
  insert a single row into the table: "eventParameters"
  """
  insert_eventParameters_one(
    """the row to be inserted"""
    object: eventParameters_insert_input!

    """upsert condition"""
    on_conflict: eventParameters_on_conflict
  ): eventParameters

  """
  insert data into the table: "eventPassNft"
  """
  insert_eventPassNft(
    """the rows to be inserted"""
    objects: [eventPassNft_insert_input!]!

    """upsert condition"""
    on_conflict: eventPassNft_on_conflict
  ): eventPassNft_mutation_response

  """
  insert data into the table: "eventPassNftContract"
  """
  insert_eventPassNftContract(
    """the rows to be inserted"""
    objects: [eventPassNftContract_insert_input!]!

    """upsert condition"""
    on_conflict: eventPassNftContract_on_conflict
  ): eventPassNftContract_mutation_response

  """
  insert data into the table: "eventPassNftContractType"
  """
  insert_eventPassNftContractType(
    """the rows to be inserted"""
    objects: [eventPassNftContractType_insert_input!]!

    """upsert condition"""
    on_conflict: eventPassNftContractType_on_conflict
  ): eventPassNftContractType_mutation_response

  """
  insert a single row into the table: "eventPassNftContractType"
  """
  insert_eventPassNftContractType_one(
    """the row to be inserted"""
    object: eventPassNftContractType_insert_input!

    """upsert condition"""
    on_conflict: eventPassNftContractType_on_conflict
  ): eventPassNftContractType

  """
  insert a single row into the table: "eventPassNftContract"
  """
  insert_eventPassNftContract_one(
    """the row to be inserted"""
    object: eventPassNftContract_insert_input!

    """upsert condition"""
    on_conflict: eventPassNftContract_on_conflict
  ): eventPassNftContract

  """
  insert a single row into the table: "eventPassNft"
  """
  insert_eventPassNft_one(
    """the row to be inserted"""
    object: eventPassNft_insert_input!

    """upsert condition"""
    on_conflict: eventPassNft_on_conflict
  ): eventPassNft

  """
  insert data into the table: "eventPassOrderSums"
  """
  insert_eventPassOrderSums(
    """the rows to be inserted"""
    objects: [eventPassOrderSums_insert_input!]!

    """upsert condition"""
    on_conflict: eventPassOrderSums_on_conflict
  ): eventPassOrderSums_mutation_response

  """
  insert a single row into the table: "eventPassOrderSums"
  """
  insert_eventPassOrderSums_one(
    """the row to be inserted"""
    object: eventPassOrderSums_insert_input!

    """upsert condition"""
    on_conflict: eventPassOrderSums_on_conflict
  ): eventPassOrderSums

  """
  insert data into the table: "eventPassType"
  """
  insert_eventPassType(
    """the rows to be inserted"""
    objects: [eventPassType_insert_input!]!

    """upsert condition"""
    on_conflict: eventPassType_on_conflict
  ): eventPassType_mutation_response

  """
  insert a single row into the table: "eventPassType"
  """
  insert_eventPassType_one(
    """the row to be inserted"""
    object: eventPassType_insert_input!

    """upsert condition"""
    on_conflict: eventPassType_on_conflict
  ): eventPassType

  """
  insert data into the table: "eventPassValidationType"
  """
  insert_eventPassValidationType(
    """the rows to be inserted"""
    objects: [eventPassValidationType_insert_input!]!

    """upsert condition"""
    on_conflict: eventPassValidationType_on_conflict
  ): eventPassValidationType_mutation_response

  """
  insert a single row into the table: "eventPassValidationType"
  """
  insert_eventPassValidationType_one(
    """the row to be inserted"""
    object: eventPassValidationType_insert_input!

    """upsert condition"""
    on_conflict: eventPassValidationType_on_conflict
  ): eventPassValidationType

  """
  insert data into the table: "eventStatus"
  """
  insert_eventStatus(
    """the rows to be inserted"""
    objects: [eventStatus_insert_input!]!

    """upsert condition"""
    on_conflict: eventStatus_on_conflict
  ): eventStatus_mutation_response

  """
  insert a single row into the table: "eventStatus"
  """
  insert_eventStatus_one(
    """the row to be inserted"""
    object: eventStatus_insert_input!

    """upsert condition"""
    on_conflict: eventStatus_on_conflict
  ): eventStatus

  """
  insert data into the table: "follow"
  """
  insert_follow(
    """the rows to be inserted"""
    objects: [follow_insert_input!]!

    """upsert condition"""
    on_conflict: follow_on_conflict
  ): follow_mutation_response

  """
  insert a single row into the table: "follow"
  """
  insert_follow_one(
    """the row to be inserted"""
    object: follow_insert_input!

    """upsert condition"""
    on_conflict: follow_on_conflict
  ): follow

  """
  insert data into the table: "kyc"
  """
  insert_kyc(
    """the rows to be inserted"""
    objects: [kyc_insert_input!]!

    """upsert condition"""
    on_conflict: kyc_on_conflict
  ): kyc_mutation_response

  """
  insert data into the table: "kycLevelName"
  """
  insert_kycLevelName(
    """the rows to be inserted"""
    objects: [kycLevelName_insert_input!]!

    """upsert condition"""
    on_conflict: kycLevelName_on_conflict
  ): kycLevelName_mutation_response

  """
  insert a single row into the table: "kycLevelName"
  """
  insert_kycLevelName_one(
    """the row to be inserted"""
    object: kycLevelName_insert_input!

    """upsert condition"""
    on_conflict: kycLevelName_on_conflict
  ): kycLevelName

  """
  insert data into the table: "kycStatus"
  """
  insert_kycStatus(
    """the rows to be inserted"""
    objects: [kycStatus_insert_input!]!

    """upsert condition"""
    on_conflict: kycStatus_on_conflict
  ): kycStatus_mutation_response

  """
  insert a single row into the table: "kycStatus"
  """
  insert_kycStatus_one(
    """the row to be inserted"""
    object: kycStatus_insert_input!

    """upsert condition"""
    on_conflict: kycStatus_on_conflict
  ): kycStatus

  """
  insert a single row into the table: "kyc"
  """
  insert_kyc_one(
    """the row to be inserted"""
    object: kyc_insert_input!

    """upsert condition"""
    on_conflict: kyc_on_conflict
  ): kyc

  """
  insert data into the table: "lotteryParameters"
  """
  insert_lotteryParameters(
    """the rows to be inserted"""
    objects: [lotteryParameters_insert_input!]!

    """upsert condition"""
    on_conflict: lotteryParameters_on_conflict
  ): lotteryParameters_mutation_response

  """
  insert a single row into the table: "lotteryParameters"
  """
  insert_lotteryParameters_one(
    """the row to be inserted"""
    object: lotteryParameters_insert_input!

    """upsert condition"""
    on_conflict: lotteryParameters_on_conflict
  ): lotteryParameters

  """
  insert data into the table: "lotteryStatus"
  """
  insert_lotteryStatus(
    """the rows to be inserted"""
    objects: [lotteryStatus_insert_input!]!

    """upsert condition"""
    on_conflict: lotteryStatus_on_conflict
  ): lotteryStatus_mutation_response

  """
  insert a single row into the table: "lotteryStatus"
  """
  insert_lotteryStatus_one(
    """the row to be inserted"""
    object: lotteryStatus_insert_input!

    """upsert condition"""
    on_conflict: lotteryStatus_on_conflict
  ): lotteryStatus

  """
  insert data into the table: "nftTransfer"
  """
  insert_nftTransfer(
    """the rows to be inserted"""
    objects: [nftTransfer_insert_input!]!

    """upsert condition"""
    on_conflict: nftTransfer_on_conflict
  ): nftTransfer_mutation_response

  """
  insert a single row into the table: "nftTransfer"
  """
  insert_nftTransfer_one(
    """the row to be inserted"""
    object: nftTransfer_insert_input!

    """upsert condition"""
    on_conflict: nftTransfer_on_conflict
  ): nftTransfer

  """
  insert data into the table: "order"
  """
  insert_order(
    """the rows to be inserted"""
    objects: [order_insert_input!]!

    """upsert condition"""
    on_conflict: order_on_conflict
  ): order_mutation_response

  """
  insert data into the table: "orderStatus"
  """
  insert_orderStatus(
    """the rows to be inserted"""
    objects: [orderStatus_insert_input!]!

    """upsert condition"""
    on_conflict: orderStatus_on_conflict
  ): orderStatus_mutation_response

  """
  insert a single row into the table: "orderStatus"
  """
  insert_orderStatus_one(
    """the row to be inserted"""
    object: orderStatus_insert_input!

    """upsert condition"""
    on_conflict: orderStatus_on_conflict
  ): orderStatus

  """
  insert a single row into the table: "order"
  """
  insert_order_one(
    """the row to be inserted"""
    object: order_insert_input!

    """upsert condition"""
    on_conflict: order_on_conflict
  ): order

  """
  insert data into the table: "packEventPassNft"
  """
  insert_packEventPassNft(
    """the rows to be inserted"""
    objects: [packEventPassNft_insert_input!]!

    """upsert condition"""
    on_conflict: packEventPassNft_on_conflict
  ): packEventPassNft_mutation_response

  """
  insert a single row into the table: "packEventPassNft"
  """
  insert_packEventPassNft_one(
    """the row to be inserted"""
    object: packEventPassNft_insert_input!

    """upsert condition"""
    on_conflict: packEventPassNft_on_conflict
  ): packEventPassNft

  """
  insert data into the table: "packNftContract"
  """
  insert_packNftContract(
    """the rows to be inserted"""
    objects: [packNftContract_insert_input!]!

    """upsert condition"""
    on_conflict: packNftContract_on_conflict
  ): packNftContract_mutation_response

  """
  insert data into the table: "packNftContractEventPass"
  """
  insert_packNftContractEventPass(
    """the rows to be inserted"""
    objects: [packNftContractEventPass_insert_input!]!

    """upsert condition"""
    on_conflict: packNftContractEventPass_on_conflict
  ): packNftContractEventPass_mutation_response

  """
  insert a single row into the table: "packNftContractEventPass"
  """
  insert_packNftContractEventPass_one(
    """the row to be inserted"""
    object: packNftContractEventPass_insert_input!

    """upsert condition"""
    on_conflict: packNftContractEventPass_on_conflict
  ): packNftContractEventPass

  """
  insert a single row into the table: "packNftContract"
  """
  insert_packNftContract_one(
    """the row to be inserted"""
    object: packNftContract_insert_input!

    """upsert condition"""
    on_conflict: packNftContract_on_conflict
  ): packNftContract

  """
  insert data into the table: "packNftSupply"
  """
  insert_packNftSupply(
    """the rows to be inserted"""
    objects: [packNftSupply_insert_input!]!

    """upsert condition"""
    on_conflict: packNftSupply_on_conflict
  ): packNftSupply_mutation_response

  """
  insert a single row into the table: "packNftSupply"
  """
  insert_packNftSupply_one(
    """the row to be inserted"""
    object: packNftSupply_insert_input!

    """upsert condition"""
    on_conflict: packNftSupply_on_conflict
  ): packNftSupply

  """
  insert data into the table: "packOrderSums"
  """
  insert_packOrderSums(
    """the rows to be inserted"""
    objects: [packOrderSums_insert_input!]!

    """upsert condition"""
    on_conflict: packOrderSums_on_conflict
  ): packOrderSums_mutation_response

  """
  insert a single row into the table: "packOrderSums"
  """
  insert_packOrderSums_one(
    """the row to be inserted"""
    object: packOrderSums_insert_input!

    """upsert condition"""
    on_conflict: packOrderSums_on_conflict
  ): packOrderSums

  """
  insert data into the table: "passAmount"
  """
  insert_passAmount(
    """the rows to be inserted"""
    objects: [passAmount_insert_input!]!

    """upsert condition"""
    on_conflict: passAmount_on_conflict
  ): passAmount_mutation_response

  """
  insert a single row into the table: "passAmount"
  """
  insert_passAmount_one(
    """the row to be inserted"""
    object: passAmount_insert_input!

    """upsert condition"""
    on_conflict: passAmount_on_conflict
  ): passAmount

  """
  insert data into the table: "passPricing"
  """
  insert_passPricing(
    """the rows to be inserted"""
    objects: [passPricing_insert_input!]!

    """upsert condition"""
    on_conflict: passPricing_on_conflict
  ): passPricing_mutation_response

  """
  insert a single row into the table: "passPricing"
  """
  insert_passPricing_one(
    """the row to be inserted"""
    object: passPricing_insert_input!

    """upsert condition"""
    on_conflict: passPricing_on_conflict
  ): passPricing

  """
  insert data into the table: "pendingOrder"
  """
  insert_pendingOrder(
    """the rows to be inserted"""
    objects: [pendingOrder_insert_input!]!

    """upsert condition"""
    on_conflict: pendingOrder_on_conflict
  ): pendingOrder_mutation_response

  """
  insert a single row into the table: "pendingOrder"
  """
  insert_pendingOrder_one(
    """the row to be inserted"""
    object: pendingOrder_insert_input!

    """upsert condition"""
    on_conflict: pendingOrder_on_conflict
  ): pendingOrder

  """
  insert data into the table: "roleAssignment"
  """
  insert_roleAssignment(
    """the rows to be inserted"""
    objects: [roleAssignment_insert_input!]!

    """upsert condition"""
    on_conflict: roleAssignment_on_conflict
  ): roleAssignment_mutation_response

  """
  insert a single row into the table: "roleAssignment"
  """
  insert_roleAssignment_one(
    """the row to be inserted"""
    object: roleAssignment_insert_input!

    """upsert condition"""
    on_conflict: roleAssignment_on_conflict
  ): roleAssignment

  """
  insert data into the table: "roles"
  """
  insert_roles(
    """the rows to be inserted"""
    objects: [roles_insert_input!]!

    """upsert condition"""
    on_conflict: roles_on_conflict
  ): roles_mutation_response

  """
  insert a single row into the table: "roles"
  """
  insert_roles_one(
    """the row to be inserted"""
    object: roles_insert_input!

    """upsert condition"""
    on_conflict: roles_on_conflict
  ): roles

  """
  insert data into the table: "stripeCheckoutSession"
  """
  insert_stripeCheckoutSession(
    """the rows to be inserted"""
    objects: [stripeCheckoutSession_insert_input!]!

    """upsert condition"""
    on_conflict: stripeCheckoutSession_on_conflict
  ): stripeCheckoutSession_mutation_response

  """
  insert data into the table: "stripeCheckoutSessionType"
  """
  insert_stripeCheckoutSessionType(
    """the rows to be inserted"""
    objects: [stripeCheckoutSessionType_insert_input!]!

    """upsert condition"""
    on_conflict: stripeCheckoutSessionType_on_conflict
  ): stripeCheckoutSessionType_mutation_response

  """
  insert a single row into the table: "stripeCheckoutSessionType"
  """
  insert_stripeCheckoutSessionType_one(
    """the row to be inserted"""
    object: stripeCheckoutSessionType_insert_input!

    """upsert condition"""
    on_conflict: stripeCheckoutSessionType_on_conflict
  ): stripeCheckoutSessionType

  """
  insert a single row into the table: "stripeCheckoutSession"
  """
  insert_stripeCheckoutSession_one(
    """the row to be inserted"""
    object: stripeCheckoutSession_insert_input!

    """upsert condition"""
    on_conflict: stripeCheckoutSession_on_conflict
  ): stripeCheckoutSession

  """
  insert data into the table: "stripeCustomer"
  """
  insert_stripeCustomer(
    """the rows to be inserted"""
    objects: [stripeCustomer_insert_input!]!

    """upsert condition"""
    on_conflict: stripeCustomer_on_conflict
  ): stripeCustomer_mutation_response

  """
  insert a single row into the table: "stripeCustomer"
  """
  insert_stripeCustomer_one(
    """the row to be inserted"""
    object: stripeCustomer_insert_input!

    """upsert condition"""
    on_conflict: stripeCustomer_on_conflict
  ): stripeCustomer

  """
  insert data into the table: "timezone"
  """
  insert_timezone(
    """the rows to be inserted"""
    objects: [timezone_insert_input!]!

    """upsert condition"""
    on_conflict: timezone_on_conflict
  ): timezone_mutation_response

  """
  insert a single row into the table: "timezone"
  """
  insert_timezone_one(
    """the row to be inserted"""
    object: timezone_insert_input!

    """upsert condition"""
    on_conflict: timezone_on_conflict
  ): timezone

  """Publish one asset"""
  publishAsset(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: AssetWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Asset

  """Publish one event"""
  publishEvent(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: EventWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Event

  """Publish one eventPass"""
  publishEventPass(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: EventPassWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): EventPass

  """Publish one eventPassDelayedRevealed"""
  publishEventPassDelayedRevealed(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: EventPassDelayedRevealedWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): EventPassDelayedRevealed

  """Publish many Asset documents"""
  publishManyAssets(
    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: AssetManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): BatchPayload!

  """Publish many Asset documents"""
  publishManyAssetsConnection(
    after: ID
    before: ID
    first: Int

    """Stage to find matching documents in"""
    from: Stage = DRAFT
    last: Int

    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true
    skip: Int

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: AssetManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): AssetConnection!

  """Publish many EventPass documents"""
  publishManyEventPasses(
    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: EventPassManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): BatchPayload!

  """Publish many EventPass documents"""
  publishManyEventPassesConnection(
    after: ID
    before: ID
    first: Int

    """Stage to find matching documents in"""
    from: Stage = DRAFT
    last: Int

    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true
    skip: Int

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: EventPassManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): EventPassConnection!

  """Publish many EventPassDelayedRevealed documents"""
  publishManyEventPassesDelayedRevealed(
    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: EventPassDelayedRevealedManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): BatchPayload!

  """Publish many EventPassDelayedRevealed documents"""
  publishManyEventPassesDelayedRevealedConnection(
    after: ID
    before: ID
    first: Int

    """Stage to find matching documents in"""
    from: Stage = DRAFT
    last: Int

    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true
    skip: Int

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: EventPassDelayedRevealedManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): EventPassDelayedRevealedConnection!

  """Publish many Event documents"""
  publishManyEvents(
    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: EventManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): BatchPayload!

  """Publish many Event documents"""
  publishManyEventsConnection(
    after: ID
    before: ID
    first: Int

    """Stage to find matching documents in"""
    from: Stage = DRAFT
    last: Int

    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true
    skip: Int

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: EventManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): EventConnection!

  """Publish many Organizer documents"""
  publishManyOrganizers(
    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: OrganizerManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): BatchPayload!

  """Publish many Organizer documents"""
  publishManyOrganizersConnection(
    after: ID
    before: ID
    first: Int

    """Stage to find matching documents in"""
    from: Stage = DRAFT
    last: Int

    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true
    skip: Int

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: OrganizerManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): OrganizerConnection!

  """Publish many Pack documents"""
  publishManyPacks(
    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: PackManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): BatchPayload!

  """Publish many Pack documents"""
  publishManyPacksConnection(
    after: ID
    before: ID
    first: Int

    """Stage to find matching documents in"""
    from: Stage = DRAFT
    last: Int

    """Document localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true
    skip: Int

    """Stages to publish documents to"""
    to: [Stage!]! = [PUBLISHED]

    """Identifies documents in each stage to be published"""
    where: PackManyWhereInput

    """Whether to include the default locale when publishBase is true"""
    withDefaultLocale: Boolean = true
  ): PackConnection!

  """Publish one organizer"""
  publishOrganizer(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: OrganizerWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Organizer

  """Publish one pack"""
  publishPack(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: PackWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Pack

  """Schedule to publish one asset"""
  schedulePublishAsset(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: AssetWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Asset

  """Schedule to publish one event"""
  schedulePublishEvent(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: EventWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Event

  """Schedule to publish one eventPass"""
  schedulePublishEventPass(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: EventPassWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): EventPass

  """Schedule to publish one eventPassDelayedRevealed"""
  schedulePublishEventPassDelayedRevealed(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: EventPassDelayedRevealedWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): EventPassDelayedRevealed

  """Schedule to publish one organizer"""
  schedulePublishOrganizer(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: OrganizerWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Organizer

  """Schedule to publish one pack"""
  schedulePublishPack(
    """Optional localizations to publish"""
    locales: [Locale!]

    """Whether to publish the base document"""
    publishBase: Boolean = true

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """Publishing target stage"""
    to: [Stage!]! = [PUBLISHED]

    """Document to publish"""
    where: PackWhereUniqueInput!

    """Whether to include the default locale when publishBase is set"""
    withDefaultLocale: Boolean = true
  ): Pack

  """
  Unpublish one asset from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  scheduleUnpublishAsset(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: AssetWhereUniqueInput!
  ): Asset

  """
  Unpublish one event from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  scheduleUnpublishEvent(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: EventWhereUniqueInput!
  ): Event

  """
  Unpublish one eventPass from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  scheduleUnpublishEventPass(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: EventPassWhereUniqueInput!
  ): EventPass

  """
  Unpublish one eventPassDelayedRevealed from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  scheduleUnpublishEventPassDelayedRevealed(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: EventPassDelayedRevealedWhereUniqueInput!
  ): EventPassDelayedRevealed

  """
  Unpublish one organizer from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  scheduleUnpublishOrganizer(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: OrganizerWhereUniqueInput!
  ): Organizer

  """
  Unpublish one pack from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  scheduleUnpublishPack(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Release at point in time, will create new release containing this operation
    """
    releaseAt: DateTime

    """Optionally attach this scheduled operation to an existing release"""
    releaseId: String

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: PackWhereUniqueInput!
  ): Pack

  """
  Unpublish one asset from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  unpublishAsset(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: AssetWhereUniqueInput!
  ): Asset

  """
  Unpublish one event from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  unpublishEvent(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: EventWhereUniqueInput!
  ): Event

  """
  Unpublish one eventPass from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  unpublishEventPass(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: EventPassWhereUniqueInput!
  ): EventPass

  """
  Unpublish one eventPassDelayedRevealed from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  unpublishEventPassDelayedRevealed(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: EventPassDelayedRevealedWhereUniqueInput!
  ): EventPassDelayedRevealed

  """Unpublish many Asset documents"""
  unpublishManyAssets(
    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]

    """Locales to unpublish"""
    locales: [Locale!]

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in each stage"""
    where: AssetManyWhereInput
  ): BatchPayload!

  """
  Find many Asset documents that match criteria in specified stage and unpublish from target stages
  """
  unpublishManyAssetsConnection(
    after: ID
    before: ID
    first: Int

    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]
    last: Int

    """Locales to unpublish"""
    locales: [Locale!]
    skip: Int

    """Stage to find matching documents in"""
    stage: Stage = DRAFT

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in draft stage"""
    where: AssetManyWhereInput
  ): AssetConnection!

  """Unpublish many EventPass documents"""
  unpublishManyEventPasses(
    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]

    """Locales to unpublish"""
    locales: [Locale!]

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in each stage"""
    where: EventPassManyWhereInput
  ): BatchPayload!

  """
  Find many EventPass documents that match criteria in specified stage and unpublish from target stages
  """
  unpublishManyEventPassesConnection(
    after: ID
    before: ID
    first: Int

    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]
    last: Int

    """Locales to unpublish"""
    locales: [Locale!]
    skip: Int

    """Stage to find matching documents in"""
    stage: Stage = DRAFT

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in draft stage"""
    where: EventPassManyWhereInput
  ): EventPassConnection!

  """Unpublish many EventPassDelayedRevealed documents"""
  unpublishManyEventPassesDelayedRevealed(
    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]

    """Locales to unpublish"""
    locales: [Locale!]

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in each stage"""
    where: EventPassDelayedRevealedManyWhereInput
  ): BatchPayload!

  """
  Find many EventPassDelayedRevealed documents that match criteria in specified stage and unpublish from target stages
  """
  unpublishManyEventPassesDelayedRevealedConnection(
    after: ID
    before: ID
    first: Int

    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]
    last: Int

    """Locales to unpublish"""
    locales: [Locale!]
    skip: Int

    """Stage to find matching documents in"""
    stage: Stage = DRAFT

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in draft stage"""
    where: EventPassDelayedRevealedManyWhereInput
  ): EventPassDelayedRevealedConnection!

  """Unpublish many Event documents"""
  unpublishManyEvents(
    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]

    """Locales to unpublish"""
    locales: [Locale!]

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in each stage"""
    where: EventManyWhereInput
  ): BatchPayload!

  """
  Find many Event documents that match criteria in specified stage and unpublish from target stages
  """
  unpublishManyEventsConnection(
    after: ID
    before: ID
    first: Int

    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]
    last: Int

    """Locales to unpublish"""
    locales: [Locale!]
    skip: Int

    """Stage to find matching documents in"""
    stage: Stage = DRAFT

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in draft stage"""
    where: EventManyWhereInput
  ): EventConnection!

  """Unpublish many Organizer documents"""
  unpublishManyOrganizers(
    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]

    """Locales to unpublish"""
    locales: [Locale!]

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in each stage"""
    where: OrganizerManyWhereInput
  ): BatchPayload!

  """
  Find many Organizer documents that match criteria in specified stage and unpublish from target stages
  """
  unpublishManyOrganizersConnection(
    after: ID
    before: ID
    first: Int

    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]
    last: Int

    """Locales to unpublish"""
    locales: [Locale!]
    skip: Int

    """Stage to find matching documents in"""
    stage: Stage = DRAFT

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in draft stage"""
    where: OrganizerManyWhereInput
  ): OrganizerConnection!

  """Unpublish many Pack documents"""
  unpublishManyPacks(
    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]

    """Locales to unpublish"""
    locales: [Locale!]

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in each stage"""
    where: PackManyWhereInput
  ): BatchPayload!

  """
  Find many Pack documents that match criteria in specified stage and unpublish from target stages
  """
  unpublishManyPacksConnection(
    after: ID
    before: ID
    first: Int

    """Stages to unpublish documents from"""
    from: [Stage!]! = [PUBLISHED]
    last: Int

    """Locales to unpublish"""
    locales: [Locale!]
    skip: Int

    """Stage to find matching documents in"""
    stage: Stage = DRAFT

    """Whether to unpublish the base document and default localization"""
    unpublishBase: Boolean = true

    """Identifies documents in draft stage"""
    where: PackManyWhereInput
  ): PackConnection!

  """
  Unpublish one organizer from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  unpublishOrganizer(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: OrganizerWhereUniqueInput!
  ): Organizer

  """
  Unpublish one pack from selected stages. Unpublish either the complete document with its relations, localizations and base data or specific localizations only.
  """
  unpublishPack(
    """Stages to unpublish document from"""
    from: [Stage!]! = [PUBLISHED]

    """
    Optional locales to unpublish. Unpublishing the default locale will completely remove the document from the selected stages
    """
    locales: [Locale!]

    """
    Unpublish complete document including default localization and relations from stages. Can be disabled.
    """
    unpublishBase: Boolean = true

    """Document to unpublish"""
    where: PackWhereUniqueInput!
  ): Pack

  """Update one asset"""
  updateAsset(data: AssetUpdateInput!, where: AssetWhereUniqueInput!): Asset

  """Update one event"""
  updateEvent(data: EventUpdateInput!, where: EventWhereUniqueInput!): Event

  """Update one eventPass"""
  updateEventPass(data: EventPassUpdateInput!, where: EventPassWhereUniqueInput!): EventPass

  """Update one eventPassDelayedRevealed"""
  updateEventPassDelayedRevealed(data: EventPassDelayedRevealedUpdateInput!, where: EventPassDelayedRevealedWhereUniqueInput!): EventPassDelayedRevealed

  """Update many assets"""
  updateManyAssets(
    """Updates to document content"""
    data: AssetUpdateManyInput!

    """Documents to apply update on"""
    where: AssetManyWhereInput
  ): BatchPayload!

  """Update many Asset documents"""
  updateManyAssetsConnection(
    after: ID
    before: ID

    """Updates to document content"""
    data: AssetUpdateManyInput!
    first: Int
    last: Int
    skip: Int

    """Documents to apply update on"""
    where: AssetManyWhereInput
  ): AssetConnection!

  """Update many eventPasses"""
  updateManyEventPasses(
    """Updates to document content"""
    data: EventPassUpdateManyInput!

    """Documents to apply update on"""
    where: EventPassManyWhereInput
  ): BatchPayload!

  """Update many EventPass documents"""
  updateManyEventPassesConnection(
    after: ID
    before: ID

    """Updates to document content"""
    data: EventPassUpdateManyInput!
    first: Int
    last: Int
    skip: Int

    """Documents to apply update on"""
    where: EventPassManyWhereInput
  ): EventPassConnection!

  """Update many eventPassesDelayedRevealed"""
  updateManyEventPassesDelayedRevealed(
    """Updates to document content"""
    data: EventPassDelayedRevealedUpdateManyInput!

    """Documents to apply update on"""
    where: EventPassDelayedRevealedManyWhereInput
  ): BatchPayload!

  """Update many EventPassDelayedRevealed documents"""
  updateManyEventPassesDelayedRevealedConnection(
    after: ID
    before: ID

    """Updates to document content"""
    data: EventPassDelayedRevealedUpdateManyInput!
    first: Int
    last: Int
    skip: Int

    """Documents to apply update on"""
    where: EventPassDelayedRevealedManyWhereInput
  ): EventPassDelayedRevealedConnection!

  """Update many events"""
  updateManyEvents(
    """Updates to document content"""
    data: EventUpdateManyInput!

    """Documents to apply update on"""
    where: EventManyWhereInput
  ): BatchPayload!

  """Update many Event documents"""
  updateManyEventsConnection(
    after: ID
    before: ID

    """Updates to document content"""
    data: EventUpdateManyInput!
    first: Int
    last: Int
    skip: Int

    """Documents to apply update on"""
    where: EventManyWhereInput
  ): EventConnection!

  """Update many organizers"""
  updateManyOrganizers(
    """Updates to document content"""
    data: OrganizerUpdateManyInput!

    """Documents to apply update on"""
    where: OrganizerManyWhereInput
  ): BatchPayload!

  """Update many Organizer documents"""
  updateManyOrganizersConnection(
    after: ID
    before: ID

    """Updates to document content"""
    data: OrganizerUpdateManyInput!
    first: Int
    last: Int
    skip: Int

    """Documents to apply update on"""
    where: OrganizerManyWhereInput
  ): OrganizerConnection!

  """Update many packs"""
  updateManyPacks(
    """Updates to document content"""
    data: PackUpdateManyInput!

    """Documents to apply update on"""
    where: PackManyWhereInput
  ): BatchPayload!

  """Update many Pack documents"""
  updateManyPacksConnection(
    after: ID
    before: ID

    """Updates to document content"""
    data: PackUpdateManyInput!
    first: Int
    last: Int
    skip: Int

    """Documents to apply update on"""
    where: PackManyWhereInput
  ): PackConnection!

  """Update one organizer"""
  updateOrganizer(data: OrganizerUpdateInput!, where: OrganizerWhereUniqueInput!): Organizer

  """Update one pack"""
  updatePack(data: PackUpdateInput!, where: PackWhereUniqueInput!): Pack

  """Update one scheduledRelease"""
  updateScheduledRelease(data: ScheduledReleaseUpdateInput!, where: ScheduledReleaseWhereUniqueInput!): ScheduledRelease

  """
  update data of the table: "account"
  """
  update_account(
    """sets the columns of the filtered rows to the given values"""
    _set: account_set_input

    """filter the rows which have to be updated"""
    where: account_bool_exp!
  ): account_mutation_response

  """
  update single row of the table: "account"
  """
  update_account_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: account_set_input
    pk_columns: account_pk_columns_input!
  ): account

  """
  update multiples rows of table: "account"
  """
  update_account_many(
    """updates to execute, in order"""
    updates: [account_updates!]!
  ): [account_mutation_response]

  """
  update data of the table: "currency"
  """
  update_currency(
    """sets the columns of the filtered rows to the given values"""
    _set: currency_set_input

    """filter the rows which have to be updated"""
    where: currency_bool_exp!
  ): currency_mutation_response

  """
  update single row of the table: "currency"
  """
  update_currency_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: currency_set_input
    pk_columns: currency_pk_columns_input!
  ): currency

  """
  update multiples rows of table: "currency"
  """
  update_currency_many(
    """updates to execute, in order"""
    updates: [currency_updates!]!
  ): [currency_mutation_response]

  """
  update data of the table: "eventParameters"
  """
  update_eventParameters(
    """sets the columns of the filtered rows to the given values"""
    _set: eventParameters_set_input

    """filter the rows which have to be updated"""
    where: eventParameters_bool_exp!
  ): eventParameters_mutation_response

  """
  update single row of the table: "eventParameters"
  """
  update_eventParameters_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: eventParameters_set_input
    pk_columns: eventParameters_pk_columns_input!
  ): eventParameters

  """
  update multiples rows of table: "eventParameters"
  """
  update_eventParameters_many(
    """updates to execute, in order"""
    updates: [eventParameters_updates!]!
  ): [eventParameters_mutation_response]

  """
  update data of the table: "eventPassNft"
  """
  update_eventPassNft(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: eventPassNft_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: eventPassNft_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: eventPassNft_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: eventPassNft_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: eventPassNft_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: eventPassNft_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: eventPassNft_set_input

    """filter the rows which have to be updated"""
    where: eventPassNft_bool_exp!
  ): eventPassNft_mutation_response

  """
  update data of the table: "eventPassNftContract"
  """
  update_eventPassNftContract(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassNftContract_set_input

    """filter the rows which have to be updated"""
    where: eventPassNftContract_bool_exp!
  ): eventPassNftContract_mutation_response

  """
  update data of the table: "eventPassNftContractType"
  """
  update_eventPassNftContractType(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassNftContractType_set_input

    """filter the rows which have to be updated"""
    where: eventPassNftContractType_bool_exp!
  ): eventPassNftContractType_mutation_response

  """
  update single row of the table: "eventPassNftContractType"
  """
  update_eventPassNftContractType_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassNftContractType_set_input
    pk_columns: eventPassNftContractType_pk_columns_input!
  ): eventPassNftContractType

  """
  update multiples rows of table: "eventPassNftContractType"
  """
  update_eventPassNftContractType_many(
    """updates to execute, in order"""
    updates: [eventPassNftContractType_updates!]!
  ): [eventPassNftContractType_mutation_response]

  """
  update single row of the table: "eventPassNftContract"
  """
  update_eventPassNftContract_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassNftContract_set_input
    pk_columns: eventPassNftContract_pk_columns_input!
  ): eventPassNftContract

  """
  update multiples rows of table: "eventPassNftContract"
  """
  update_eventPassNftContract_many(
    """updates to execute, in order"""
    updates: [eventPassNftContract_updates!]!
  ): [eventPassNftContract_mutation_response]

  """
  update single row of the table: "eventPassNft"
  """
  update_eventPassNft_by_pk(
    """append existing jsonb value of filtered columns with new jsonb value"""
    _append: eventPassNft_append_input

    """
    delete the field or element with specified path (for JSON arrays, negative integers count from the end)
    """
    _delete_at_path: eventPassNft_delete_at_path_input

    """
    delete the array element with specified index (negative integers count from the end). throws an error if top level container is not an array
    """
    _delete_elem: eventPassNft_delete_elem_input

    """
    delete key/value pair or string element. key/value pairs are matched based on their key value
    """
    _delete_key: eventPassNft_delete_key_input

    """increments the numeric columns with given value of the filtered values"""
    _inc: eventPassNft_inc_input

    """prepend existing jsonb value of filtered columns with new jsonb value"""
    _prepend: eventPassNft_prepend_input

    """sets the columns of the filtered rows to the given values"""
    _set: eventPassNft_set_input
    pk_columns: eventPassNft_pk_columns_input!
  ): eventPassNft

  """
  update multiples rows of table: "eventPassNft"
  """
  update_eventPassNft_many(
    """updates to execute, in order"""
    updates: [eventPassNft_updates!]!
  ): [eventPassNft_mutation_response]

  """
  update data of the table: "eventPassOrderSums"
  """
  update_eventPassOrderSums(
    """increments the numeric columns with given value of the filtered values"""
    _inc: eventPassOrderSums_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: eventPassOrderSums_set_input

    """filter the rows which have to be updated"""
    where: eventPassOrderSums_bool_exp!
  ): eventPassOrderSums_mutation_response

  """
  update single row of the table: "eventPassOrderSums"
  """
  update_eventPassOrderSums_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: eventPassOrderSums_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: eventPassOrderSums_set_input
    pk_columns: eventPassOrderSums_pk_columns_input!
  ): eventPassOrderSums

  """
  update multiples rows of table: "eventPassOrderSums"
  """
  update_eventPassOrderSums_many(
    """updates to execute, in order"""
    updates: [eventPassOrderSums_updates!]!
  ): [eventPassOrderSums_mutation_response]

  """
  update data of the table: "eventPassType"
  """
  update_eventPassType(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassType_set_input

    """filter the rows which have to be updated"""
    where: eventPassType_bool_exp!
  ): eventPassType_mutation_response

  """
  update single row of the table: "eventPassType"
  """
  update_eventPassType_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassType_set_input
    pk_columns: eventPassType_pk_columns_input!
  ): eventPassType

  """
  update multiples rows of table: "eventPassType"
  """
  update_eventPassType_many(
    """updates to execute, in order"""
    updates: [eventPassType_updates!]!
  ): [eventPassType_mutation_response]

  """
  update data of the table: "eventPassValidationType"
  """
  update_eventPassValidationType(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassValidationType_set_input

    """filter the rows which have to be updated"""
    where: eventPassValidationType_bool_exp!
  ): eventPassValidationType_mutation_response

  """
  update single row of the table: "eventPassValidationType"
  """
  update_eventPassValidationType_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: eventPassValidationType_set_input
    pk_columns: eventPassValidationType_pk_columns_input!
  ): eventPassValidationType

  """
  update multiples rows of table: "eventPassValidationType"
  """
  update_eventPassValidationType_many(
    """updates to execute, in order"""
    updates: [eventPassValidationType_updates!]!
  ): [eventPassValidationType_mutation_response]

  """
  update data of the table: "eventStatus"
  """
  update_eventStatus(
    """sets the columns of the filtered rows to the given values"""
    _set: eventStatus_set_input

    """filter the rows which have to be updated"""
    where: eventStatus_bool_exp!
  ): eventStatus_mutation_response

  """
  update single row of the table: "eventStatus"
  """
  update_eventStatus_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: eventStatus_set_input
    pk_columns: eventStatus_pk_columns_input!
  ): eventStatus

  """
  update multiples rows of table: "eventStatus"
  """
  update_eventStatus_many(
    """updates to execute, in order"""
    updates: [eventStatus_updates!]!
  ): [eventStatus_mutation_response]

  """
  update data of the table: "follow"
  """
  update_follow(
    """sets the columns of the filtered rows to the given values"""
    _set: follow_set_input

    """filter the rows which have to be updated"""
    where: follow_bool_exp!
  ): follow_mutation_response

  """
  update single row of the table: "follow"
  """
  update_follow_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: follow_set_input
    pk_columns: follow_pk_columns_input!
  ): follow

  """
  update multiples rows of table: "follow"
  """
  update_follow_many(
    """updates to execute, in order"""
    updates: [follow_updates!]!
  ): [follow_mutation_response]

  """
  update data of the table: "kyc"
  """
  update_kyc(
    """sets the columns of the filtered rows to the given values"""
    _set: kyc_set_input

    """filter the rows which have to be updated"""
    where: kyc_bool_exp!
  ): kyc_mutation_response

  """
  update data of the table: "kycLevelName"
  """
  update_kycLevelName(
    """sets the columns of the filtered rows to the given values"""
    _set: kycLevelName_set_input

    """filter the rows which have to be updated"""
    where: kycLevelName_bool_exp!
  ): kycLevelName_mutation_response

  """
  update single row of the table: "kycLevelName"
  """
  update_kycLevelName_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: kycLevelName_set_input
    pk_columns: kycLevelName_pk_columns_input!
  ): kycLevelName

  """
  update multiples rows of table: "kycLevelName"
  """
  update_kycLevelName_many(
    """updates to execute, in order"""
    updates: [kycLevelName_updates!]!
  ): [kycLevelName_mutation_response]

  """
  update data of the table: "kycStatus"
  """
  update_kycStatus(
    """sets the columns of the filtered rows to the given values"""
    _set: kycStatus_set_input

    """filter the rows which have to be updated"""
    where: kycStatus_bool_exp!
  ): kycStatus_mutation_response

  """
  update single row of the table: "kycStatus"
  """
  update_kycStatus_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: kycStatus_set_input
    pk_columns: kycStatus_pk_columns_input!
  ): kycStatus

  """
  update multiples rows of table: "kycStatus"
  """
  update_kycStatus_many(
    """updates to execute, in order"""
    updates: [kycStatus_updates!]!
  ): [kycStatus_mutation_response]

  """
  update single row of the table: "kyc"
  """
  update_kyc_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: kyc_set_input
    pk_columns: kyc_pk_columns_input!
  ): kyc

  """
  update multiples rows of table: "kyc"
  """
  update_kyc_many(
    """updates to execute, in order"""
    updates: [kyc_updates!]!
  ): [kyc_mutation_response]

  """
  update data of the table: "lotteryParameters"
  """
  update_lotteryParameters(
    """sets the columns of the filtered rows to the given values"""
    _set: lotteryParameters_set_input

    """filter the rows which have to be updated"""
    where: lotteryParameters_bool_exp!
  ): lotteryParameters_mutation_response

  """
  update single row of the table: "lotteryParameters"
  """
  update_lotteryParameters_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: lotteryParameters_set_input
    pk_columns: lotteryParameters_pk_columns_input!
  ): lotteryParameters

  """
  update multiples rows of table: "lotteryParameters"
  """
  update_lotteryParameters_many(
    """updates to execute, in order"""
    updates: [lotteryParameters_updates!]!
  ): [lotteryParameters_mutation_response]

  """
  update data of the table: "lotteryStatus"
  """
  update_lotteryStatus(
    """sets the columns of the filtered rows to the given values"""
    _set: lotteryStatus_set_input

    """filter the rows which have to be updated"""
    where: lotteryStatus_bool_exp!
  ): lotteryStatus_mutation_response

  """
  update single row of the table: "lotteryStatus"
  """
  update_lotteryStatus_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: lotteryStatus_set_input
    pk_columns: lotteryStatus_pk_columns_input!
  ): lotteryStatus

  """
  update multiples rows of table: "lotteryStatus"
  """
  update_lotteryStatus_many(
    """updates to execute, in order"""
    updates: [lotteryStatus_updates!]!
  ): [lotteryStatus_mutation_response]

  """
  update data of the table: "nftTransfer"
  """
  update_nftTransfer(
    """increments the numeric columns with given value of the filtered values"""
    _inc: nftTransfer_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: nftTransfer_set_input

    """filter the rows which have to be updated"""
    where: nftTransfer_bool_exp!
  ): nftTransfer_mutation_response

  """
  update single row of the table: "nftTransfer"
  """
  update_nftTransfer_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: nftTransfer_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: nftTransfer_set_input
    pk_columns: nftTransfer_pk_columns_input!
  ): nftTransfer

  """
  update multiples rows of table: "nftTransfer"
  """
  update_nftTransfer_many(
    """updates to execute, in order"""
    updates: [nftTransfer_updates!]!
  ): [nftTransfer_mutation_response]

  """
  update data of the table: "order"
  """
  update_order(
    """increments the numeric columns with given value of the filtered values"""
    _inc: order_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: order_set_input

    """filter the rows which have to be updated"""
    where: order_bool_exp!
  ): order_mutation_response

  """
  update data of the table: "orderStatus"
  """
  update_orderStatus(
    """sets the columns of the filtered rows to the given values"""
    _set: orderStatus_set_input

    """filter the rows which have to be updated"""
    where: orderStatus_bool_exp!
  ): orderStatus_mutation_response

  """
  update single row of the table: "orderStatus"
  """
  update_orderStatus_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: orderStatus_set_input
    pk_columns: orderStatus_pk_columns_input!
  ): orderStatus

  """
  update multiples rows of table: "orderStatus"
  """
  update_orderStatus_many(
    """updates to execute, in order"""
    updates: [orderStatus_updates!]!
  ): [orderStatus_mutation_response]

  """
  update single row of the table: "order"
  """
  update_order_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: order_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: order_set_input
    pk_columns: order_pk_columns_input!
  ): order

  """
  update multiples rows of table: "order"
  """
  update_order_many(
    """updates to execute, in order"""
    updates: [order_updates!]!
  ): [order_mutation_response]

  """
  update data of the table: "packEventPassNft"
  """
  update_packEventPassNft(
    """sets the columns of the filtered rows to the given values"""
    _set: packEventPassNft_set_input

    """filter the rows which have to be updated"""
    where: packEventPassNft_bool_exp!
  ): packEventPassNft_mutation_response

  """
  update single row of the table: "packEventPassNft"
  """
  update_packEventPassNft_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: packEventPassNft_set_input
    pk_columns: packEventPassNft_pk_columns_input!
  ): packEventPassNft

  """
  update multiples rows of table: "packEventPassNft"
  """
  update_packEventPassNft_many(
    """updates to execute, in order"""
    updates: [packEventPassNft_updates!]!
  ): [packEventPassNft_mutation_response]

  """
  update data of the table: "packNftContract"
  """
  update_packNftContract(
    """increments the numeric columns with given value of the filtered values"""
    _inc: packNftContract_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: packNftContract_set_input

    """filter the rows which have to be updated"""
    where: packNftContract_bool_exp!
  ): packNftContract_mutation_response

  """
  update data of the table: "packNftContractEventPass"
  """
  update_packNftContractEventPass(
    """increments the numeric columns with given value of the filtered values"""
    _inc: packNftContractEventPass_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: packNftContractEventPass_set_input

    """filter the rows which have to be updated"""
    where: packNftContractEventPass_bool_exp!
  ): packNftContractEventPass_mutation_response

  """
  update single row of the table: "packNftContractEventPass"
  """
  update_packNftContractEventPass_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: packNftContractEventPass_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: packNftContractEventPass_set_input
    pk_columns: packNftContractEventPass_pk_columns_input!
  ): packNftContractEventPass

  """
  update multiples rows of table: "packNftContractEventPass"
  """
  update_packNftContractEventPass_many(
    """updates to execute, in order"""
    updates: [packNftContractEventPass_updates!]!
  ): [packNftContractEventPass_mutation_response]

  """
  update single row of the table: "packNftContract"
  """
  update_packNftContract_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: packNftContract_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: packNftContract_set_input
    pk_columns: packNftContract_pk_columns_input!
  ): packNftContract

  """
  update multiples rows of table: "packNftContract"
  """
  update_packNftContract_many(
    """updates to execute, in order"""
    updates: [packNftContract_updates!]!
  ): [packNftContract_mutation_response]

  """
  update data of the table: "packNftSupply"
  """
  update_packNftSupply(
    """sets the columns of the filtered rows to the given values"""
    _set: packNftSupply_set_input

    """filter the rows which have to be updated"""
    where: packNftSupply_bool_exp!
  ): packNftSupply_mutation_response

  """
  update single row of the table: "packNftSupply"
  """
  update_packNftSupply_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: packNftSupply_set_input
    pk_columns: packNftSupply_pk_columns_input!
  ): packNftSupply

  """
  update multiples rows of table: "packNftSupply"
  """
  update_packNftSupply_many(
    """updates to execute, in order"""
    updates: [packNftSupply_updates!]!
  ): [packNftSupply_mutation_response]

  """
  update data of the table: "packOrderSums"
  """
  update_packOrderSums(
    """increments the numeric columns with given value of the filtered values"""
    _inc: packOrderSums_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: packOrderSums_set_input

    """filter the rows which have to be updated"""
    where: packOrderSums_bool_exp!
  ): packOrderSums_mutation_response

  """
  update single row of the table: "packOrderSums"
  """
  update_packOrderSums_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: packOrderSums_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: packOrderSums_set_input
    pk_columns: packOrderSums_pk_columns_input!
  ): packOrderSums

  """
  update multiples rows of table: "packOrderSums"
  """
  update_packOrderSums_many(
    """updates to execute, in order"""
    updates: [packOrderSums_updates!]!
  ): [packOrderSums_mutation_response]

  """
  update data of the table: "passAmount"
  """
  update_passAmount(
    """increments the numeric columns with given value of the filtered values"""
    _inc: passAmount_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: passAmount_set_input

    """filter the rows which have to be updated"""
    where: passAmount_bool_exp!
  ): passAmount_mutation_response

  """
  update single row of the table: "passAmount"
  """
  update_passAmount_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: passAmount_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: passAmount_set_input
    pk_columns: passAmount_pk_columns_input!
  ): passAmount

  """
  update multiples rows of table: "passAmount"
  """
  update_passAmount_many(
    """updates to execute, in order"""
    updates: [passAmount_updates!]!
  ): [passAmount_mutation_response]

  """
  update data of the table: "passPricing"
  """
  update_passPricing(
    """increments the numeric columns with given value of the filtered values"""
    _inc: passPricing_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: passPricing_set_input

    """filter the rows which have to be updated"""
    where: passPricing_bool_exp!
  ): passPricing_mutation_response

  """
  update single row of the table: "passPricing"
  """
  update_passPricing_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: passPricing_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: passPricing_set_input
    pk_columns: passPricing_pk_columns_input!
  ): passPricing

  """
  update multiples rows of table: "passPricing"
  """
  update_passPricing_many(
    """updates to execute, in order"""
    updates: [passPricing_updates!]!
  ): [passPricing_mutation_response]

  """
  update data of the table: "pendingOrder"
  """
  update_pendingOrder(
    """increments the numeric columns with given value of the filtered values"""
    _inc: pendingOrder_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: pendingOrder_set_input

    """filter the rows which have to be updated"""
    where: pendingOrder_bool_exp!
  ): pendingOrder_mutation_response

  """
  update single row of the table: "pendingOrder"
  """
  update_pendingOrder_by_pk(
    """increments the numeric columns with given value of the filtered values"""
    _inc: pendingOrder_inc_input

    """sets the columns of the filtered rows to the given values"""
    _set: pendingOrder_set_input
    pk_columns: pendingOrder_pk_columns_input!
  ): pendingOrder

  """
  update multiples rows of table: "pendingOrder"
  """
  update_pendingOrder_many(
    """updates to execute, in order"""
    updates: [pendingOrder_updates!]!
  ): [pendingOrder_mutation_response]

  """
  update data of the table: "roleAssignment"
  """
  update_roleAssignment(
    """sets the columns of the filtered rows to the given values"""
    _set: roleAssignment_set_input

    """filter the rows which have to be updated"""
    where: roleAssignment_bool_exp!
  ): roleAssignment_mutation_response

  """
  update multiples rows of table: "roleAssignment"
  """
  update_roleAssignment_many(
    """updates to execute, in order"""
    updates: [roleAssignment_updates!]!
  ): [roleAssignment_mutation_response]

  """
  update data of the table: "roles"
  """
  update_roles(
    """sets the columns of the filtered rows to the given values"""
    _set: roles_set_input

    """filter the rows which have to be updated"""
    where: roles_bool_exp!
  ): roles_mutation_response

  """
  update single row of the table: "roles"
  """
  update_roles_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: roles_set_input
    pk_columns: roles_pk_columns_input!
  ): roles

  """
  update multiples rows of table: "roles"
  """
  update_roles_many(
    """updates to execute, in order"""
    updates: [roles_updates!]!
  ): [roles_mutation_response]

  """
  update data of the table: "stripeCheckoutSession"
  """
  update_stripeCheckoutSession(
    """sets the columns of the filtered rows to the given values"""
    _set: stripeCheckoutSession_set_input

    """filter the rows which have to be updated"""
    where: stripeCheckoutSession_bool_exp!
  ): stripeCheckoutSession_mutation_response

  """
  update data of the table: "stripeCheckoutSessionType"
  """
  update_stripeCheckoutSessionType(
    """sets the columns of the filtered rows to the given values"""
    _set: stripeCheckoutSessionType_set_input

    """filter the rows which have to be updated"""
    where: stripeCheckoutSessionType_bool_exp!
  ): stripeCheckoutSessionType_mutation_response

  """
  update single row of the table: "stripeCheckoutSessionType"
  """
  update_stripeCheckoutSessionType_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: stripeCheckoutSessionType_set_input
    pk_columns: stripeCheckoutSessionType_pk_columns_input!
  ): stripeCheckoutSessionType

  """
  update multiples rows of table: "stripeCheckoutSessionType"
  """
  update_stripeCheckoutSessionType_many(
    """updates to execute, in order"""
    updates: [stripeCheckoutSessionType_updates!]!
  ): [stripeCheckoutSessionType_mutation_response]

  """
  update single row of the table: "stripeCheckoutSession"
  """
  update_stripeCheckoutSession_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: stripeCheckoutSession_set_input
    pk_columns: stripeCheckoutSession_pk_columns_input!
  ): stripeCheckoutSession

  """
  update multiples rows of table: "stripeCheckoutSession"
  """
  update_stripeCheckoutSession_many(
    """updates to execute, in order"""
    updates: [stripeCheckoutSession_updates!]!
  ): [stripeCheckoutSession_mutation_response]

  """
  update data of the table: "stripeCustomer"
  """
  update_stripeCustomer(
    """sets the columns of the filtered rows to the given values"""
    _set: stripeCustomer_set_input

    """filter the rows which have to be updated"""
    where: stripeCustomer_bool_exp!
  ): stripeCustomer_mutation_response

  """
  update single row of the table: "stripeCustomer"
  """
  update_stripeCustomer_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: stripeCustomer_set_input
    pk_columns: stripeCustomer_pk_columns_input!
  ): stripeCustomer

  """
  update multiples rows of table: "stripeCustomer"
  """
  update_stripeCustomer_many(
    """updates to execute, in order"""
    updates: [stripeCustomer_updates!]!
  ): [stripeCustomer_mutation_response]

  """
  update data of the table: "timezone"
  """
  update_timezone(
    """sets the columns of the filtered rows to the given values"""
    _set: timezone_set_input

    """filter the rows which have to be updated"""
    where: timezone_bool_exp!
  ): timezone_mutation_response

  """
  update single row of the table: "timezone"
  """
  update_timezone_by_pk(
    """sets the columns of the filtered rows to the given values"""
    _set: timezone_set_input
    pk_columns: timezone_pk_columns_input!
  ): timezone

  """
  update multiples rows of table: "timezone"
  """
  update_timezone_many(
    """updates to execute, in order"""
    updates: [timezone_updates!]!
  ): [timezone_mutation_response]

  """Upsert one asset"""
  upsertAsset(upsert: AssetUpsertInput!, where: AssetWhereUniqueInput!): Asset

  """Upsert one event"""
  upsertEvent(upsert: EventUpsertInput!, where: EventWhereUniqueInput!): Event

  """Upsert one eventPass"""
  upsertEventPass(upsert: EventPassUpsertInput!, where: EventPassWhereUniqueInput!): EventPass

  """Upsert one eventPassDelayedRevealed"""
  upsertEventPassDelayedRevealed(upsert: EventPassDelayedRevealedUpsertInput!, where: EventPassDelayedRevealedWhereUniqueInput!): EventPassDelayedRevealed

  """Upsert one organizer"""
  upsertOrganizer(upsert: OrganizerUpsertInput!, where: OrganizerWhereUniqueInput!): Organizer

  """Upsert one pack"""
  upsertPack(upsert: PackUpsertInput!, where: PackWhereUniqueInput!): Pack
}

"""
The nftTransfer model is built to record and chronicle the transfer of NFTs between addresses. This model is crucial in tracing the movement of an NFT, especially when validating that an event pass has reached its intended recipient. Such a system facilitates debugging and reduces the need for excessive querying of our indexer. Entries in this table are populated through two primary avenues: either via an activity webhook responding to real-time NFT transfers or through a regular cron job as a failsafe, ensuring data integrity even if the webhook fails to capture certain events.
"""
type nftTransfer {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint!

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: String!

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String!
  created_at: timestamptz!

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: String

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: String

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: String!
  id: uuid!

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: String!

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: String

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: String!

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint!

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: String!
}

"""
aggregated selection of "nftTransfer"
"""
type nftTransfer_aggregate {
  aggregate: nftTransfer_aggregate_fields
  nodes: [nftTransfer!]!
}

input nftTransfer_aggregate_bool_exp {
  count: nftTransfer_aggregate_bool_exp_count
}

input nftTransfer_aggregate_bool_exp_count {
  arguments: [nftTransfer_select_column!]
  distinct: Boolean
  filter: nftTransfer_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "nftTransfer"
"""
type nftTransfer_aggregate_fields {
  avg: nftTransfer_avg_fields
  count(columns: [nftTransfer_select_column!], distinct: Boolean): Int!
  max: nftTransfer_max_fields
  min: nftTransfer_min_fields
  stddev: nftTransfer_stddev_fields
  stddev_pop: nftTransfer_stddev_pop_fields
  stddev_samp: nftTransfer_stddev_samp_fields
  sum: nftTransfer_sum_fields
  var_pop: nftTransfer_var_pop_fields
  var_samp: nftTransfer_var_samp_fields
  variance: nftTransfer_variance_fields
}

"""
order by aggregate values of table "nftTransfer"
"""
input nftTransfer_aggregate_order_by {
  avg: nftTransfer_avg_order_by
  count: order_by
  max: nftTransfer_max_order_by
  min: nftTransfer_min_order_by
  stddev: nftTransfer_stddev_order_by
  stddev_pop: nftTransfer_stddev_pop_order_by
  stddev_samp: nftTransfer_stddev_samp_order_by
  sum: nftTransfer_sum_order_by
  var_pop: nftTransfer_var_pop_order_by
  var_samp: nftTransfer_var_samp_order_by
  variance: nftTransfer_variance_order_by
}

"""
input type for inserting array relation for remote table "nftTransfer"
"""
input nftTransfer_arr_rel_insert_input {
  data: [nftTransfer_insert_input!]!

  """upsert condition"""
  on_conflict: nftTransfer_on_conflict
}

"""aggregate avg on columns"""
type nftTransfer_avg_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: Float

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Float

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: Float
}

"""
order by avg() on columns of table "nftTransfer"
"""
input nftTransfer_avg_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""
Boolean expression to filter rows from the table "nftTransfer". All fields are combined with a logical 'AND'.
"""
input nftTransfer_bool_exp {
  _and: [nftTransfer_bool_exp!]
  _not: nftTransfer_bool_exp
  _or: [nftTransfer_bool_exp!]
  blockNumber: bigint_comparison_exp
  chainId: String_comparison_exp
  contractAddress: String_comparison_exp
  created_at: timestamptz_comparison_exp
  eventId: String_comparison_exp
  eventPassId: String_comparison_exp
  fromAddress: String_comparison_exp
  id: uuid_comparison_exp
  organizerId: String_comparison_exp
  packAmount: Int_comparison_exp
  packId: String_comparison_exp
  toAddress: String_comparison_exp
  tokenId: bigint_comparison_exp
  transactionHash: String_comparison_exp
}

"""
unique or primary key constraints on table "nftTransfer"
"""
enum nftTransfer_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  nftTransfer_pkey

  """
  unique or primary key constraint on columns "transactionHash", "contractAddress", "tokenId"
  """
  nft_transfer_unique_transfer
}

"""
input type for incrementing numeric columns in table "nftTransfer"
"""
input nftTransfer_inc_input {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint
}

"""
input type for inserting data into table "nftTransfer"
"""
input nftTransfer_insert_input {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: String

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: String

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: String

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: String
  id: uuid

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: String

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: String

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: String

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: String
}

"""aggregate max on columns"""
type nftTransfer_max_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: String

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: String

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: String

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: String
  id: uuid

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: String

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: String

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: String

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: String
}

"""
order by max() on columns of table "nftTransfer"
"""
input nftTransfer_max_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: order_by

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: order_by
  created_at: order_by

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: order_by

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: order_by

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: order_by
  id: order_by

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: order_by

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: order_by
}

"""aggregate min on columns"""
type nftTransfer_min_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: String

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: String

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: String

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: String
  id: uuid

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: String

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: String

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: String

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: String
}

"""
order by min() on columns of table "nftTransfer"
"""
input nftTransfer_min_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: order_by

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: order_by
  created_at: order_by

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: order_by

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: order_by

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: order_by
  id: order_by

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: order_by

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: order_by
}

"""
response of any mutation on the table "nftTransfer"
"""
type nftTransfer_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [nftTransfer!]!
}

"""
input type for inserting object relation for remote table "nftTransfer"
"""
input nftTransfer_obj_rel_insert_input {
  data: nftTransfer_insert_input!

  """upsert condition"""
  on_conflict: nftTransfer_on_conflict
}

"""
on_conflict condition type for table "nftTransfer"
"""
input nftTransfer_on_conflict {
  constraint: nftTransfer_constraint!
  update_columns: [nftTransfer_update_column!]! = []
  where: nftTransfer_bool_exp
}

"""Ordering options when selecting data from "nftTransfer"."""
input nftTransfer_order_by {
  blockNumber: order_by
  chainId: order_by
  contractAddress: order_by
  created_at: order_by
  eventId: order_by
  eventPassId: order_by
  fromAddress: order_by
  id: order_by
  organizerId: order_by
  packAmount: order_by
  packId: order_by
  toAddress: order_by
  tokenId: order_by
  transactionHash: order_by
}

"""primary key columns input for table: nftTransfer"""
input nftTransfer_pk_columns_input {
  id: uuid!
}

"""
select columns of table "nftTransfer"
"""
enum nftTransfer_select_column {
  """column name"""
  blockNumber

  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  eventId

  """column name"""
  eventPassId

  """column name"""
  fromAddress

  """column name"""
  id

  """column name"""
  organizerId

  """column name"""
  packAmount

  """column name"""
  packId

  """column name"""
  toAddress

  """column name"""
  tokenId

  """column name"""
  transactionHash
}

"""
input type for updating data in table "nftTransfer"
"""
input nftTransfer_set_input {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: String

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: String

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: String

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: String
  id: uuid

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: String

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: String

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: String

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: String
}

"""aggregate stddev on columns"""
type nftTransfer_stddev_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: Float

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Float

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: Float
}

"""
order by stddev() on columns of table "nftTransfer"
"""
input nftTransfer_stddev_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""aggregate stddev_pop on columns"""
type nftTransfer_stddev_pop_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: Float

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Float

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: Float
}

"""
order by stddev_pop() on columns of table "nftTransfer"
"""
input nftTransfer_stddev_pop_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""aggregate stddev_samp on columns"""
type nftTransfer_stddev_samp_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: Float

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Float

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: Float
}

"""
order by stddev_samp() on columns of table "nftTransfer"
"""
input nftTransfer_stddev_samp_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""
Streaming cursor of the table "nftTransfer"
"""
input nftTransfer_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: nftTransfer_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input nftTransfer_stream_cursor_value_input {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint

  """
  Indicates the specific blockchain or network where the NFT resides. Useful in a multi-chain environment to distinguish between various chains.
  """
  chainId: String

  """
  Identifies the smart contract associated with the NFT. This provides a direct link to the NFTs origin and behavior on the blockchain.
  """
  contractAddress: String
  created_at: timestamptz

  """
  Refers to the associated event ID for which the NFT was transferred. Ties the NFT transfer to a particular event in the platform.
  """
  eventId: String

  """
  Denotes the specific Event Pass associated with the NFT. Helps in tracking the lifecycle of a particular event pass.
  """
  eventPassId: String

  """
  Denotes the source address from which the NFT was transferred. Essential to trace the sender in the NFTs movement.
  """
  fromAddress: String
  id: uuid

  """
  Identifies the organizer who facilitated the event linked to the NFT transfer. Aids in associating NFT movements with specific organizers.
  """
  organizerId: String

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  Identifies the specific pack associated with the NFT. This field is only populated if the NFT is part of a pack.
  """
  packId: String

  """
  Specifies the destination address receiving the NFT. Critical for determining the current holder of the NFT.
  """
  toAddress: String

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint

  """
  Represents the unique hash of the transaction in which the NFT was transferred. Ensures traceability and verification on the blockchain.
  """
  transactionHash: String
}

"""aggregate sum on columns"""
type nftTransfer_sum_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: bigint

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Int

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: bigint
}

"""
order by sum() on columns of table "nftTransfer"
"""
input nftTransfer_sum_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""
update columns of table "nftTransfer"
"""
enum nftTransfer_update_column {
  """column name"""
  blockNumber

  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  eventId

  """column name"""
  eventPassId

  """column name"""
  fromAddress

  """column name"""
  id

  """column name"""
  organizerId

  """column name"""
  packAmount

  """column name"""
  packId

  """column name"""
  toAddress

  """column name"""
  tokenId

  """column name"""
  transactionHash
}

input nftTransfer_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: nftTransfer_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: nftTransfer_set_input

  """filter the rows which have to be updated"""
  where: nftTransfer_bool_exp!
}

"""aggregate var_pop on columns"""
type nftTransfer_var_pop_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: Float

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Float

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: Float
}

"""
order by var_pop() on columns of table "nftTransfer"
"""
input nftTransfer_var_pop_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""aggregate var_samp on columns"""
type nftTransfer_var_samp_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: Float

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Float

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: Float
}

"""
order by var_samp() on columns of table "nftTransfer"
"""
input nftTransfer_var_samp_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""aggregate variance on columns"""
type nftTransfer_variance_fields {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: Float

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: Float

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: Float
}

"""
order by variance() on columns of table "nftTransfer"
"""
input nftTransfer_variance_order_by {
  """
  The specific block on the blockchain where this transfer was recorded. Allows for pinpointing the exact point of transfer in the blockchain history.
  """
  blockNumber: order_by

  """
  Specifies the number of NFTs transferred in the transaction. This field is only populated if the NFT is part of a pack.
  """
  packAmount: order_by

  """
  The unique identifier for the NFT within its associated smart contract. Maintains a constant reference to the NFT across platforms.
  """
  tokenId: order_by
}

"""
Order a quantity of Event Pass or Pack (linked to Hygraph model EventPass or Pack) and associated to an Account
"""
type order {
  """An object relationship"""
  account: account
  accountId: uuid!
  created_at: timestamptz!
  eventPass(
    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
  ): EventPass
  eventPassId: String

  """An object relationship"""
  eventPassNftContract: eventPassNftContract
  id: uuid!
  pack(
    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
  ): EventPass

  """An object relationship"""
  packAmount: passAmount
  packId: String

  """An object relationship"""
  packNftContract: packNftContract

  """An object relationship"""
  packPricing: passPricing

  """An object relationship"""
  passAmount: passAmount

  """An object relationship"""
  passPricing: passPricing
  quantity: Int!
  status: orderStatus_enum!
  stripeCheckoutSessionId: String
  updated_at: timestamptz!
}

"""
columns and relationships of "orderStatus"
"""
type orderStatus {
  value: String!
}

"""
aggregated selection of "orderStatus"
"""
type orderStatus_aggregate {
  aggregate: orderStatus_aggregate_fields
  nodes: [orderStatus!]!
}

"""
aggregate fields of "orderStatus"
"""
type orderStatus_aggregate_fields {
  count(columns: [orderStatus_select_column!], distinct: Boolean): Int!
  max: orderStatus_max_fields
  min: orderStatus_min_fields
}

"""
Boolean expression to filter rows from the table "orderStatus". All fields are combined with a logical 'AND'.
"""
input orderStatus_bool_exp {
  _and: [orderStatus_bool_exp!]
  _not: orderStatus_bool_exp
  _or: [orderStatus_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "orderStatus"
"""
enum orderStatus_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  orderStatus_pkey
}

enum orderStatus_enum {
  CANCELLED
  COMPLETED
  CONFIRMED
  ERROR
  IS_MINTING
  REFUNDED
  UNAUTHORIZED
}

"""
Boolean expression to compare columns of type "orderStatus_enum". All fields are combined with logical 'AND'.
"""
input orderStatus_enum_comparison_exp {
  _eq: orderStatus_enum
  _in: [orderStatus_enum!]
  _is_null: Boolean
  _neq: orderStatus_enum
  _nin: [orderStatus_enum!]
}

"""
input type for inserting data into table "orderStatus"
"""
input orderStatus_insert_input {
  value: String
}

"""aggregate max on columns"""
type orderStatus_max_fields {
  value: String
}

"""aggregate min on columns"""
type orderStatus_min_fields {
  value: String
}

"""
response of any mutation on the table "orderStatus"
"""
type orderStatus_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [orderStatus!]!
}

"""
on_conflict condition type for table "orderStatus"
"""
input orderStatus_on_conflict {
  constraint: orderStatus_constraint!
  update_columns: [orderStatus_update_column!]! = []
  where: orderStatus_bool_exp
}

"""Ordering options when selecting data from "orderStatus"."""
input orderStatus_order_by {
  value: order_by
}

"""primary key columns input for table: orderStatus"""
input orderStatus_pk_columns_input {
  value: String!
}

"""
select columns of table "orderStatus"
"""
enum orderStatus_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "orderStatus"
"""
input orderStatus_set_input {
  value: String
}

"""
Streaming cursor of the table "orderStatus"
"""
input orderStatus_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: orderStatus_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input orderStatus_stream_cursor_value_input {
  value: String
}

"""
update columns of table "orderStatus"
"""
enum orderStatus_update_column {
  """column name"""
  value
}

input orderStatus_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: orderStatus_set_input

  """filter the rows which have to be updated"""
  where: orderStatus_bool_exp!
}

"""
aggregated selection of "order"
"""
type order_aggregate {
  aggregate: order_aggregate_fields
  nodes: [order!]!
}

input order_aggregate_bool_exp {
  count: order_aggregate_bool_exp_count
}

input order_aggregate_bool_exp_count {
  arguments: [order_select_column!]
  distinct: Boolean
  filter: order_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "order"
"""
type order_aggregate_fields {
  avg: order_avg_fields
  count(columns: [order_select_column!], distinct: Boolean): Int!
  max: order_max_fields
  min: order_min_fields
  stddev: order_stddev_fields
  stddev_pop: order_stddev_pop_fields
  stddev_samp: order_stddev_samp_fields
  sum: order_sum_fields
  var_pop: order_var_pop_fields
  var_samp: order_var_samp_fields
  variance: order_variance_fields
}

"""
order by aggregate values of table "order"
"""
input order_aggregate_order_by {
  avg: order_avg_order_by
  count: order_by
  max: order_max_order_by
  min: order_min_order_by
  stddev: order_stddev_order_by
  stddev_pop: order_stddev_pop_order_by
  stddev_samp: order_stddev_samp_order_by
  sum: order_sum_order_by
  var_pop: order_var_pop_order_by
  var_samp: order_var_samp_order_by
  variance: order_variance_order_by
}

"""
input type for inserting array relation for remote table "order"
"""
input order_arr_rel_insert_input {
  data: [order_insert_input!]!

  """upsert condition"""
  on_conflict: order_on_conflict
}

"""aggregate avg on columns"""
type order_avg_fields {
  quantity: Float
}

"""
order by avg() on columns of table "order"
"""
input order_avg_order_by {
  quantity: order_by
}

"""
Boolean expression to filter rows from the table "order". All fields are combined with a logical 'AND'.
"""
input order_bool_exp {
  _and: [order_bool_exp!]
  _not: order_bool_exp
  _or: [order_bool_exp!]
  account: account_bool_exp
  accountId: uuid_comparison_exp
  created_at: timestamptz_comparison_exp
  eventPassId: String_comparison_exp
  eventPassNftContract: eventPassNftContract_bool_exp
  id: uuid_comparison_exp
  packAmount: passAmount_bool_exp
  packId: String_comparison_exp
  packNftContract: packNftContract_bool_exp
  packPricing: passPricing_bool_exp
  passAmount: passAmount_bool_exp
  passPricing: passPricing_bool_exp
  quantity: Int_comparison_exp
  status: orderStatus_enum_comparison_exp
  stripeCheckoutSessionId: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""column ordering options"""
enum order_by {
  """in ascending order, nulls last"""
  asc

  """in ascending order, nulls first"""
  asc_nulls_first

  """in ascending order, nulls last"""
  asc_nulls_last

  """in descending order, nulls first"""
  desc

  """in descending order, nulls first"""
  desc_nulls_first

  """in descending order, nulls last"""
  desc_nulls_last
}

"""
unique or primary key constraints on table "order"
"""
enum order_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  order_pkey
}

"""
input type for incrementing numeric columns in table "order"
"""
input order_inc_input {
  quantity: Int
}

"""
input type for inserting data into table "order"
"""
input order_insert_input {
  account: account_obj_rel_insert_input
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  eventPassNftContract: eventPassNftContract_obj_rel_insert_input
  id: uuid
  packAmount: passAmount_obj_rel_insert_input
  packId: String
  packNftContract: packNftContract_obj_rel_insert_input
  packPricing: passPricing_obj_rel_insert_input
  passAmount: passAmount_obj_rel_insert_input
  passPricing: passPricing_obj_rel_insert_input
  quantity: Int
  status: orderStatus_enum
  stripeCheckoutSessionId: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type order_max_fields {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
  stripeCheckoutSessionId: String
  updated_at: timestamptz
}

"""
order by max() on columns of table "order"
"""
input order_max_order_by {
  accountId: order_by
  created_at: order_by
  eventPassId: order_by
  id: order_by
  packId: order_by
  quantity: order_by
  stripeCheckoutSessionId: order_by
  updated_at: order_by
}

"""aggregate min on columns"""
type order_min_fields {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
  stripeCheckoutSessionId: String
  updated_at: timestamptz
}

"""
order by min() on columns of table "order"
"""
input order_min_order_by {
  accountId: order_by
  created_at: order_by
  eventPassId: order_by
  id: order_by
  packId: order_by
  quantity: order_by
  stripeCheckoutSessionId: order_by
  updated_at: order_by
}

"""
response of any mutation on the table "order"
"""
type order_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [order!]!
}

"""
on_conflict condition type for table "order"
"""
input order_on_conflict {
  constraint: order_constraint!
  update_columns: [order_update_column!]! = []
  where: order_bool_exp
}

"""Ordering options when selecting data from "order"."""
input order_order_by {
  account: account_order_by
  accountId: order_by
  created_at: order_by
  eventPassId: order_by
  eventPassNftContract: eventPassNftContract_order_by
  id: order_by
  packAmount: passAmount_order_by
  packId: order_by
  packNftContract: packNftContract_order_by
  packPricing: passPricing_order_by
  passAmount: passAmount_order_by
  passPricing: passPricing_order_by
  quantity: order_by
  status: order_by
  stripeCheckoutSessionId: order_by
  updated_at: order_by
}

"""primary key columns input for table: order"""
input order_pk_columns_input {
  id: uuid!
}

"""
select columns of table "order"
"""
enum order_select_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  packId

  """column name"""
  quantity

  """column name"""
  status

  """column name"""
  stripeCheckoutSessionId

  """column name"""
  updated_at
}

"""
input type for updating data in table "order"
"""
input order_set_input {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
  status: orderStatus_enum
  stripeCheckoutSessionId: String
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type order_stddev_fields {
  quantity: Float
}

"""
order by stddev() on columns of table "order"
"""
input order_stddev_order_by {
  quantity: order_by
}

"""aggregate stddev_pop on columns"""
type order_stddev_pop_fields {
  quantity: Float
}

"""
order by stddev_pop() on columns of table "order"
"""
input order_stddev_pop_order_by {
  quantity: order_by
}

"""aggregate stddev_samp on columns"""
type order_stddev_samp_fields {
  quantity: Float
}

"""
order by stddev_samp() on columns of table "order"
"""
input order_stddev_samp_order_by {
  quantity: order_by
}

"""
Streaming cursor of the table "order"
"""
input order_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: order_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input order_stream_cursor_value_input {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
  status: orderStatus_enum
  stripeCheckoutSessionId: String
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type order_sum_fields {
  quantity: Int
}

"""
order by sum() on columns of table "order"
"""
input order_sum_order_by {
  quantity: order_by
}

"""
update columns of table "order"
"""
enum order_update_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  packId

  """column name"""
  quantity

  """column name"""
  status

  """column name"""
  stripeCheckoutSessionId

  """column name"""
  updated_at
}

input order_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: order_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: order_set_input

  """filter the rows which have to be updated"""
  where: order_bool_exp!
}

"""aggregate var_pop on columns"""
type order_var_pop_fields {
  quantity: Float
}

"""
order by var_pop() on columns of table "order"
"""
input order_var_pop_order_by {
  quantity: order_by
}

"""aggregate var_samp on columns"""
type order_var_samp_fields {
  quantity: Float
}

"""
order by var_samp() on columns of table "order"
"""
input order_var_samp_order_by {
  quantity: order_by
}

"""aggregate variance on columns"""
type order_variance_fields {
  quantity: Float
}

"""
order by variance() on columns of table "order"
"""
input order_variance_order_by {
  quantity: order_by
}

"""
Junction table linking pack NFTs to event pass NFTs. Ensures that each event pass NFT is uniquely associated with a pack.
"""
type packEventPassNft {
  """Identifier for the event pass NFT."""
  eventPassNftId: uuid!

  """Identifier for the pack NFT supply."""
  packNftSupplyId: uuid!
}

"""
aggregated selection of "packEventPassNft"
"""
type packEventPassNft_aggregate {
  aggregate: packEventPassNft_aggregate_fields
  nodes: [packEventPassNft!]!
}

input packEventPassNft_aggregate_bool_exp {
  count: packEventPassNft_aggregate_bool_exp_count
}

input packEventPassNft_aggregate_bool_exp_count {
  arguments: [packEventPassNft_select_column!]
  distinct: Boolean
  filter: packEventPassNft_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "packEventPassNft"
"""
type packEventPassNft_aggregate_fields {
  count(columns: [packEventPassNft_select_column!], distinct: Boolean): Int!
  max: packEventPassNft_max_fields
  min: packEventPassNft_min_fields
}

"""
order by aggregate values of table "packEventPassNft"
"""
input packEventPassNft_aggregate_order_by {
  count: order_by
  max: packEventPassNft_max_order_by
  min: packEventPassNft_min_order_by
}

"""
input type for inserting array relation for remote table "packEventPassNft"
"""
input packEventPassNft_arr_rel_insert_input {
  data: [packEventPassNft_insert_input!]!

  """upsert condition"""
  on_conflict: packEventPassNft_on_conflict
}

"""
Boolean expression to filter rows from the table "packEventPassNft". All fields are combined with a logical 'AND'.
"""
input packEventPassNft_bool_exp {
  _and: [packEventPassNft_bool_exp!]
  _not: packEventPassNft_bool_exp
  _or: [packEventPassNft_bool_exp!]
  eventPassNftId: uuid_comparison_exp
  packNftSupplyId: uuid_comparison_exp
}

"""
unique or primary key constraints on table "packEventPassNft"
"""
enum packEventPassNft_constraint {
  """
  unique or primary key constraint on columns "eventPassNftId"
  """
  packEventPassNft_eventPassNftId_key

  """
  unique or primary key constraint on columns "packNftSupplyId", "eventPassNftId"
  """
  packEventPassNft_pkey
}

"""
input type for inserting data into table "packEventPassNft"
"""
input packEventPassNft_insert_input {
  """Identifier for the event pass NFT."""
  eventPassNftId: uuid

  """Identifier for the pack NFT supply."""
  packNftSupplyId: uuid
}

"""aggregate max on columns"""
type packEventPassNft_max_fields {
  """Identifier for the event pass NFT."""
  eventPassNftId: uuid

  """Identifier for the pack NFT supply."""
  packNftSupplyId: uuid
}

"""
order by max() on columns of table "packEventPassNft"
"""
input packEventPassNft_max_order_by {
  """Identifier for the event pass NFT."""
  eventPassNftId: order_by

  """Identifier for the pack NFT supply."""
  packNftSupplyId: order_by
}

"""aggregate min on columns"""
type packEventPassNft_min_fields {
  """Identifier for the event pass NFT."""
  eventPassNftId: uuid

  """Identifier for the pack NFT supply."""
  packNftSupplyId: uuid
}

"""
order by min() on columns of table "packEventPassNft"
"""
input packEventPassNft_min_order_by {
  """Identifier for the event pass NFT."""
  eventPassNftId: order_by

  """Identifier for the pack NFT supply."""
  packNftSupplyId: order_by
}

"""
response of any mutation on the table "packEventPassNft"
"""
type packEventPassNft_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [packEventPassNft!]!
}

"""
on_conflict condition type for table "packEventPassNft"
"""
input packEventPassNft_on_conflict {
  constraint: packEventPassNft_constraint!
  update_columns: [packEventPassNft_update_column!]! = []
  where: packEventPassNft_bool_exp
}

"""Ordering options when selecting data from "packEventPassNft"."""
input packEventPassNft_order_by {
  eventPassNftId: order_by
  packNftSupplyId: order_by
}

"""primary key columns input for table: packEventPassNft"""
input packEventPassNft_pk_columns_input {
  """Identifier for the event pass NFT."""
  eventPassNftId: uuid!

  """Identifier for the pack NFT supply."""
  packNftSupplyId: uuid!
}

"""
select columns of table "packEventPassNft"
"""
enum packEventPassNft_select_column {
  """column name"""
  eventPassNftId

  """column name"""
  packNftSupplyId
}

"""
input type for updating data in table "packEventPassNft"
"""
input packEventPassNft_set_input {
  """Identifier for the event pass NFT."""
  eventPassNftId: uuid

  """Identifier for the pack NFT supply."""
  packNftSupplyId: uuid
}

"""
Streaming cursor of the table "packEventPassNft"
"""
input packEventPassNft_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: packEventPassNft_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input packEventPassNft_stream_cursor_value_input {
  """Identifier for the event pass NFT."""
  eventPassNftId: uuid

  """Identifier for the pack NFT supply."""
  packNftSupplyId: uuid
}

"""
update columns of table "packEventPassNft"
"""
enum packEventPassNft_update_column {
  """column name"""
  eventPassNftId

  """column name"""
  packNftSupplyId
}

input packEventPassNft_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: packEventPassNft_set_input

  """filter the rows which have to be updated"""
  where: packEventPassNft_bool_exp!
}

"""
Manages the NFTs associated with each pack, including details like contract address, chain ID, and the contents of each pack.
"""
type packNftContract {
  """Blockchain network identifier where the NFT contract resides."""
  chainId: String!

  """Smart contract address for the NFT collection."""
  contractAddress: String!
  created_at: timestamptz!

  """An array relationship"""
  eventPassNftContracts(
    """distinct select on columns"""
    distinct_on: [packNftContractEventPass_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContractEventPass_order_by!]

    """filter the rows returned"""
    where: packNftContractEventPass_bool_exp
  ): [packNftContractEventPass!]!

  """An aggregate relationship"""
  eventPassNftContracts_aggregate(
    """distinct select on columns"""
    distinct_on: [packNftContractEventPass_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContractEventPass_order_by!]

    """filter the rows returned"""
    where: packNftContractEventPass_bool_exp
  ): packNftContractEventPass_aggregate!

  """An array relationship"""
  eventPassNfts(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): [eventPassNft!]!

  """An aggregate relationship"""
  eventPassNfts_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): eventPassNft_aggregate!

  """Unique identifier for each pack NFT contract."""
  id: uuid!

  """
  Indicates whether the pack is distributed through an airdrop. True for airdrops, False otherwise.
  """
  isAirdrop: Boolean!

  """Identifier for the lottery associated with the pack."""
  lotteryId: String!

  """Identifier for the organizer responsible for the pack."""
  organizerId: String!

  """Unique identifier for each pack, ensuring no duplicates in the system."""
  packId: String!

  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int!
  updated_at: timestamptz!
}

"""
This junction table links each pack NFT contract to various event pass NFT contracts, along with the quantity of each event pass type included in the pack. It facilitates the management of event passes bundled within a specific pack.
"""
type packNftContractEventPass {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int!

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: String!

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: uuid!
}

"""
aggregated selection of "packNftContractEventPass"
"""
type packNftContractEventPass_aggregate {
  aggregate: packNftContractEventPass_aggregate_fields
  nodes: [packNftContractEventPass!]!
}

input packNftContractEventPass_aggregate_bool_exp {
  count: packNftContractEventPass_aggregate_bool_exp_count
}

input packNftContractEventPass_aggregate_bool_exp_count {
  arguments: [packNftContractEventPass_select_column!]
  distinct: Boolean
  filter: packNftContractEventPass_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "packNftContractEventPass"
"""
type packNftContractEventPass_aggregate_fields {
  avg: packNftContractEventPass_avg_fields
  count(columns: [packNftContractEventPass_select_column!], distinct: Boolean): Int!
  max: packNftContractEventPass_max_fields
  min: packNftContractEventPass_min_fields
  stddev: packNftContractEventPass_stddev_fields
  stddev_pop: packNftContractEventPass_stddev_pop_fields
  stddev_samp: packNftContractEventPass_stddev_samp_fields
  sum: packNftContractEventPass_sum_fields
  var_pop: packNftContractEventPass_var_pop_fields
  var_samp: packNftContractEventPass_var_samp_fields
  variance: packNftContractEventPass_variance_fields
}

"""
order by aggregate values of table "packNftContractEventPass"
"""
input packNftContractEventPass_aggregate_order_by {
  avg: packNftContractEventPass_avg_order_by
  count: order_by
  max: packNftContractEventPass_max_order_by
  min: packNftContractEventPass_min_order_by
  stddev: packNftContractEventPass_stddev_order_by
  stddev_pop: packNftContractEventPass_stddev_pop_order_by
  stddev_samp: packNftContractEventPass_stddev_samp_order_by
  sum: packNftContractEventPass_sum_order_by
  var_pop: packNftContractEventPass_var_pop_order_by
  var_samp: packNftContractEventPass_var_samp_order_by
  variance: packNftContractEventPass_variance_order_by
}

"""
input type for inserting array relation for remote table "packNftContractEventPass"
"""
input packNftContractEventPass_arr_rel_insert_input {
  data: [packNftContractEventPass_insert_input!]!

  """upsert condition"""
  on_conflict: packNftContractEventPass_on_conflict
}

"""aggregate avg on columns"""
type packNftContractEventPass_avg_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Float
}

"""
order by avg() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_avg_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""
Boolean expression to filter rows from the table "packNftContractEventPass". All fields are combined with a logical 'AND'.
"""
input packNftContractEventPass_bool_exp {
  _and: [packNftContractEventPass_bool_exp!]
  _not: packNftContractEventPass_bool_exp
  _or: [packNftContractEventPass_bool_exp!]
  amount: Int_comparison_exp
  eventPassId: String_comparison_exp
  packNftContractId: uuid_comparison_exp
}

"""
unique or primary key constraints on table "packNftContractEventPass"
"""
enum packNftContractEventPass_constraint {
  """
  unique or primary key constraint on columns "eventPassId", "packNftContractId"
  """
  packNftContractEventPass_pkey
}

"""
input type for incrementing numeric columns in table "packNftContractEventPass"
"""
input packNftContractEventPass_inc_input {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int
}

"""
input type for inserting data into table "packNftContractEventPass"
"""
input packNftContractEventPass_insert_input {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: String

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: uuid
}

"""aggregate max on columns"""
type packNftContractEventPass_max_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: String

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: uuid
}

"""
order by max() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_max_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: order_by

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: order_by
}

"""aggregate min on columns"""
type packNftContractEventPass_min_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: String

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: uuid
}

"""
order by min() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_min_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: order_by

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: order_by
}

"""
response of any mutation on the table "packNftContractEventPass"
"""
type packNftContractEventPass_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [packNftContractEventPass!]!
}

"""
on_conflict condition type for table "packNftContractEventPass"
"""
input packNftContractEventPass_on_conflict {
  constraint: packNftContractEventPass_constraint!
  update_columns: [packNftContractEventPass_update_column!]! = []
  where: packNftContractEventPass_bool_exp
}

"""Ordering options when selecting data from "packNftContractEventPass"."""
input packNftContractEventPass_order_by {
  amount: order_by
  eventPassId: order_by
  packNftContractId: order_by
}

"""primary key columns input for table: packNftContractEventPass"""
input packNftContractEventPass_pk_columns_input {
  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: String!

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: uuid!
}

"""
select columns of table "packNftContractEventPass"
"""
enum packNftContractEventPass_select_column {
  """column name"""
  amount

  """column name"""
  eventPassId

  """column name"""
  packNftContractId
}

"""
input type for updating data in table "packNftContractEventPass"
"""
input packNftContractEventPass_set_input {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: String

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: uuid
}

"""aggregate stddev on columns"""
type packNftContractEventPass_stddev_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Float
}

"""
order by stddev() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_stddev_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""aggregate stddev_pop on columns"""
type packNftContractEventPass_stddev_pop_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Float
}

"""
order by stddev_pop() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_stddev_pop_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""aggregate stddev_samp on columns"""
type packNftContractEventPass_stddev_samp_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Float
}

"""
order by stddev_samp() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_stddev_samp_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""
Streaming cursor of the table "packNftContractEventPass"
"""
input packNftContractEventPass_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: packNftContractEventPass_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input packNftContractEventPass_stream_cursor_value_input {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int

  """
  Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
  """
  eventPassId: String

  """
  Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
  """
  packNftContractId: uuid
}

"""aggregate sum on columns"""
type packNftContractEventPass_sum_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Int
}

"""
order by sum() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_sum_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""
update columns of table "packNftContractEventPass"
"""
enum packNftContractEventPass_update_column {
  """column name"""
  amount

  """column name"""
  eventPassId

  """column name"""
  packNftContractId
}

input packNftContractEventPass_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: packNftContractEventPass_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: packNftContractEventPass_set_input

  """filter the rows which have to be updated"""
  where: packNftContractEventPass_bool_exp!
}

"""aggregate var_pop on columns"""
type packNftContractEventPass_var_pop_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Float
}

"""
order by var_pop() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_var_pop_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""aggregate var_samp on columns"""
type packNftContractEventPass_var_samp_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Float
}

"""
order by var_samp() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_var_samp_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""aggregate variance on columns"""
type packNftContractEventPass_variance_fields {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: Float
}

"""
order by variance() on columns of table "packNftContractEventPass"
"""
input packNftContractEventPass_variance_order_by {
  """
  The quantity of this specific event pass NFT included in the pack. Indicates how many of this type of event pass are bundled in the associated pack NFT contract.
  """
  amount: order_by
}

"""
aggregated selection of "packNftContract"
"""
type packNftContract_aggregate {
  aggregate: packNftContract_aggregate_fields
  nodes: [packNftContract!]!
}

"""
aggregate fields of "packNftContract"
"""
type packNftContract_aggregate_fields {
  avg: packNftContract_avg_fields
  count(columns: [packNftContract_select_column!], distinct: Boolean): Int!
  max: packNftContract_max_fields
  min: packNftContract_min_fields
  stddev: packNftContract_stddev_fields
  stddev_pop: packNftContract_stddev_pop_fields
  stddev_samp: packNftContract_stddev_samp_fields
  sum: packNftContract_sum_fields
  var_pop: packNftContract_var_pop_fields
  var_samp: packNftContract_var_samp_fields
  variance: packNftContract_variance_fields
}

"""aggregate avg on columns"""
type packNftContract_avg_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Float
}

"""
Boolean expression to filter rows from the table "packNftContract". All fields are combined with a logical 'AND'.
"""
input packNftContract_bool_exp {
  _and: [packNftContract_bool_exp!]
  _not: packNftContract_bool_exp
  _or: [packNftContract_bool_exp!]
  chainId: String_comparison_exp
  contractAddress: String_comparison_exp
  created_at: timestamptz_comparison_exp
  eventPassNftContracts: packNftContractEventPass_bool_exp
  eventPassNftContracts_aggregate: packNftContractEventPass_aggregate_bool_exp
  eventPassNfts: eventPassNft_bool_exp
  eventPassNfts_aggregate: eventPassNft_aggregate_bool_exp
  id: uuid_comparison_exp
  isAirdrop: Boolean_comparison_exp
  lotteryId: String_comparison_exp
  organizerId: String_comparison_exp
  packId: String_comparison_exp
  rewardsPerPack: Int_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "packNftContract"
"""
enum packNftContract_constraint {
  """
  unique or primary key constraint on columns "packId"
  """
  packId_unique

  """
  unique or primary key constraint on columns "chainId", "contractAddress"
  """
  packNftContract_contractAddress_chainId_key

  """
  unique or primary key constraint on columns "id"
  """
  packNftContract_pkey
}

"""
input type for incrementing numeric columns in table "packNftContract"
"""
input packNftContract_inc_input {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int
}

"""
input type for inserting data into table "packNftContract"
"""
input packNftContract_insert_input {
  """Blockchain network identifier where the NFT contract resides."""
  chainId: String

  """Smart contract address for the NFT collection."""
  contractAddress: String
  created_at: timestamptz
  eventPassNftContracts: packNftContractEventPass_arr_rel_insert_input
  eventPassNfts: eventPassNft_arr_rel_insert_input

  """Unique identifier for each pack NFT contract."""
  id: uuid

  """
  Indicates whether the pack is distributed through an airdrop. True for airdrops, False otherwise.
  """
  isAirdrop: Boolean

  """Identifier for the lottery associated with the pack."""
  lotteryId: String

  """Identifier for the organizer responsible for the pack."""
  organizerId: String

  """Unique identifier for each pack, ensuring no duplicates in the system."""
  packId: String

  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int
  updated_at: timestamptz
}

"""aggregate max on columns"""
type packNftContract_max_fields {
  """Blockchain network identifier where the NFT contract resides."""
  chainId: String

  """Smart contract address for the NFT collection."""
  contractAddress: String
  created_at: timestamptz

  """Unique identifier for each pack NFT contract."""
  id: uuid

  """Identifier for the lottery associated with the pack."""
  lotteryId: String

  """Identifier for the organizer responsible for the pack."""
  organizerId: String

  """Unique identifier for each pack, ensuring no duplicates in the system."""
  packId: String

  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int
  updated_at: timestamptz
}

"""aggregate min on columns"""
type packNftContract_min_fields {
  """Blockchain network identifier where the NFT contract resides."""
  chainId: String

  """Smart contract address for the NFT collection."""
  contractAddress: String
  created_at: timestamptz

  """Unique identifier for each pack NFT contract."""
  id: uuid

  """Identifier for the lottery associated with the pack."""
  lotteryId: String

  """Identifier for the organizer responsible for the pack."""
  organizerId: String

  """Unique identifier for each pack, ensuring no duplicates in the system."""
  packId: String

  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int
  updated_at: timestamptz
}

"""
response of any mutation on the table "packNftContract"
"""
type packNftContract_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [packNftContract!]!
}

"""
input type for inserting object relation for remote table "packNftContract"
"""
input packNftContract_obj_rel_insert_input {
  data: packNftContract_insert_input!

  """upsert condition"""
  on_conflict: packNftContract_on_conflict
}

"""
on_conflict condition type for table "packNftContract"
"""
input packNftContract_on_conflict {
  constraint: packNftContract_constraint!
  update_columns: [packNftContract_update_column!]! = []
  where: packNftContract_bool_exp
}

"""Ordering options when selecting data from "packNftContract"."""
input packNftContract_order_by {
  chainId: order_by
  contractAddress: order_by
  created_at: order_by
  eventPassNftContracts_aggregate: packNftContractEventPass_aggregate_order_by
  eventPassNfts_aggregate: eventPassNft_aggregate_order_by
  id: order_by
  isAirdrop: order_by
  lotteryId: order_by
  organizerId: order_by
  packId: order_by
  rewardsPerPack: order_by
  updated_at: order_by
}

"""primary key columns input for table: packNftContract"""
input packNftContract_pk_columns_input {
  """Unique identifier for each pack NFT contract."""
  id: uuid!
}

"""
select columns of table "packNftContract"
"""
enum packNftContract_select_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  id

  """column name"""
  isAirdrop

  """column name"""
  lotteryId

  """column name"""
  organizerId

  """column name"""
  packId

  """column name"""
  rewardsPerPack

  """column name"""
  updated_at
}

"""
input type for updating data in table "packNftContract"
"""
input packNftContract_set_input {
  """Blockchain network identifier where the NFT contract resides."""
  chainId: String

  """Smart contract address for the NFT collection."""
  contractAddress: String
  created_at: timestamptz

  """Unique identifier for each pack NFT contract."""
  id: uuid

  """
  Indicates whether the pack is distributed through an airdrop. True for airdrops, False otherwise.
  """
  isAirdrop: Boolean

  """Identifier for the lottery associated with the pack."""
  lotteryId: String

  """Identifier for the organizer responsible for the pack."""
  organizerId: String

  """Unique identifier for each pack, ensuring no duplicates in the system."""
  packId: String

  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type packNftContract_stddev_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Float
}

"""aggregate stddev_pop on columns"""
type packNftContract_stddev_pop_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Float
}

"""aggregate stddev_samp on columns"""
type packNftContract_stddev_samp_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Float
}

"""
Streaming cursor of the table "packNftContract"
"""
input packNftContract_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: packNftContract_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input packNftContract_stream_cursor_value_input {
  """Blockchain network identifier where the NFT contract resides."""
  chainId: String

  """Smart contract address for the NFT collection."""
  contractAddress: String
  created_at: timestamptz

  """Unique identifier for each pack NFT contract."""
  id: uuid

  """
  Indicates whether the pack is distributed through an airdrop. True for airdrops, False otherwise.
  """
  isAirdrop: Boolean

  """Identifier for the lottery associated with the pack."""
  lotteryId: String

  """Identifier for the organizer responsible for the pack."""
  organizerId: String

  """Unique identifier for each pack, ensuring no duplicates in the system."""
  packId: String

  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type packNftContract_sum_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Int
}

"""
update columns of table "packNftContract"
"""
enum packNftContract_update_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  id

  """column name"""
  isAirdrop

  """column name"""
  lotteryId

  """column name"""
  organizerId

  """column name"""
  packId

  """column name"""
  rewardsPerPack

  """column name"""
  updated_at
}

input packNftContract_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: packNftContract_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: packNftContract_set_input

  """filter the rows which have to be updated"""
  where: packNftContract_bool_exp!
}

"""aggregate var_pop on columns"""
type packNftContract_var_pop_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Float
}

"""aggregate var_samp on columns"""
type packNftContract_var_samp_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Float
}

"""aggregate variance on columns"""
type packNftContract_variance_fields {
  """Number of rewards (or items) contained within each pack."""
  rewardsPerPack: Float
}

"""
This table represents the supply details of pack NFTs, tracking the ownership, contents, and metadata associated with each pack.
"""
type packNftSupply {
  """The specific blockchain or network on which the pack NFT exists."""
  chainId: String!

  """
  The address of the smart contract representing the pack NFT. Essential for blockchain interactions.
  """
  contractAddress: String!
  created_at: timestamptz!

  """The blockchain address of the current owner of the pack NFT."""
  currentOwnerAddress: String

  """
  Any error messages related to this pack NFT, particularly during transactions or metadata retrieval.
  """
  error: String
  id: uuid!

  """The reference to the latest transfer record for this pack NFT."""
  lastNftTransferId: uuid

  """The identifier of the organizer associated with this pack NFT."""
  organizerId: String!

  """An array relationship"""
  packEventPassNfts(
    """distinct select on columns"""
    distinct_on: [packEventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packEventPassNft_order_by!]

    """filter the rows returned"""
    where: packEventPassNft_bool_exp
  ): [packEventPassNft!]!

  """An aggregate relationship"""
  packEventPassNfts_aggregate(
    """distinct select on columns"""
    distinct_on: [packEventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packEventPassNft_order_by!]

    """filter the rows returned"""
    where: packEventPassNft_bool_exp
  ): packEventPassNft_aggregate!

  """A unique identifier for the pack within the platform."""
  packId: String!

  """The URI pointing to the metadata of the pack NFT."""
  tokenUri: String
  updated_at: timestamptz!
}

"""
aggregated selection of "packNftSupply"
"""
type packNftSupply_aggregate {
  aggregate: packNftSupply_aggregate_fields
  nodes: [packNftSupply!]!
}

"""
aggregate fields of "packNftSupply"
"""
type packNftSupply_aggregate_fields {
  count(columns: [packNftSupply_select_column!], distinct: Boolean): Int!
  max: packNftSupply_max_fields
  min: packNftSupply_min_fields
}

"""
Boolean expression to filter rows from the table "packNftSupply". All fields are combined with a logical 'AND'.
"""
input packNftSupply_bool_exp {
  _and: [packNftSupply_bool_exp!]
  _not: packNftSupply_bool_exp
  _or: [packNftSupply_bool_exp!]
  chainId: String_comparison_exp
  contractAddress: String_comparison_exp
  created_at: timestamptz_comparison_exp
  currentOwnerAddress: String_comparison_exp
  error: String_comparison_exp
  id: uuid_comparison_exp
  lastNftTransferId: uuid_comparison_exp
  organizerId: String_comparison_exp
  packEventPassNfts: packEventPassNft_bool_exp
  packEventPassNfts_aggregate: packEventPassNft_aggregate_bool_exp
  packId: String_comparison_exp
  tokenUri: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "packNftSupply"
"""
enum packNftSupply_constraint {
  """
  unique or primary key constraint on columns "chainId", "contractAddress", "packId"
  """
  packNftSupply_contractAddress_chainId_packId_key

  """
  unique or primary key constraint on columns "id"
  """
  packNftSupply_pkey
}

"""
input type for inserting data into table "packNftSupply"
"""
input packNftSupply_insert_input {
  """The specific blockchain or network on which the pack NFT exists."""
  chainId: String

  """
  The address of the smart contract representing the pack NFT. Essential for blockchain interactions.
  """
  contractAddress: String
  created_at: timestamptz

  """The blockchain address of the current owner of the pack NFT."""
  currentOwnerAddress: String

  """
  Any error messages related to this pack NFT, particularly during transactions or metadata retrieval.
  """
  error: String
  id: uuid

  """The reference to the latest transfer record for this pack NFT."""
  lastNftTransferId: uuid

  """The identifier of the organizer associated with this pack NFT."""
  organizerId: String
  packEventPassNfts: packEventPassNft_arr_rel_insert_input

  """A unique identifier for the pack within the platform."""
  packId: String

  """The URI pointing to the metadata of the pack NFT."""
  tokenUri: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type packNftSupply_max_fields {
  """The specific blockchain or network on which the pack NFT exists."""
  chainId: String

  """
  The address of the smart contract representing the pack NFT. Essential for blockchain interactions.
  """
  contractAddress: String
  created_at: timestamptz

  """The blockchain address of the current owner of the pack NFT."""
  currentOwnerAddress: String

  """
  Any error messages related to this pack NFT, particularly during transactions or metadata retrieval.
  """
  error: String
  id: uuid

  """The reference to the latest transfer record for this pack NFT."""
  lastNftTransferId: uuid

  """The identifier of the organizer associated with this pack NFT."""
  organizerId: String

  """A unique identifier for the pack within the platform."""
  packId: String

  """The URI pointing to the metadata of the pack NFT."""
  tokenUri: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type packNftSupply_min_fields {
  """The specific blockchain or network on which the pack NFT exists."""
  chainId: String

  """
  The address of the smart contract representing the pack NFT. Essential for blockchain interactions.
  """
  contractAddress: String
  created_at: timestamptz

  """The blockchain address of the current owner of the pack NFT."""
  currentOwnerAddress: String

  """
  Any error messages related to this pack NFT, particularly during transactions or metadata retrieval.
  """
  error: String
  id: uuid

  """The reference to the latest transfer record for this pack NFT."""
  lastNftTransferId: uuid

  """The identifier of the organizer associated with this pack NFT."""
  organizerId: String

  """A unique identifier for the pack within the platform."""
  packId: String

  """The URI pointing to the metadata of the pack NFT."""
  tokenUri: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "packNftSupply"
"""
type packNftSupply_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [packNftSupply!]!
}

"""
on_conflict condition type for table "packNftSupply"
"""
input packNftSupply_on_conflict {
  constraint: packNftSupply_constraint!
  update_columns: [packNftSupply_update_column!]! = []
  where: packNftSupply_bool_exp
}

"""Ordering options when selecting data from "packNftSupply"."""
input packNftSupply_order_by {
  chainId: order_by
  contractAddress: order_by
  created_at: order_by
  currentOwnerAddress: order_by
  error: order_by
  id: order_by
  lastNftTransferId: order_by
  organizerId: order_by
  packEventPassNfts_aggregate: packEventPassNft_aggregate_order_by
  packId: order_by
  tokenUri: order_by
  updated_at: order_by
}

"""primary key columns input for table: packNftSupply"""
input packNftSupply_pk_columns_input {
  id: uuid!
}

"""
select columns of table "packNftSupply"
"""
enum packNftSupply_select_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  currentOwnerAddress

  """column name"""
  error

  """column name"""
  id

  """column name"""
  lastNftTransferId

  """column name"""
  organizerId

  """column name"""
  packId

  """column name"""
  tokenUri

  """column name"""
  updated_at
}

"""
input type for updating data in table "packNftSupply"
"""
input packNftSupply_set_input {
  """The specific blockchain or network on which the pack NFT exists."""
  chainId: String

  """
  The address of the smart contract representing the pack NFT. Essential for blockchain interactions.
  """
  contractAddress: String
  created_at: timestamptz

  """The blockchain address of the current owner of the pack NFT."""
  currentOwnerAddress: String

  """
  Any error messages related to this pack NFT, particularly during transactions or metadata retrieval.
  """
  error: String
  id: uuid

  """The reference to the latest transfer record for this pack NFT."""
  lastNftTransferId: uuid

  """The identifier of the organizer associated with this pack NFT."""
  organizerId: String

  """A unique identifier for the pack within the platform."""
  packId: String

  """The URI pointing to the metadata of the pack NFT."""
  tokenUri: String
  updated_at: timestamptz
}

"""
Streaming cursor of the table "packNftSupply"
"""
input packNftSupply_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: packNftSupply_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input packNftSupply_stream_cursor_value_input {
  """The specific blockchain or network on which the pack NFT exists."""
  chainId: String

  """
  The address of the smart contract representing the pack NFT. Essential for blockchain interactions.
  """
  contractAddress: String
  created_at: timestamptz

  """The blockchain address of the current owner of the pack NFT."""
  currentOwnerAddress: String

  """
  Any error messages related to this pack NFT, particularly during transactions or metadata retrieval.
  """
  error: String
  id: uuid

  """The reference to the latest transfer record for this pack NFT."""
  lastNftTransferId: uuid

  """The identifier of the organizer associated with this pack NFT."""
  organizerId: String

  """A unique identifier for the pack within the platform."""
  packId: String

  """The URI pointing to the metadata of the pack NFT."""
  tokenUri: String
  updated_at: timestamptz
}

"""
update columns of table "packNftSupply"
"""
enum packNftSupply_update_column {
  """column name"""
  chainId

  """column name"""
  contractAddress

  """column name"""
  created_at

  """column name"""
  currentOwnerAddress

  """column name"""
  error

  """column name"""
  id

  """column name"""
  lastNftTransferId

  """column name"""
  organizerId

  """column name"""
  packId

  """column name"""
  tokenUri

  """column name"""
  updated_at
}

input packNftSupply_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: packNftSupply_set_input

  """filter the rows which have to be updated"""
  where: packNftSupply_bool_exp!
}

"""Hold the sums for the Pack Orders"""
type packOrderSums {
  packId: String!
  totalReserved: Int!
}

"""
aggregated selection of "packOrderSums"
"""
type packOrderSums_aggregate {
  aggregate: packOrderSums_aggregate_fields
  nodes: [packOrderSums!]!
}

"""
aggregate fields of "packOrderSums"
"""
type packOrderSums_aggregate_fields {
  avg: packOrderSums_avg_fields
  count(columns: [packOrderSums_select_column!], distinct: Boolean): Int!
  max: packOrderSums_max_fields
  min: packOrderSums_min_fields
  stddev: packOrderSums_stddev_fields
  stddev_pop: packOrderSums_stddev_pop_fields
  stddev_samp: packOrderSums_stddev_samp_fields
  sum: packOrderSums_sum_fields
  var_pop: packOrderSums_var_pop_fields
  var_samp: packOrderSums_var_samp_fields
  variance: packOrderSums_variance_fields
}

"""aggregate avg on columns"""
type packOrderSums_avg_fields {
  totalReserved: Float
}

"""
Boolean expression to filter rows from the table "packOrderSums". All fields are combined with a logical 'AND'.
"""
input packOrderSums_bool_exp {
  _and: [packOrderSums_bool_exp!]
  _not: packOrderSums_bool_exp
  _or: [packOrderSums_bool_exp!]
  packId: String_comparison_exp
  totalReserved: Int_comparison_exp
}

"""
unique or primary key constraints on table "packOrderSums"
"""
enum packOrderSums_constraint {
  """
  unique or primary key constraint on columns "packId"
  """
  packOrderSums_pkey
}

"""
input type for incrementing numeric columns in table "packOrderSums"
"""
input packOrderSums_inc_input {
  totalReserved: Int
}

"""
input type for inserting data into table "packOrderSums"
"""
input packOrderSums_insert_input {
  packId: String
  totalReserved: Int
}

"""aggregate max on columns"""
type packOrderSums_max_fields {
  packId: String
  totalReserved: Int
}

"""aggregate min on columns"""
type packOrderSums_min_fields {
  packId: String
  totalReserved: Int
}

"""
response of any mutation on the table "packOrderSums"
"""
type packOrderSums_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [packOrderSums!]!
}

"""
on_conflict condition type for table "packOrderSums"
"""
input packOrderSums_on_conflict {
  constraint: packOrderSums_constraint!
  update_columns: [packOrderSums_update_column!]! = []
  where: packOrderSums_bool_exp
}

"""Ordering options when selecting data from "packOrderSums"."""
input packOrderSums_order_by {
  packId: order_by
  totalReserved: order_by
}

"""primary key columns input for table: packOrderSums"""
input packOrderSums_pk_columns_input {
  packId: String!
}

"""
select columns of table "packOrderSums"
"""
enum packOrderSums_select_column {
  """column name"""
  packId

  """column name"""
  totalReserved
}

"""
input type for updating data in table "packOrderSums"
"""
input packOrderSums_set_input {
  packId: String
  totalReserved: Int
}

"""aggregate stddev on columns"""
type packOrderSums_stddev_fields {
  totalReserved: Float
}

"""aggregate stddev_pop on columns"""
type packOrderSums_stddev_pop_fields {
  totalReserved: Float
}

"""aggregate stddev_samp on columns"""
type packOrderSums_stddev_samp_fields {
  totalReserved: Float
}

"""
Streaming cursor of the table "packOrderSums"
"""
input packOrderSums_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: packOrderSums_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input packOrderSums_stream_cursor_value_input {
  packId: String
  totalReserved: Int
}

"""aggregate sum on columns"""
type packOrderSums_sum_fields {
  totalReserved: Int
}

"""
update columns of table "packOrderSums"
"""
enum packOrderSums_update_column {
  """column name"""
  packId

  """column name"""
  totalReserved
}

input packOrderSums_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: packOrderSums_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: packOrderSums_set_input

  """filter the rows which have to be updated"""
  where: packOrderSums_bool_exp!
}

"""aggregate var_pop on columns"""
type packOrderSums_var_pop_fields {
  totalReserved: Float
}

"""aggregate var_samp on columns"""
type packOrderSums_var_samp_fields {
  totalReserved: Float
}

"""aggregate variance on columns"""
type packOrderSums_variance_fields {
  totalReserved: Float
}

"""
The passAmount table stores quantity information related to each eventPass or Pack
"""
type passAmount {
  created_at: timestamptz!
  eventPassId: String
  id: uuid!
  maxAmount: Int!
  maxAmountPerUser: Int
  packId: String
  timeBeforeDelete: Int!
  updated_at: timestamptz!
}

"""
aggregated selection of "passAmount"
"""
type passAmount_aggregate {
  aggregate: passAmount_aggregate_fields
  nodes: [passAmount!]!
}

"""
aggregate fields of "passAmount"
"""
type passAmount_aggregate_fields {
  avg: passAmount_avg_fields
  count(columns: [passAmount_select_column!], distinct: Boolean): Int!
  max: passAmount_max_fields
  min: passAmount_min_fields
  stddev: passAmount_stddev_fields
  stddev_pop: passAmount_stddev_pop_fields
  stddev_samp: passAmount_stddev_samp_fields
  sum: passAmount_sum_fields
  var_pop: passAmount_var_pop_fields
  var_samp: passAmount_var_samp_fields
  variance: passAmount_variance_fields
}

"""aggregate avg on columns"""
type passAmount_avg_fields {
  maxAmount: Float
  maxAmountPerUser: Float
  timeBeforeDelete: Float
}

"""
Boolean expression to filter rows from the table "passAmount". All fields are combined with a logical 'AND'.
"""
input passAmount_bool_exp {
  _and: [passAmount_bool_exp!]
  _not: passAmount_bool_exp
  _or: [passAmount_bool_exp!]
  created_at: timestamptz_comparison_exp
  eventPassId: String_comparison_exp
  id: uuid_comparison_exp
  maxAmount: Int_comparison_exp
  maxAmountPerUser: Int_comparison_exp
  packId: String_comparison_exp
  timeBeforeDelete: Int_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "passAmount"
"""
enum passAmount_constraint {
  """
  unique or primary key constraint on columns "eventPassId"
  """
  idx_passamount_eventpassid

  """
  unique or primary key constraint on columns "packId"
  """
  idx_passamount_packid

  """
  unique or primary key constraint on columns "id"
  """
  passAmount_pkey
}

"""
input type for incrementing numeric columns in table "passAmount"
"""
input passAmount_inc_input {
  maxAmount: Int
  maxAmountPerUser: Int
  timeBeforeDelete: Int
}

"""
input type for inserting data into table "passAmount"
"""
input passAmount_insert_input {
  created_at: timestamptz
  eventPassId: String
  id: uuid
  maxAmount: Int
  maxAmountPerUser: Int
  packId: String
  timeBeforeDelete: Int
  updated_at: timestamptz
}

"""aggregate max on columns"""
type passAmount_max_fields {
  created_at: timestamptz
  eventPassId: String
  id: uuid
  maxAmount: Int
  maxAmountPerUser: Int
  packId: String
  timeBeforeDelete: Int
  updated_at: timestamptz
}

"""aggregate min on columns"""
type passAmount_min_fields {
  created_at: timestamptz
  eventPassId: String
  id: uuid
  maxAmount: Int
  maxAmountPerUser: Int
  packId: String
  timeBeforeDelete: Int
  updated_at: timestamptz
}

"""
response of any mutation on the table "passAmount"
"""
type passAmount_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [passAmount!]!
}

"""
input type for inserting object relation for remote table "passAmount"
"""
input passAmount_obj_rel_insert_input {
  data: passAmount_insert_input!

  """upsert condition"""
  on_conflict: passAmount_on_conflict
}

"""
on_conflict condition type for table "passAmount"
"""
input passAmount_on_conflict {
  constraint: passAmount_constraint!
  update_columns: [passAmount_update_column!]! = []
  where: passAmount_bool_exp
}

"""Ordering options when selecting data from "passAmount"."""
input passAmount_order_by {
  created_at: order_by
  eventPassId: order_by
  id: order_by
  maxAmount: order_by
  maxAmountPerUser: order_by
  packId: order_by
  timeBeforeDelete: order_by
  updated_at: order_by
}

"""primary key columns input for table: passAmount"""
input passAmount_pk_columns_input {
  id: uuid!
}

"""
select columns of table "passAmount"
"""
enum passAmount_select_column {
  """column name"""
  created_at

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  maxAmount

  """column name"""
  maxAmountPerUser

  """column name"""
  packId

  """column name"""
  timeBeforeDelete

  """column name"""
  updated_at
}

"""
input type for updating data in table "passAmount"
"""
input passAmount_set_input {
  created_at: timestamptz
  eventPassId: String
  id: uuid
  maxAmount: Int
  maxAmountPerUser: Int
  packId: String
  timeBeforeDelete: Int
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type passAmount_stddev_fields {
  maxAmount: Float
  maxAmountPerUser: Float
  timeBeforeDelete: Float
}

"""aggregate stddev_pop on columns"""
type passAmount_stddev_pop_fields {
  maxAmount: Float
  maxAmountPerUser: Float
  timeBeforeDelete: Float
}

"""aggregate stddev_samp on columns"""
type passAmount_stddev_samp_fields {
  maxAmount: Float
  maxAmountPerUser: Float
  timeBeforeDelete: Float
}

"""
Streaming cursor of the table "passAmount"
"""
input passAmount_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: passAmount_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input passAmount_stream_cursor_value_input {
  created_at: timestamptz
  eventPassId: String
  id: uuid
  maxAmount: Int
  maxAmountPerUser: Int
  packId: String
  timeBeforeDelete: Int
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type passAmount_sum_fields {
  maxAmount: Int
  maxAmountPerUser: Int
  timeBeforeDelete: Int
}

"""
update columns of table "passAmount"
"""
enum passAmount_update_column {
  """column name"""
  created_at

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  maxAmount

  """column name"""
  maxAmountPerUser

  """column name"""
  packId

  """column name"""
  timeBeforeDelete

  """column name"""
  updated_at
}

input passAmount_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: passAmount_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: passAmount_set_input

  """filter the rows which have to be updated"""
  where: passAmount_bool_exp!
}

"""aggregate var_pop on columns"""
type passAmount_var_pop_fields {
  maxAmount: Float
  maxAmountPerUser: Float
  timeBeforeDelete: Float
}

"""aggregate var_samp on columns"""
type passAmount_var_samp_fields {
  maxAmount: Float
  maxAmountPerUser: Float
  timeBeforeDelete: Float
}

"""aggregate variance on columns"""
type passAmount_variance_fields {
  maxAmount: Float
  maxAmountPerUser: Float
  timeBeforeDelete: Float
}

"""
The passPricing table stores pricing information for an eventPass or Pack.
"""
type passPricing {
  amount: Int!
  created_at: timestamptz!
  currency: currency_enum!
  eventPassId: String
  id: uuid!
  packId: String
  updated_at: timestamptz!
}

"""
aggregated selection of "passPricing"
"""
type passPricing_aggregate {
  aggregate: passPricing_aggregate_fields
  nodes: [passPricing!]!
}

"""
aggregate fields of "passPricing"
"""
type passPricing_aggregate_fields {
  avg: passPricing_avg_fields
  count(columns: [passPricing_select_column!], distinct: Boolean): Int!
  max: passPricing_max_fields
  min: passPricing_min_fields
  stddev: passPricing_stddev_fields
  stddev_pop: passPricing_stddev_pop_fields
  stddev_samp: passPricing_stddev_samp_fields
  sum: passPricing_sum_fields
  var_pop: passPricing_var_pop_fields
  var_samp: passPricing_var_samp_fields
  variance: passPricing_variance_fields
}

"""aggregate avg on columns"""
type passPricing_avg_fields {
  amount: Float
}

"""
Boolean expression to filter rows from the table "passPricing". All fields are combined with a logical 'AND'.
"""
input passPricing_bool_exp {
  _and: [passPricing_bool_exp!]
  _not: passPricing_bool_exp
  _or: [passPricing_bool_exp!]
  amount: Int_comparison_exp
  created_at: timestamptz_comparison_exp
  currency: currency_enum_comparison_exp
  eventPassId: String_comparison_exp
  id: uuid_comparison_exp
  packId: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "passPricing"
"""
enum passPricing_constraint {
  """
  unique or primary key constraint on columns "id"
  """
  passPricing_pkey
}

"""
input type for incrementing numeric columns in table "passPricing"
"""
input passPricing_inc_input {
  amount: Int
}

"""
input type for inserting data into table "passPricing"
"""
input passPricing_insert_input {
  amount: Int
  created_at: timestamptz
  currency: currency_enum
  eventPassId: String
  id: uuid
  packId: String
  updated_at: timestamptz
}

"""aggregate max on columns"""
type passPricing_max_fields {
  amount: Int
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  updated_at: timestamptz
}

"""aggregate min on columns"""
type passPricing_min_fields {
  amount: Int
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  updated_at: timestamptz
}

"""
response of any mutation on the table "passPricing"
"""
type passPricing_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [passPricing!]!
}

"""
input type for inserting object relation for remote table "passPricing"
"""
input passPricing_obj_rel_insert_input {
  data: passPricing_insert_input!

  """upsert condition"""
  on_conflict: passPricing_on_conflict
}

"""
on_conflict condition type for table "passPricing"
"""
input passPricing_on_conflict {
  constraint: passPricing_constraint!
  update_columns: [passPricing_update_column!]! = []
  where: passPricing_bool_exp
}

"""Ordering options when selecting data from "passPricing"."""
input passPricing_order_by {
  amount: order_by
  created_at: order_by
  currency: order_by
  eventPassId: order_by
  id: order_by
  packId: order_by
  updated_at: order_by
}

"""primary key columns input for table: passPricing"""
input passPricing_pk_columns_input {
  id: uuid!
}

"""
select columns of table "passPricing"
"""
enum passPricing_select_column {
  """column name"""
  amount

  """column name"""
  created_at

  """column name"""
  currency

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  packId

  """column name"""
  updated_at
}

"""
input type for updating data in table "passPricing"
"""
input passPricing_set_input {
  amount: Int
  created_at: timestamptz
  currency: currency_enum
  eventPassId: String
  id: uuid
  packId: String
  updated_at: timestamptz
}

"""aggregate stddev on columns"""
type passPricing_stddev_fields {
  amount: Float
}

"""aggregate stddev_pop on columns"""
type passPricing_stddev_pop_fields {
  amount: Float
}

"""aggregate stddev_samp on columns"""
type passPricing_stddev_samp_fields {
  amount: Float
}

"""
Streaming cursor of the table "passPricing"
"""
input passPricing_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: passPricing_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input passPricing_stream_cursor_value_input {
  amount: Int
  created_at: timestamptz
  currency: currency_enum
  eventPassId: String
  id: uuid
  packId: String
  updated_at: timestamptz
}

"""aggregate sum on columns"""
type passPricing_sum_fields {
  amount: Int
}

"""
update columns of table "passPricing"
"""
enum passPricing_update_column {
  """column name"""
  amount

  """column name"""
  created_at

  """column name"""
  currency

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  packId

  """column name"""
  updated_at
}

input passPricing_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: passPricing_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: passPricing_set_input

  """filter the rows which have to be updated"""
  where: passPricing_bool_exp!
}

"""aggregate var_pop on columns"""
type passPricing_var_pop_fields {
  amount: Float
}

"""aggregate var_samp on columns"""
type passPricing_var_samp_fields {
  amount: Float
}

"""aggregate variance on columns"""
type passPricing_variance_fields {
  amount: Float
}

"""
Order a quantity of Event Pass or Pack (linked to Hygraph model EventPass or Pack) and associated to an Account. Those orders are time bound and are automatically destroyed given an amount of time to preserve access to the event for other users.
"""
type pendingOrder {
  """An object relationship"""
  account: account
  accountId: uuid!
  created_at: timestamptz!
  eventPass(
    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
  ): EventPass
  eventPassId: String

  """An object relationship"""
  eventPassNftContract: eventPassNftContract
  id: uuid!
  pack(
    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
  ): EventPass

  """An object relationship"""
  packAmount: passAmount
  packId: String

  """An object relationship"""
  packNftContract: packNftContract

  """An object relationship"""
  packPricing: passPricing

  """An object relationship"""
  passAmount: passAmount

  """An object relationship"""
  passPricing: passPricing
  quantity: Int!
}

"""
aggregated selection of "pendingOrder"
"""
type pendingOrder_aggregate {
  aggregate: pendingOrder_aggregate_fields
  nodes: [pendingOrder!]!
}

"""
aggregate fields of "pendingOrder"
"""
type pendingOrder_aggregate_fields {
  avg: pendingOrder_avg_fields
  count(columns: [pendingOrder_select_column!], distinct: Boolean): Int!
  max: pendingOrder_max_fields
  min: pendingOrder_min_fields
  stddev: pendingOrder_stddev_fields
  stddev_pop: pendingOrder_stddev_pop_fields
  stddev_samp: pendingOrder_stddev_samp_fields
  sum: pendingOrder_sum_fields
  var_pop: pendingOrder_var_pop_fields
  var_samp: pendingOrder_var_samp_fields
  variance: pendingOrder_variance_fields
}

"""aggregate avg on columns"""
type pendingOrder_avg_fields {
  quantity: Float
}

"""
Boolean expression to filter rows from the table "pendingOrder". All fields are combined with a logical 'AND'.
"""
input pendingOrder_bool_exp {
  _and: [pendingOrder_bool_exp!]
  _not: pendingOrder_bool_exp
  _or: [pendingOrder_bool_exp!]
  account: account_bool_exp
  accountId: uuid_comparison_exp
  created_at: timestamptz_comparison_exp
  eventPassId: String_comparison_exp
  eventPassNftContract: eventPassNftContract_bool_exp
  id: uuid_comparison_exp
  packAmount: passAmount_bool_exp
  packId: String_comparison_exp
  packNftContract: packNftContract_bool_exp
  packPricing: passPricing_bool_exp
  passAmount: passAmount_bool_exp
  passPricing: passPricing_bool_exp
  quantity: Int_comparison_exp
}

"""
unique or primary key constraints on table "pendingOrder"
"""
enum pendingOrder_constraint {
  """
  unique or primary key constraint on columns "eventPassId", "accountId"
  """
  idx_pendingorder_eventpassid_accountid

  """
  unique or primary key constraint on columns "accountId", "packId"
  """
  idx_pendingorder_packid_accountid

  """
  unique or primary key constraint on columns "id"
  """
  pendingOrder_pkey
}

"""
input type for incrementing numeric columns in table "pendingOrder"
"""
input pendingOrder_inc_input {
  quantity: Int
}

"""
input type for inserting data into table "pendingOrder"
"""
input pendingOrder_insert_input {
  account: account_obj_rel_insert_input
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  eventPassNftContract: eventPassNftContract_obj_rel_insert_input
  id: uuid
  packAmount: passAmount_obj_rel_insert_input
  packId: String
  packNftContract: packNftContract_obj_rel_insert_input
  packPricing: passPricing_obj_rel_insert_input
  passAmount: passAmount_obj_rel_insert_input
  passPricing: passPricing_obj_rel_insert_input
  quantity: Int
}

"""aggregate max on columns"""
type pendingOrder_max_fields {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
}

"""aggregate min on columns"""
type pendingOrder_min_fields {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
}

"""
response of any mutation on the table "pendingOrder"
"""
type pendingOrder_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [pendingOrder!]!
}

"""
on_conflict condition type for table "pendingOrder"
"""
input pendingOrder_on_conflict {
  constraint: pendingOrder_constraint!
  update_columns: [pendingOrder_update_column!]! = []
  where: pendingOrder_bool_exp
}

"""Ordering options when selecting data from "pendingOrder"."""
input pendingOrder_order_by {
  account: account_order_by
  accountId: order_by
  created_at: order_by
  eventPassId: order_by
  eventPassNftContract: eventPassNftContract_order_by
  id: order_by
  packAmount: passAmount_order_by
  packId: order_by
  packNftContract: packNftContract_order_by
  packPricing: passPricing_order_by
  passAmount: passAmount_order_by
  passPricing: passPricing_order_by
  quantity: order_by
}

"""primary key columns input for table: pendingOrder"""
input pendingOrder_pk_columns_input {
  id: uuid!
}

"""
select columns of table "pendingOrder"
"""
enum pendingOrder_select_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  packId

  """column name"""
  quantity
}

"""
input type for updating data in table "pendingOrder"
"""
input pendingOrder_set_input {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
}

"""aggregate stddev on columns"""
type pendingOrder_stddev_fields {
  quantity: Float
}

"""aggregate stddev_pop on columns"""
type pendingOrder_stddev_pop_fields {
  quantity: Float
}

"""aggregate stddev_samp on columns"""
type pendingOrder_stddev_samp_fields {
  quantity: Float
}

"""
Streaming cursor of the table "pendingOrder"
"""
input pendingOrder_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: pendingOrder_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input pendingOrder_stream_cursor_value_input {
  accountId: uuid
  created_at: timestamptz
  eventPassId: String
  id: uuid
  packId: String
  quantity: Int
}

"""aggregate sum on columns"""
type pendingOrder_sum_fields {
  quantity: Int
}

"""
update columns of table "pendingOrder"
"""
enum pendingOrder_update_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  eventPassId

  """column name"""
  id

  """column name"""
  packId

  """column name"""
  quantity
}

input pendingOrder_updates {
  """increments the numeric columns with given value of the filtered values"""
  _inc: pendingOrder_inc_input

  """sets the columns of the filtered rows to the given values"""
  _set: pendingOrder_set_input

  """filter the rows which have to be updated"""
  where: pendingOrder_bool_exp!
}

"""aggregate var_pop on columns"""
type pendingOrder_var_pop_fields {
  quantity: Float
}

"""aggregate var_samp on columns"""
type pendingOrder_var_samp_fields {
  quantity: Float
}

"""aggregate variance on columns"""
type pendingOrder_variance_fields {
  quantity: Float
}

type query_root {
  """
  fetch data from the table: "account"
  """
  account(
    """distinct select on columns"""
    distinct_on: [account_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [account_order_by!]

    """filter the rows returned"""
    where: account_bool_exp
  ): [account!]!

  """
  fetch aggregated fields from the table: "account"
  """
  account_aggregate(
    """distinct select on columns"""
    distinct_on: [account_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [account_order_by!]

    """filter the rows returned"""
    where: account_bool_exp
  ): account_aggregate!

  """fetch data from the table: "account" using primary key columns"""
  account_by_pk(id: uuid!): account

  """Retrieve a single asset"""
  asset(
    """
    Defines which locales should be returned.
    
    Note that `Asset` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: AssetWhereUniqueInput!
  ): Asset

  """Retrieve document version"""
  assetVersion(where: VersionWhereInput!): DocumentVersion

  """Retrieve multiple assets"""
  assets(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Asset` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: AssetOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: AssetWhereInput
  ): [Asset!]!

  """Retrieve multiple assets using the Relay connection interface"""
  assetsConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Asset` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: AssetOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: AssetWhereInput
  ): AssetConnection!

  """
  fetch data from the table: "currency"
  """
  currency(
    """distinct select on columns"""
    distinct_on: [currency_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [currency_order_by!]

    """filter the rows returned"""
    where: currency_bool_exp
  ): [currency!]!

  """
  fetch aggregated fields from the table: "currency"
  """
  currency_aggregate(
    """distinct select on columns"""
    distinct_on: [currency_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [currency_order_by!]

    """filter the rows returned"""
    where: currency_bool_exp
  ): currency_aggregate!

  """fetch data from the table: "currency" using primary key columns"""
  currency_by_pk(value: String!): currency

  """Fetches an object given its ID"""
  entities(
    """The where parameters to query components"""
    where: [EntityWhereInput!]!
  ): [Entity!]

  """Retrieve a single event"""
  event(
    """
    Defines which locales should be returned.
    
    Note that `Event` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: EventWhereUniqueInput!
  ): Event

  """
  fetch data from the table: "eventParameters"
  """
  eventParameters(
    """distinct select on columns"""
    distinct_on: [eventParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventParameters_order_by!]

    """filter the rows returned"""
    where: eventParameters_bool_exp
  ): [eventParameters!]!

  """
  fetch aggregated fields from the table: "eventParameters"
  """
  eventParameters_aggregate(
    """distinct select on columns"""
    distinct_on: [eventParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventParameters_order_by!]

    """filter the rows returned"""
    where: eventParameters_bool_exp
  ): eventParameters_aggregate!

  """fetch data from the table: "eventParameters" using primary key columns"""
  eventParameters_by_pk(id: uuid!): eventParameters

  """Retrieve a single eventPass"""
  eventPass(
    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: EventPassWhereUniqueInput!
  ): EventPass

  """Retrieve a single eventPassDelayedRevealed"""
  eventPassDelayedRevealed(
    """
    Defines which locales should be returned.
    
    Note that `EventPassDelayedRevealed` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: EventPassDelayedRevealedWhereUniqueInput!
  ): EventPassDelayedRevealed

  """Retrieve document version"""
  eventPassDelayedRevealedVersion(where: VersionWhereInput!): DocumentVersion

  """
  fetch data from the table: "eventPassNft"
  """
  eventPassNft(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): [eventPassNft!]!

  """
  fetch data from the table: "eventPassNftContract"
  """
  eventPassNftContract(
    """distinct select on columns"""
    distinct_on: [eventPassNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContract_order_by!]

    """filter the rows returned"""
    where: eventPassNftContract_bool_exp
  ): [eventPassNftContract!]!

  """
  fetch data from the table: "eventPassNftContractType"
  """
  eventPassNftContractType(
    """distinct select on columns"""
    distinct_on: [eventPassNftContractType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContractType_order_by!]

    """filter the rows returned"""
    where: eventPassNftContractType_bool_exp
  ): [eventPassNftContractType!]!

  """
  fetch aggregated fields from the table: "eventPassNftContractType"
  """
  eventPassNftContractType_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNftContractType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContractType_order_by!]

    """filter the rows returned"""
    where: eventPassNftContractType_bool_exp
  ): eventPassNftContractType_aggregate!

  """
  fetch data from the table: "eventPassNftContractType" using primary key columns
  """
  eventPassNftContractType_by_pk(
    """Type name for event pass NFT contract."""
    value: String!
  ): eventPassNftContractType

  """
  fetch aggregated fields from the table: "eventPassNftContract"
  """
  eventPassNftContract_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContract_order_by!]

    """filter the rows returned"""
    where: eventPassNftContract_bool_exp
  ): eventPassNftContract_aggregate!

  """
  fetch data from the table: "eventPassNftContract" using primary key columns
  """
  eventPassNftContract_by_pk(id: uuid!): eventPassNftContract

  """
  fetch aggregated fields from the table: "eventPassNft"
  """
  eventPassNft_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): eventPassNft_aggregate!

  """fetch data from the table: "eventPassNft" using primary key columns"""
  eventPassNft_by_pk(id: uuid!): eventPassNft

  """
  fetch data from the table: "eventPassOrderSums"
  """
  eventPassOrderSums(
    """distinct select on columns"""
    distinct_on: [eventPassOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassOrderSums_order_by!]

    """filter the rows returned"""
    where: eventPassOrderSums_bool_exp
  ): [eventPassOrderSums!]!

  """
  fetch aggregated fields from the table: "eventPassOrderSums"
  """
  eventPassOrderSums_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassOrderSums_order_by!]

    """filter the rows returned"""
    where: eventPassOrderSums_bool_exp
  ): eventPassOrderSums_aggregate!

  """
  fetch data from the table: "eventPassOrderSums" using primary key columns
  """
  eventPassOrderSums_by_pk(eventPassId: String!): eventPassOrderSums

  """
  fetch data from the table: "eventPassType"
  """
  eventPassType(
    """distinct select on columns"""
    distinct_on: [eventPassType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassType_order_by!]

    """filter the rows returned"""
    where: eventPassType_bool_exp
  ): [eventPassType!]!

  """
  fetch aggregated fields from the table: "eventPassType"
  """
  eventPassType_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassType_order_by!]

    """filter the rows returned"""
    where: eventPassType_bool_exp
  ): eventPassType_aggregate!

  """fetch data from the table: "eventPassType" using primary key columns"""
  eventPassType_by_pk(
    """Type name for event pass."""
    value: String!
  ): eventPassType

  """
  fetch data from the table: "eventPassValidationType"
  """
  eventPassValidationType(
    """distinct select on columns"""
    distinct_on: [eventPassValidationType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassValidationType_order_by!]

    """filter the rows returned"""
    where: eventPassValidationType_bool_exp
  ): [eventPassValidationType!]!

  """
  fetch aggregated fields from the table: "eventPassValidationType"
  """
  eventPassValidationType_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassValidationType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassValidationType_order_by!]

    """filter the rows returned"""
    where: eventPassValidationType_bool_exp
  ): eventPassValidationType_aggregate!

  """
  fetch data from the table: "eventPassValidationType" using primary key columns
  """
  eventPassValidationType_by_pk(
    """Type name for event pass validation."""
    value: String!
  ): eventPassValidationType

  """Retrieve document version"""
  eventPassVersion(where: VersionWhereInput!): DocumentVersion

  """Retrieve multiple eventPasses"""
  eventPasses(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: EventPassOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: EventPassWhereInput
  ): [EventPass!]!

  """Retrieve multiple eventPasses using the Relay connection interface"""
  eventPassesConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `EventPass` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: EventPassOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: EventPassWhereInput
  ): EventPassConnection!

  """Retrieve multiple eventPassesDelayedRevealed"""
  eventPassesDelayedRevealed(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `EventPassDelayedRevealed` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: EventPassDelayedRevealedOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: EventPassDelayedRevealedWhereInput
  ): [EventPassDelayedRevealed!]!

  """
  Retrieve multiple eventPassesDelayedRevealed using the Relay connection interface
  """
  eventPassesDelayedRevealedConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `EventPassDelayedRevealed` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: EventPassDelayedRevealedOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: EventPassDelayedRevealedWhereInput
  ): EventPassDelayedRevealedConnection!

  """
  fetch data from the table: "eventStatus"
  """
  eventStatus(
    """distinct select on columns"""
    distinct_on: [eventStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventStatus_order_by!]

    """filter the rows returned"""
    where: eventStatus_bool_exp
  ): [eventStatus!]!

  """
  fetch aggregated fields from the table: "eventStatus"
  """
  eventStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [eventStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventStatus_order_by!]

    """filter the rows returned"""
    where: eventStatus_bool_exp
  ): eventStatus_aggregate!

  """fetch data from the table: "eventStatus" using primary key columns"""
  eventStatus_by_pk(value: String!): eventStatus

  """Retrieve document version"""
  eventVersion(where: VersionWhereInput!): DocumentVersion

  """Retrieve multiple events"""
  events(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Event` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: EventOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: EventWhereInput
  ): [Event!]!

  """Retrieve multiple events using the Relay connection interface"""
  eventsConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Event` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: EventOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: EventWhereInput
  ): EventConnection!

  """
  fetch data from the table: "follow"
  """
  follow(
    """distinct select on columns"""
    distinct_on: [follow_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [follow_order_by!]

    """filter the rows returned"""
    where: follow_bool_exp
  ): [follow!]!

  """
  fetch aggregated fields from the table: "follow"
  """
  follow_aggregate(
    """distinct select on columns"""
    distinct_on: [follow_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [follow_order_by!]

    """filter the rows returned"""
    where: follow_bool_exp
  ): follow_aggregate!

  """fetch data from the table: "follow" using primary key columns"""
  follow_by_pk(
    """
    References the unique identifier of the account that is following an organizer.
    """
    accountId: uuid!

    """
    Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
    """
    organizerSlug: String!
  ): follow

  """
  fetch data from the table: "kyc"
  """
  kyc(
    """distinct select on columns"""
    distinct_on: [kyc_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kyc_order_by!]

    """filter the rows returned"""
    where: kyc_bool_exp
  ): [kyc!]!

  """
  fetch data from the table: "kycLevelName"
  """
  kycLevelName(
    """distinct select on columns"""
    distinct_on: [kycLevelName_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycLevelName_order_by!]

    """filter the rows returned"""
    where: kycLevelName_bool_exp
  ): [kycLevelName!]!

  """
  fetch aggregated fields from the table: "kycLevelName"
  """
  kycLevelName_aggregate(
    """distinct select on columns"""
    distinct_on: [kycLevelName_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycLevelName_order_by!]

    """filter the rows returned"""
    where: kycLevelName_bool_exp
  ): kycLevelName_aggregate!

  """fetch data from the table: "kycLevelName" using primary key columns"""
  kycLevelName_by_pk(
    """
    basic_kyc_level: Basic level of KYC verification.
    advanced_kyc_level: Advanced level of KYC verification.
    """
    value: String!
  ): kycLevelName

  """
  fetch data from the table: "kycStatus"
  """
  kycStatus(
    """distinct select on columns"""
    distinct_on: [kycStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycStatus_order_by!]

    """filter the rows returned"""
    where: kycStatus_bool_exp
  ): [kycStatus!]!

  """
  fetch aggregated fields from the table: "kycStatus"
  """
  kycStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [kycStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycStatus_order_by!]

    """filter the rows returned"""
    where: kycStatus_bool_exp
  ): kycStatus_aggregate!

  """fetch data from the table: "kycStatus" using primary key columns"""
  kycStatus_by_pk(
    """
    init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
    pending: An applicant is ready to be processed.
    prechecked: The check is in a half way of being finished.
    queued: The checks have been started for the applicant.
    completed: The check has been completed.
    onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
    """
    value: String!
  ): kycStatus

  """
  fetch aggregated fields from the table: "kyc"
  """
  kyc_aggregate(
    """distinct select on columns"""
    distinct_on: [kyc_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kyc_order_by!]

    """filter the rows returned"""
    where: kyc_bool_exp
  ): kyc_aggregate!

  """fetch data from the table: "kyc" using primary key columns"""
  kyc_by_pk(
    """UUID referencing the user ID in the existing accounts table."""
    externalUserId: uuid!
  ): kyc

  """
  fetch data from the table: "lotteryParameters"
  """
  lotteryParameters(
    """distinct select on columns"""
    distinct_on: [lotteryParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryParameters_order_by!]

    """filter the rows returned"""
    where: lotteryParameters_bool_exp
  ): [lotteryParameters!]!

  """
  fetch aggregated fields from the table: "lotteryParameters"
  """
  lotteryParameters_aggregate(
    """distinct select on columns"""
    distinct_on: [lotteryParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryParameters_order_by!]

    """filter the rows returned"""
    where: lotteryParameters_bool_exp
  ): lotteryParameters_aggregate!

  """
  fetch data from the table: "lotteryParameters" using primary key columns
  """
  lotteryParameters_by_pk(id: uuid!): lotteryParameters

  """
  fetch data from the table: "lotteryStatus"
  """
  lotteryStatus(
    """distinct select on columns"""
    distinct_on: [lotteryStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryStatus_order_by!]

    """filter the rows returned"""
    where: lotteryStatus_bool_exp
  ): [lotteryStatus!]!

  """
  fetch aggregated fields from the table: "lotteryStatus"
  """
  lotteryStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [lotteryStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryStatus_order_by!]

    """filter the rows returned"""
    where: lotteryStatus_bool_exp
  ): lotteryStatus_aggregate!

  """fetch data from the table: "lotteryStatus" using primary key columns"""
  lotteryStatus_by_pk(value: String!): lotteryStatus

  """
  fetch data from the table: "nftTransfer"
  """
  nftTransfer(
    """distinct select on columns"""
    distinct_on: [nftTransfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [nftTransfer_order_by!]

    """filter the rows returned"""
    where: nftTransfer_bool_exp
  ): [nftTransfer!]!

  """
  fetch aggregated fields from the table: "nftTransfer"
  """
  nftTransfer_aggregate(
    """distinct select on columns"""
    distinct_on: [nftTransfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [nftTransfer_order_by!]

    """filter the rows returned"""
    where: nftTransfer_bool_exp
  ): nftTransfer_aggregate!

  """fetch data from the table: "nftTransfer" using primary key columns"""
  nftTransfer_by_pk(id: uuid!): nftTransfer

  """Fetches an object given its ID"""
  node(
    """The ID of an object"""
    id: ID!

    """
    Defines which locales should be returned.
    
    Note that `Node` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
  ): Node

  """
  fetch data from the table: "order"
  """
  order(
    """distinct select on columns"""
    distinct_on: [order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [order_order_by!]

    """filter the rows returned"""
    where: order_bool_exp
  ): [order!]!

  """
  fetch data from the table: "orderStatus"
  """
  orderStatus(
    """distinct select on columns"""
    distinct_on: [orderStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [orderStatus_order_by!]

    """filter the rows returned"""
    where: orderStatus_bool_exp
  ): [orderStatus!]!

  """
  fetch aggregated fields from the table: "orderStatus"
  """
  orderStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [orderStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [orderStatus_order_by!]

    """filter the rows returned"""
    where: orderStatus_bool_exp
  ): orderStatus_aggregate!

  """fetch data from the table: "orderStatus" using primary key columns"""
  orderStatus_by_pk(value: String!): orderStatus

  """
  fetch aggregated fields from the table: "order"
  """
  order_aggregate(
    """distinct select on columns"""
    distinct_on: [order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [order_order_by!]

    """filter the rows returned"""
    where: order_bool_exp
  ): order_aggregate!

  """fetch data from the table: "order" using primary key columns"""
  order_by_pk(id: uuid!): order

  """Retrieve a single organizer"""
  organizer(
    """
    Defines which locales should be returned.
    
    Note that `Organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: OrganizerWhereUniqueInput!
  ): Organizer

  """Retrieve document version"""
  organizerVersion(where: VersionWhereInput!): DocumentVersion

  """Retrieve multiple organizers"""
  organizers(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: OrganizerOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: OrganizerWhereInput
  ): [Organizer!]!

  """Retrieve multiple organizers using the Relay connection interface"""
  organizersConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: OrganizerOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: OrganizerWhereInput
  ): OrganizerConnection!

  """Retrieve a single pack"""
  pack(
    """
    Defines which locales should be returned.
    
    Note that `Pack` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: PackWhereUniqueInput!
  ): Pack

  """
  fetch data from the table: "packEventPassNft"
  """
  packEventPassNft(
    """distinct select on columns"""
    distinct_on: [packEventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packEventPassNft_order_by!]

    """filter the rows returned"""
    where: packEventPassNft_bool_exp
  ): [packEventPassNft!]!

  """
  fetch aggregated fields from the table: "packEventPassNft"
  """
  packEventPassNft_aggregate(
    """distinct select on columns"""
    distinct_on: [packEventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packEventPassNft_order_by!]

    """filter the rows returned"""
    where: packEventPassNft_bool_exp
  ): packEventPassNft_aggregate!

  """
  fetch data from the table: "packEventPassNft" using primary key columns
  """
  packEventPassNft_by_pk(
    """Identifier for the event pass NFT."""
    eventPassNftId: uuid!

    """Identifier for the pack NFT supply."""
    packNftSupplyId: uuid!
  ): packEventPassNft

  """
  fetch data from the table: "packNftContract"
  """
  packNftContract(
    """distinct select on columns"""
    distinct_on: [packNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContract_order_by!]

    """filter the rows returned"""
    where: packNftContract_bool_exp
  ): [packNftContract!]!

  """
  fetch data from the table: "packNftContractEventPass"
  """
  packNftContractEventPass(
    """distinct select on columns"""
    distinct_on: [packNftContractEventPass_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContractEventPass_order_by!]

    """filter the rows returned"""
    where: packNftContractEventPass_bool_exp
  ): [packNftContractEventPass!]!

  """
  fetch aggregated fields from the table: "packNftContractEventPass"
  """
  packNftContractEventPass_aggregate(
    """distinct select on columns"""
    distinct_on: [packNftContractEventPass_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContractEventPass_order_by!]

    """filter the rows returned"""
    where: packNftContractEventPass_bool_exp
  ): packNftContractEventPass_aggregate!

  """
  fetch data from the table: "packNftContractEventPass" using primary key columns
  """
  packNftContractEventPass_by_pk(
    """
    Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
    """
    eventPassId: String!

    """
    Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
    """
    packNftContractId: uuid!
  ): packNftContractEventPass

  """
  fetch aggregated fields from the table: "packNftContract"
  """
  packNftContract_aggregate(
    """distinct select on columns"""
    distinct_on: [packNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContract_order_by!]

    """filter the rows returned"""
    where: packNftContract_bool_exp
  ): packNftContract_aggregate!

  """fetch data from the table: "packNftContract" using primary key columns"""
  packNftContract_by_pk(
    """Unique identifier for each pack NFT contract."""
    id: uuid!
  ): packNftContract

  """
  fetch data from the table: "packNftSupply"
  """
  packNftSupply(
    """distinct select on columns"""
    distinct_on: [packNftSupply_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftSupply_order_by!]

    """filter the rows returned"""
    where: packNftSupply_bool_exp
  ): [packNftSupply!]!

  """
  fetch aggregated fields from the table: "packNftSupply"
  """
  packNftSupply_aggregate(
    """distinct select on columns"""
    distinct_on: [packNftSupply_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftSupply_order_by!]

    """filter the rows returned"""
    where: packNftSupply_bool_exp
  ): packNftSupply_aggregate!

  """fetch data from the table: "packNftSupply" using primary key columns"""
  packNftSupply_by_pk(id: uuid!): packNftSupply

  """
  fetch data from the table: "packOrderSums"
  """
  packOrderSums(
    """distinct select on columns"""
    distinct_on: [packOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packOrderSums_order_by!]

    """filter the rows returned"""
    where: packOrderSums_bool_exp
  ): [packOrderSums!]!

  """
  fetch aggregated fields from the table: "packOrderSums"
  """
  packOrderSums_aggregate(
    """distinct select on columns"""
    distinct_on: [packOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packOrderSums_order_by!]

    """filter the rows returned"""
    where: packOrderSums_bool_exp
  ): packOrderSums_aggregate!

  """fetch data from the table: "packOrderSums" using primary key columns"""
  packOrderSums_by_pk(packId: String!): packOrderSums

  """Retrieve document version"""
  packVersion(where: VersionWhereInput!): DocumentVersion

  """Retrieve multiple packs"""
  packs(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Pack` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: PackOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: PackWhereInput
  ): [Pack!]!

  """Retrieve multiple packs using the Relay connection interface"""
  packsConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `Pack` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: PackOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: PackWhereInput
  ): PackConnection!

  """
  fetch data from the table: "passAmount"
  """
  passAmount(
    """distinct select on columns"""
    distinct_on: [passAmount_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passAmount_order_by!]

    """filter the rows returned"""
    where: passAmount_bool_exp
  ): [passAmount!]!

  """
  fetch aggregated fields from the table: "passAmount"
  """
  passAmount_aggregate(
    """distinct select on columns"""
    distinct_on: [passAmount_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passAmount_order_by!]

    """filter the rows returned"""
    where: passAmount_bool_exp
  ): passAmount_aggregate!

  """fetch data from the table: "passAmount" using primary key columns"""
  passAmount_by_pk(id: uuid!): passAmount

  """
  fetch data from the table: "passPricing"
  """
  passPricing(
    """distinct select on columns"""
    distinct_on: [passPricing_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passPricing_order_by!]

    """filter the rows returned"""
    where: passPricing_bool_exp
  ): [passPricing!]!

  """
  fetch aggregated fields from the table: "passPricing"
  """
  passPricing_aggregate(
    """distinct select on columns"""
    distinct_on: [passPricing_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passPricing_order_by!]

    """filter the rows returned"""
    where: passPricing_bool_exp
  ): passPricing_aggregate!

  """fetch data from the table: "passPricing" using primary key columns"""
  passPricing_by_pk(id: uuid!): passPricing

  """
  fetch data from the table: "pendingOrder"
  """
  pendingOrder(
    """distinct select on columns"""
    distinct_on: [pendingOrder_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [pendingOrder_order_by!]

    """filter the rows returned"""
    where: pendingOrder_bool_exp
  ): [pendingOrder!]!

  """
  fetch aggregated fields from the table: "pendingOrder"
  """
  pendingOrder_aggregate(
    """distinct select on columns"""
    distinct_on: [pendingOrder_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [pendingOrder_order_by!]

    """filter the rows returned"""
    where: pendingOrder_bool_exp
  ): pendingOrder_aggregate!

  """fetch data from the table: "pendingOrder" using primary key columns"""
  pendingOrder_by_pk(id: uuid!): pendingOrder

  """
  fetch data from the table: "roleAssignment"
  """
  roleAssignment(
    """distinct select on columns"""
    distinct_on: [roleAssignment_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roleAssignment_order_by!]

    """filter the rows returned"""
    where: roleAssignment_bool_exp
  ): [roleAssignment!]!

  """
  fetch aggregated fields from the table: "roleAssignment"
  """
  roleAssignment_aggregate(
    """distinct select on columns"""
    distinct_on: [roleAssignment_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roleAssignment_order_by!]

    """filter the rows returned"""
    where: roleAssignment_bool_exp
  ): roleAssignment_aggregate!

  """
  fetch data from the table: "roles"
  """
  roles(
    """distinct select on columns"""
    distinct_on: [roles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roles_order_by!]

    """filter the rows returned"""
    where: roles_bool_exp
  ): [roles!]!

  """
  fetch aggregated fields from the table: "roles"
  """
  roles_aggregate(
    """distinct select on columns"""
    distinct_on: [roles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roles_order_by!]

    """filter the rows returned"""
    where: roles_bool_exp
  ): roles_aggregate!

  """fetch data from the table: "roles" using primary key columns"""
  roles_by_pk(
    "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
    value: String!
  ): roles

  """Retrieve a single scheduledOperation"""
  scheduledOperation(
    """
    Defines which locales should be returned.
    
    Note that `ScheduledOperation` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: ScheduledOperationWhereUniqueInput!
  ): ScheduledOperation

  """Retrieve multiple scheduledOperations"""
  scheduledOperations(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `ScheduledOperation` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: ScheduledOperationOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: ScheduledOperationWhereInput
  ): [ScheduledOperation!]!

  """
  Retrieve multiple scheduledOperations using the Relay connection interface
  """
  scheduledOperationsConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `ScheduledOperation` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: ScheduledOperationOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: ScheduledOperationWhereInput
  ): ScheduledOperationConnection!

  """Retrieve a single scheduledRelease"""
  scheduledRelease(
    """
    Defines which locales should be returned.
    
    Note that `ScheduledRelease` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: ScheduledReleaseWhereUniqueInput!
  ): ScheduledRelease

  """Retrieve multiple scheduledReleases"""
  scheduledReleases(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `ScheduledRelease` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: ScheduledReleaseOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: ScheduledReleaseWhereInput
  ): [ScheduledRelease!]!

  """
  Retrieve multiple scheduledReleases using the Relay connection interface
  """
  scheduledReleasesConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `ScheduledRelease` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: ScheduledReleaseOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: ScheduledReleaseWhereInput
  ): ScheduledReleaseConnection!

  """
  fetch data from the table: "stripeCheckoutSession"
  """
  stripeCheckoutSession(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSession_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSession_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSession_bool_exp
  ): [stripeCheckoutSession!]!

  """
  fetch data from the table: "stripeCheckoutSessionType"
  """
  stripeCheckoutSessionType(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSessionType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSessionType_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSessionType_bool_exp
  ): [stripeCheckoutSessionType!]!

  """
  fetch aggregated fields from the table: "stripeCheckoutSessionType"
  """
  stripeCheckoutSessionType_aggregate(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSessionType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSessionType_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSessionType_bool_exp
  ): stripeCheckoutSessionType_aggregate!

  """
  fetch data from the table: "stripeCheckoutSessionType" using primary key columns
  """
  stripeCheckoutSessionType_by_pk(
    """Type value."""
    value: String!
  ): stripeCheckoutSessionType

  """
  fetch aggregated fields from the table: "stripeCheckoutSession"
  """
  stripeCheckoutSession_aggregate(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSession_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSession_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSession_bool_exp
  ): stripeCheckoutSession_aggregate!

  """
  fetch data from the table: "stripeCheckoutSession" using primary key columns
  """
  stripeCheckoutSession_by_pk(
    """Unique identifier for the Stripe Checkout Session."""
    stripeSessionId: String!
  ): stripeCheckoutSession

  """
  fetch data from the table: "stripeCustomer"
  """
  stripeCustomer(
    """distinct select on columns"""
    distinct_on: [stripeCustomer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCustomer_order_by!]

    """filter the rows returned"""
    where: stripeCustomer_bool_exp
  ): [stripeCustomer!]!

  """
  fetch aggregated fields from the table: "stripeCustomer"
  """
  stripeCustomer_aggregate(
    """distinct select on columns"""
    distinct_on: [stripeCustomer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCustomer_order_by!]

    """filter the rows returned"""
    where: stripeCustomer_bool_exp
  ): stripeCustomer_aggregate!

  """fetch data from the table: "stripeCustomer" using primary key columns"""
  stripeCustomer_by_pk(
    """Unique identifier for the Stripe Customer."""
    stripeCustomerId: String!
  ): stripeCustomer

  """
  fetch data from the table: "timezone"
  """
  timezone(
    """distinct select on columns"""
    distinct_on: [timezone_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [timezone_order_by!]

    """filter the rows returned"""
    where: timezone_bool_exp
  ): [timezone!]!

  """
  fetch aggregated fields from the table: "timezone"
  """
  timezone_aggregate(
    """distinct select on columns"""
    distinct_on: [timezone_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [timezone_order_by!]

    """filter the rows returned"""
    where: timezone_bool_exp
  ): timezone_aggregate!

  """fetch data from the table: "timezone" using primary key columns"""
  timezone_by_pk(value: String!): timezone

  """Retrieve a single user"""
  user(
    """
    Defines which locales should be returned.
    
    Note that `User` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: UserWhereUniqueInput!
  ): User

  """Retrieve multiple users"""
  users(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `User` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: UserOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: UserWhereInput
  ): [User!]!

  """Retrieve multiple users using the Relay connection interface"""
  usersConnection(
    after: String
    before: String
    first: Int
    last: Int

    """
    Defines which locales should be returned.
    
    Note that `User` is a model without localized fields and will not be affected directly by this argument, however the locales will be passed on to any relational fields in the query's subtree for filtering.
    For related models with localized fields in the query's subtree, the first locale matching the provided list of locales will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    orderBy: UserOrderByInput
    skip: Int
    stage: Stage! = PUBLISHED
    where: UserWhereInput
  ): UserConnection!
}

"""
Table to assign roles to accounts, allowing a many-to-many relationship. Each account can have multiple roles and each role can be assigned to multiple accounts. This is part of the RBAC system integration.
"""
type roleAssignment {
  accountId: uuid!
  created_at: timestamptz!
  eventId: String!
  id: uuid!
  invitedById: uuid!

  """An object relationship"""
  inviter: account!
  organizer(
    """
    Defines which locales should be returned.
    
    Note that `Organizer` will be affected directly by this argument, as well as any other related models with localized fields in the query's subtree.
    The first locale matching the provided list will be returned, entries with non matching locales will be filtered out.
    
    This argument may be overwritten by another locales definition in a relational child field, this will effectively use the overwritten argument for the affected query's subtree.
    """
    locales: [Locale!]! = [en]
    stage: Stage! = PUBLISHED
    where: OrganizerWhereUniqueInput_remote_rel_roleAssignmentorganizer!
  ): Organizer
  organizerId: String!
  role: roles_enum!
}

"""
aggregated selection of "roleAssignment"
"""
type roleAssignment_aggregate {
  aggregate: roleAssignment_aggregate_fields
  nodes: [roleAssignment!]!
}

input roleAssignment_aggregate_bool_exp {
  count: roleAssignment_aggregate_bool_exp_count
}

input roleAssignment_aggregate_bool_exp_count {
  arguments: [roleAssignment_select_column!]
  distinct: Boolean
  filter: roleAssignment_bool_exp
  predicate: Int_comparison_exp!
}

"""
aggregate fields of "roleAssignment"
"""
type roleAssignment_aggregate_fields {
  count(columns: [roleAssignment_select_column!], distinct: Boolean): Int!
  max: roleAssignment_max_fields
  min: roleAssignment_min_fields
}

"""
order by aggregate values of table "roleAssignment"
"""
input roleAssignment_aggregate_order_by {
  count: order_by
  max: roleAssignment_max_order_by
  min: roleAssignment_min_order_by
}

"""
input type for inserting array relation for remote table "roleAssignment"
"""
input roleAssignment_arr_rel_insert_input {
  data: [roleAssignment_insert_input!]!

  """upsert condition"""
  on_conflict: roleAssignment_on_conflict
}

"""
Boolean expression to filter rows from the table "roleAssignment". All fields are combined with a logical 'AND'.
"""
input roleAssignment_bool_exp {
  _and: [roleAssignment_bool_exp!]
  _not: roleAssignment_bool_exp
  _or: [roleAssignment_bool_exp!]
  accountId: uuid_comparison_exp
  created_at: timestamptz_comparison_exp
  eventId: String_comparison_exp
  id: uuid_comparison_exp
  invitedById: uuid_comparison_exp
  inviter: account_bool_exp
  organizerId: String_comparison_exp
  role: roles_enum_comparison_exp
}

"""
unique or primary key constraints on table "roleAssignment"
"""
enum roleAssignment_constraint {
  """
  unique or primary key constraint on columns "organizerId", "accountId", "role", "eventId"
  """
  unique_role_assignment
}

"""
input type for inserting data into table "roleAssignment"
"""
input roleAssignment_insert_input {
  accountId: uuid
  created_at: timestamptz
  eventId: String
  id: uuid
  invitedById: uuid
  inviter: account_obj_rel_insert_input
  organizerId: String
  role: roles_enum
}

"""aggregate max on columns"""
type roleAssignment_max_fields {
  accountId: uuid
  created_at: timestamptz
  eventId: String
  id: uuid
  invitedById: uuid
  organizerId: String
}

"""
order by max() on columns of table "roleAssignment"
"""
input roleAssignment_max_order_by {
  accountId: order_by
  created_at: order_by
  eventId: order_by
  id: order_by
  invitedById: order_by
  organizerId: order_by
}

"""aggregate min on columns"""
type roleAssignment_min_fields {
  accountId: uuid
  created_at: timestamptz
  eventId: String
  id: uuid
  invitedById: uuid
  organizerId: String
}

"""
order by min() on columns of table "roleAssignment"
"""
input roleAssignment_min_order_by {
  accountId: order_by
  created_at: order_by
  eventId: order_by
  id: order_by
  invitedById: order_by
  organizerId: order_by
}

"""
response of any mutation on the table "roleAssignment"
"""
type roleAssignment_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [roleAssignment!]!
}

"""
on_conflict condition type for table "roleAssignment"
"""
input roleAssignment_on_conflict {
  constraint: roleAssignment_constraint!
  update_columns: [roleAssignment_update_column!]! = []
  where: roleAssignment_bool_exp
}

"""Ordering options when selecting data from "roleAssignment"."""
input roleAssignment_order_by {
  accountId: order_by
  created_at: order_by
  eventId: order_by
  id: order_by
  invitedById: order_by
  inviter: account_order_by
  organizerId: order_by
  role: order_by
}

"""
select columns of table "roleAssignment"
"""
enum roleAssignment_select_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  eventId

  """column name"""
  id

  """column name"""
  invitedById

  """column name"""
  organizerId

  """column name"""
  role
}

"""
input type for updating data in table "roleAssignment"
"""
input roleAssignment_set_input {
  accountId: uuid
  created_at: timestamptz
  eventId: String
  id: uuid
  invitedById: uuid
  organizerId: String
  role: roles_enum
}

"""
Streaming cursor of the table "roleAssignment"
"""
input roleAssignment_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: roleAssignment_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input roleAssignment_stream_cursor_value_input {
  accountId: uuid
  created_at: timestamptz
  eventId: String
  id: uuid
  invitedById: uuid
  organizerId: String
  role: roles_enum
}

"""
update columns of table "roleAssignment"
"""
enum roleAssignment_update_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  eventId

  """column name"""
  id

  """column name"""
  invitedById

  """column name"""
  organizerId

  """column name"""
  role
}

input roleAssignment_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: roleAssignment_set_input

  """filter the rows which have to be updated"""
  where: roleAssignment_bool_exp!
}

"""
Stores user roles defining access levels and permissions within the Offline platform.
"""
type roles {
  "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
  value: String!
}

"""
aggregated selection of "roles"
"""
type roles_aggregate {
  aggregate: roles_aggregate_fields
  nodes: [roles!]!
}

"""
aggregate fields of "roles"
"""
type roles_aggregate_fields {
  count(columns: [roles_select_column!], distinct: Boolean): Int!
  max: roles_max_fields
  min: roles_min_fields
}

"""
Boolean expression to filter rows from the table "roles". All fields are combined with a logical 'AND'.
"""
input roles_bool_exp {
  _and: [roles_bool_exp!]
  _not: roles_bool_exp
  _or: [roles_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "roles"
"""
enum roles_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  roles_pkey
}

enum roles_enum {
  organizer_admin
  organizer_auditor
  organizer_content_manager
  organizer_finance_manager
  organizer_guest
  organizer_human_resources
  organizer_operations_manager
  organizer_super_admin
  organizer_validator
}

"""
Boolean expression to compare columns of type "roles_enum". All fields are combined with logical 'AND'.
"""
input roles_enum_comparison_exp {
  _eq: roles_enum
  _in: [roles_enum!]
  _is_null: Boolean
  _neq: roles_enum
  _nin: [roles_enum!]
}

"""
input type for inserting data into table "roles"
"""
input roles_insert_input {
  "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
  value: String
}

"""aggregate max on columns"""
type roles_max_fields {
  "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
  value: String
}

"""aggregate min on columns"""
type roles_min_fields {
  "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
  value: String
}

"""
response of any mutation on the table "roles"
"""
type roles_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [roles!]!
}

"""
on_conflict condition type for table "roles"
"""
input roles_on_conflict {
  constraint: roles_constraint!
  update_columns: [roles_update_column!]! = []
  where: roles_bool_exp
}

"""Ordering options when selecting data from "roles"."""
input roles_order_by {
  value: order_by
}

"""primary key columns input for table: roles"""
input roles_pk_columns_input {
  "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
  value: String!
}

"""
select columns of table "roles"
"""
enum roles_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "roles"
"""
input roles_set_input {
  "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
  value: String
}

"""
Streaming cursor of the table "roles"
"""
input roles_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: roles_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input roles_stream_cursor_value_input {
  "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
  value: String
}

"""
update columns of table "roles"
"""
enum roles_update_column {
  """column name"""
  value
}

input roles_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: roles_set_input

  """filter the rows which have to be updated"""
  where: roles_bool_exp!
}

"""
Table to store Stripe Checkout Sessions for tracking user checkout processes. Sessions are deleted once they are successful or expired.
"""
type stripeCheckoutSession {
  """Timestamp automatically set when the row is created."""
  created_at: timestamptz!

  """Stripe Customer ID referencing to the stripeCustomer table."""
  stripeCustomerId: String!

  """Unique identifier for the Stripe Checkout Session."""
  stripeSessionId: String!

  """
  Type of the Stripe Checkout Session. Default is event_pass_order. References to the stripeCheckoutSessionType table.
  """
  type: stripeCheckoutSessionType_enum!

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz!
}

"""Types of Stripe Checkout Sessions."""
type stripeCheckoutSessionType {
  """Type value."""
  value: String!
}

"""
aggregated selection of "stripeCheckoutSessionType"
"""
type stripeCheckoutSessionType_aggregate {
  aggregate: stripeCheckoutSessionType_aggregate_fields
  nodes: [stripeCheckoutSessionType!]!
}

"""
aggregate fields of "stripeCheckoutSessionType"
"""
type stripeCheckoutSessionType_aggregate_fields {
  count(columns: [stripeCheckoutSessionType_select_column!], distinct: Boolean): Int!
  max: stripeCheckoutSessionType_max_fields
  min: stripeCheckoutSessionType_min_fields
}

"""
Boolean expression to filter rows from the table "stripeCheckoutSessionType". All fields are combined with a logical 'AND'.
"""
input stripeCheckoutSessionType_bool_exp {
  _and: [stripeCheckoutSessionType_bool_exp!]
  _not: stripeCheckoutSessionType_bool_exp
  _or: [stripeCheckoutSessionType_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "stripeCheckoutSessionType"
"""
enum stripeCheckoutSessionType_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  stripeCheckoutSessionType_pkey
}

enum stripeCheckoutSessionType_enum {
  event_pass_order
}

"""
Boolean expression to compare columns of type "stripeCheckoutSessionType_enum". All fields are combined with logical 'AND'.
"""
input stripeCheckoutSessionType_enum_comparison_exp {
  _eq: stripeCheckoutSessionType_enum
  _in: [stripeCheckoutSessionType_enum!]
  _is_null: Boolean
  _neq: stripeCheckoutSessionType_enum
  _nin: [stripeCheckoutSessionType_enum!]
}

"""
input type for inserting data into table "stripeCheckoutSessionType"
"""
input stripeCheckoutSessionType_insert_input {
  """Type value."""
  value: String
}

"""aggregate max on columns"""
type stripeCheckoutSessionType_max_fields {
  """Type value."""
  value: String
}

"""aggregate min on columns"""
type stripeCheckoutSessionType_min_fields {
  """Type value."""
  value: String
}

"""
response of any mutation on the table "stripeCheckoutSessionType"
"""
type stripeCheckoutSessionType_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [stripeCheckoutSessionType!]!
}

"""
on_conflict condition type for table "stripeCheckoutSessionType"
"""
input stripeCheckoutSessionType_on_conflict {
  constraint: stripeCheckoutSessionType_constraint!
  update_columns: [stripeCheckoutSessionType_update_column!]! = []
  where: stripeCheckoutSessionType_bool_exp
}

"""Ordering options when selecting data from "stripeCheckoutSessionType"."""
input stripeCheckoutSessionType_order_by {
  value: order_by
}

"""primary key columns input for table: stripeCheckoutSessionType"""
input stripeCheckoutSessionType_pk_columns_input {
  """Type value."""
  value: String!
}

"""
select columns of table "stripeCheckoutSessionType"
"""
enum stripeCheckoutSessionType_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "stripeCheckoutSessionType"
"""
input stripeCheckoutSessionType_set_input {
  """Type value."""
  value: String
}

"""
Streaming cursor of the table "stripeCheckoutSessionType"
"""
input stripeCheckoutSessionType_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: stripeCheckoutSessionType_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input stripeCheckoutSessionType_stream_cursor_value_input {
  """Type value."""
  value: String
}

"""
update columns of table "stripeCheckoutSessionType"
"""
enum stripeCheckoutSessionType_update_column {
  """column name"""
  value
}

input stripeCheckoutSessionType_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: stripeCheckoutSessionType_set_input

  """filter the rows which have to be updated"""
  where: stripeCheckoutSessionType_bool_exp!
}

"""
aggregated selection of "stripeCheckoutSession"
"""
type stripeCheckoutSession_aggregate {
  aggregate: stripeCheckoutSession_aggregate_fields
  nodes: [stripeCheckoutSession!]!
}

"""
aggregate fields of "stripeCheckoutSession"
"""
type stripeCheckoutSession_aggregate_fields {
  count(columns: [stripeCheckoutSession_select_column!], distinct: Boolean): Int!
  max: stripeCheckoutSession_max_fields
  min: stripeCheckoutSession_min_fields
}

"""
Boolean expression to filter rows from the table "stripeCheckoutSession". All fields are combined with a logical 'AND'.
"""
input stripeCheckoutSession_bool_exp {
  _and: [stripeCheckoutSession_bool_exp!]
  _not: stripeCheckoutSession_bool_exp
  _or: [stripeCheckoutSession_bool_exp!]
  created_at: timestamptz_comparison_exp
  stripeCustomerId: String_comparison_exp
  stripeSessionId: String_comparison_exp
  type: stripeCheckoutSessionType_enum_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "stripeCheckoutSession"
"""
enum stripeCheckoutSession_constraint {
  """
  unique or primary key constraint on columns "stripeSessionId"
  """
  stripeCheckoutSession_pkey
}

"""
input type for inserting data into table "stripeCheckoutSession"
"""
input stripeCheckoutSession_insert_input {
  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Stripe Customer ID referencing to the stripeCustomer table."""
  stripeCustomerId: String

  """Unique identifier for the Stripe Checkout Session."""
  stripeSessionId: String

  """
  Type of the Stripe Checkout Session. Default is event_pass_order. References to the stripeCheckoutSessionType table.
  """
  type: stripeCheckoutSessionType_enum

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""aggregate max on columns"""
type stripeCheckoutSession_max_fields {
  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Stripe Customer ID referencing to the stripeCustomer table."""
  stripeCustomerId: String

  """Unique identifier for the Stripe Checkout Session."""
  stripeSessionId: String

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""aggregate min on columns"""
type stripeCheckoutSession_min_fields {
  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Stripe Customer ID referencing to the stripeCustomer table."""
  stripeCustomerId: String

  """Unique identifier for the Stripe Checkout Session."""
  stripeSessionId: String

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""
response of any mutation on the table "stripeCheckoutSession"
"""
type stripeCheckoutSession_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [stripeCheckoutSession!]!
}

"""
on_conflict condition type for table "stripeCheckoutSession"
"""
input stripeCheckoutSession_on_conflict {
  constraint: stripeCheckoutSession_constraint!
  update_columns: [stripeCheckoutSession_update_column!]! = []
  where: stripeCheckoutSession_bool_exp
}

"""Ordering options when selecting data from "stripeCheckoutSession"."""
input stripeCheckoutSession_order_by {
  created_at: order_by
  stripeCustomerId: order_by
  stripeSessionId: order_by
  type: order_by
  updated_at: order_by
}

"""primary key columns input for table: stripeCheckoutSession"""
input stripeCheckoutSession_pk_columns_input {
  """Unique identifier for the Stripe Checkout Session."""
  stripeSessionId: String!
}

"""
select columns of table "stripeCheckoutSession"
"""
enum stripeCheckoutSession_select_column {
  """column name"""
  created_at

  """column name"""
  stripeCustomerId

  """column name"""
  stripeSessionId

  """column name"""
  type

  """column name"""
  updated_at
}

"""
input type for updating data in table "stripeCheckoutSession"
"""
input stripeCheckoutSession_set_input {
  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Stripe Customer ID referencing to the stripeCustomer table."""
  stripeCustomerId: String

  """Unique identifier for the Stripe Checkout Session."""
  stripeSessionId: String

  """
  Type of the Stripe Checkout Session. Default is event_pass_order. References to the stripeCheckoutSessionType table.
  """
  type: stripeCheckoutSessionType_enum

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""
Streaming cursor of the table "stripeCheckoutSession"
"""
input stripeCheckoutSession_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: stripeCheckoutSession_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input stripeCheckoutSession_stream_cursor_value_input {
  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Stripe Customer ID referencing to the stripeCustomer table."""
  stripeCustomerId: String

  """Unique identifier for the Stripe Checkout Session."""
  stripeSessionId: String

  """
  Type of the Stripe Checkout Session. Default is event_pass_order. References to the stripeCheckoutSessionType table.
  """
  type: stripeCheckoutSessionType_enum

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""
update columns of table "stripeCheckoutSession"
"""
enum stripeCheckoutSession_update_column {
  """column name"""
  created_at

  """column name"""
  stripeCustomerId

  """column name"""
  stripeSessionId

  """column name"""
  type

  """column name"""
  updated_at
}

input stripeCheckoutSession_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: stripeCheckoutSession_set_input

  """filter the rows which have to be updated"""
  where: stripeCheckoutSession_bool_exp!
}

"""
Table to store Stripe Customer IDs for tracking user payment processes.
"""
type stripeCustomer {
  """UUID referencing to the account ID in the existing accounts table."""
  accountId: uuid!

  """Timestamp automatically set when the row is created."""
  created_at: timestamptz!

  """Unique identifier for the Stripe Customer."""
  stripeCustomerId: String!

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz!
}

"""
aggregated selection of "stripeCustomer"
"""
type stripeCustomer_aggregate {
  aggregate: stripeCustomer_aggregate_fields
  nodes: [stripeCustomer!]!
}

"""
aggregate fields of "stripeCustomer"
"""
type stripeCustomer_aggregate_fields {
  count(columns: [stripeCustomer_select_column!], distinct: Boolean): Int!
  max: stripeCustomer_max_fields
  min: stripeCustomer_min_fields
}

"""
Boolean expression to filter rows from the table "stripeCustomer". All fields are combined with a logical 'AND'.
"""
input stripeCustomer_bool_exp {
  _and: [stripeCustomer_bool_exp!]
  _not: stripeCustomer_bool_exp
  _or: [stripeCustomer_bool_exp!]
  accountId: uuid_comparison_exp
  created_at: timestamptz_comparison_exp
  stripeCustomerId: String_comparison_exp
  updated_at: timestamptz_comparison_exp
}

"""
unique or primary key constraints on table "stripeCustomer"
"""
enum stripeCustomer_constraint {
  """
  unique or primary key constraint on columns "stripeCustomerId"
  """
  stripeCustomer_pkey
}

"""
input type for inserting data into table "stripeCustomer"
"""
input stripeCustomer_insert_input {
  """UUID referencing to the account ID in the existing accounts table."""
  accountId: uuid

  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Unique identifier for the Stripe Customer."""
  stripeCustomerId: String

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""aggregate max on columns"""
type stripeCustomer_max_fields {
  """UUID referencing to the account ID in the existing accounts table."""
  accountId: uuid

  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Unique identifier for the Stripe Customer."""
  stripeCustomerId: String

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""aggregate min on columns"""
type stripeCustomer_min_fields {
  """UUID referencing to the account ID in the existing accounts table."""
  accountId: uuid

  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Unique identifier for the Stripe Customer."""
  stripeCustomerId: String

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""
response of any mutation on the table "stripeCustomer"
"""
type stripeCustomer_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [stripeCustomer!]!
}

"""
input type for inserting object relation for remote table "stripeCustomer"
"""
input stripeCustomer_obj_rel_insert_input {
  data: stripeCustomer_insert_input!

  """upsert condition"""
  on_conflict: stripeCustomer_on_conflict
}

"""
on_conflict condition type for table "stripeCustomer"
"""
input stripeCustomer_on_conflict {
  constraint: stripeCustomer_constraint!
  update_columns: [stripeCustomer_update_column!]! = []
  where: stripeCustomer_bool_exp
}

"""Ordering options when selecting data from "stripeCustomer"."""
input stripeCustomer_order_by {
  accountId: order_by
  created_at: order_by
  stripeCustomerId: order_by
  updated_at: order_by
}

"""primary key columns input for table: stripeCustomer"""
input stripeCustomer_pk_columns_input {
  """Unique identifier for the Stripe Customer."""
  stripeCustomerId: String!
}

"""
select columns of table "stripeCustomer"
"""
enum stripeCustomer_select_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  stripeCustomerId

  """column name"""
  updated_at
}

"""
input type for updating data in table "stripeCustomer"
"""
input stripeCustomer_set_input {
  """UUID referencing to the account ID in the existing accounts table."""
  accountId: uuid

  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Unique identifier for the Stripe Customer."""
  stripeCustomerId: String

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""
Streaming cursor of the table "stripeCustomer"
"""
input stripeCustomer_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: stripeCustomer_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input stripeCustomer_stream_cursor_value_input {
  """UUID referencing to the account ID in the existing accounts table."""
  accountId: uuid

  """Timestamp automatically set when the row is created."""
  created_at: timestamptz

  """Unique identifier for the Stripe Customer."""
  stripeCustomerId: String

  """Timestamp automatically updated whenever the row changes."""
  updated_at: timestamptz
}

"""
update columns of table "stripeCustomer"
"""
enum stripeCustomer_update_column {
  """column name"""
  accountId

  """column name"""
  created_at

  """column name"""
  stripeCustomerId

  """column name"""
  updated_at
}

input stripeCustomer_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: stripeCustomer_set_input

  """filter the rows which have to be updated"""
  where: stripeCustomer_bool_exp!
}

type subscription_root {
  """
  fetch data from the table: "account"
  """
  account(
    """distinct select on columns"""
    distinct_on: [account_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [account_order_by!]

    """filter the rows returned"""
    where: account_bool_exp
  ): [account!]!

  """
  fetch aggregated fields from the table: "account"
  """
  account_aggregate(
    """distinct select on columns"""
    distinct_on: [account_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [account_order_by!]

    """filter the rows returned"""
    where: account_bool_exp
  ): account_aggregate!

  """fetch data from the table: "account" using primary key columns"""
  account_by_pk(id: uuid!): account

  """
  fetch data from the table in a streaming manner: "account"
  """
  account_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [account_stream_cursor_input]!

    """filter the rows returned"""
    where: account_bool_exp
  ): [account!]!

  """
  fetch data from the table: "currency"
  """
  currency(
    """distinct select on columns"""
    distinct_on: [currency_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [currency_order_by!]

    """filter the rows returned"""
    where: currency_bool_exp
  ): [currency!]!

  """
  fetch aggregated fields from the table: "currency"
  """
  currency_aggregate(
    """distinct select on columns"""
    distinct_on: [currency_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [currency_order_by!]

    """filter the rows returned"""
    where: currency_bool_exp
  ): currency_aggregate!

  """fetch data from the table: "currency" using primary key columns"""
  currency_by_pk(value: String!): currency

  """
  fetch data from the table in a streaming manner: "currency"
  """
  currency_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [currency_stream_cursor_input]!

    """filter the rows returned"""
    where: currency_bool_exp
  ): [currency!]!

  """
  fetch data from the table: "eventParameters"
  """
  eventParameters(
    """distinct select on columns"""
    distinct_on: [eventParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventParameters_order_by!]

    """filter the rows returned"""
    where: eventParameters_bool_exp
  ): [eventParameters!]!

  """
  fetch aggregated fields from the table: "eventParameters"
  """
  eventParameters_aggregate(
    """distinct select on columns"""
    distinct_on: [eventParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventParameters_order_by!]

    """filter the rows returned"""
    where: eventParameters_bool_exp
  ): eventParameters_aggregate!

  """fetch data from the table: "eventParameters" using primary key columns"""
  eventParameters_by_pk(id: uuid!): eventParameters

  """
  fetch data from the table in a streaming manner: "eventParameters"
  """
  eventParameters_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventParameters_stream_cursor_input]!

    """filter the rows returned"""
    where: eventParameters_bool_exp
  ): [eventParameters!]!

  """
  fetch data from the table: "eventPassNft"
  """
  eventPassNft(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): [eventPassNft!]!

  """
  fetch data from the table: "eventPassNftContract"
  """
  eventPassNftContract(
    """distinct select on columns"""
    distinct_on: [eventPassNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContract_order_by!]

    """filter the rows returned"""
    where: eventPassNftContract_bool_exp
  ): [eventPassNftContract!]!

  """
  fetch data from the table: "eventPassNftContractType"
  """
  eventPassNftContractType(
    """distinct select on columns"""
    distinct_on: [eventPassNftContractType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContractType_order_by!]

    """filter the rows returned"""
    where: eventPassNftContractType_bool_exp
  ): [eventPassNftContractType!]!

  """
  fetch aggregated fields from the table: "eventPassNftContractType"
  """
  eventPassNftContractType_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNftContractType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContractType_order_by!]

    """filter the rows returned"""
    where: eventPassNftContractType_bool_exp
  ): eventPassNftContractType_aggregate!

  """
  fetch data from the table: "eventPassNftContractType" using primary key columns
  """
  eventPassNftContractType_by_pk(
    """Type name for event pass NFT contract."""
    value: String!
  ): eventPassNftContractType

  """
  fetch data from the table in a streaming manner: "eventPassNftContractType"
  """
  eventPassNftContractType_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventPassNftContractType_stream_cursor_input]!

    """filter the rows returned"""
    where: eventPassNftContractType_bool_exp
  ): [eventPassNftContractType!]!

  """
  fetch aggregated fields from the table: "eventPassNftContract"
  """
  eventPassNftContract_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNftContract_order_by!]

    """filter the rows returned"""
    where: eventPassNftContract_bool_exp
  ): eventPassNftContract_aggregate!

  """
  fetch data from the table: "eventPassNftContract" using primary key columns
  """
  eventPassNftContract_by_pk(id: uuid!): eventPassNftContract

  """
  fetch data from the table in a streaming manner: "eventPassNftContract"
  """
  eventPassNftContract_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventPassNftContract_stream_cursor_input]!

    """filter the rows returned"""
    where: eventPassNftContract_bool_exp
  ): [eventPassNftContract!]!

  """
  fetch aggregated fields from the table: "eventPassNft"
  """
  eventPassNft_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassNft_order_by!]

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): eventPassNft_aggregate!

  """fetch data from the table: "eventPassNft" using primary key columns"""
  eventPassNft_by_pk(id: uuid!): eventPassNft

  """
  fetch data from the table in a streaming manner: "eventPassNft"
  """
  eventPassNft_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventPassNft_stream_cursor_input]!

    """filter the rows returned"""
    where: eventPassNft_bool_exp
  ): [eventPassNft!]!

  """
  fetch data from the table: "eventPassOrderSums"
  """
  eventPassOrderSums(
    """distinct select on columns"""
    distinct_on: [eventPassOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassOrderSums_order_by!]

    """filter the rows returned"""
    where: eventPassOrderSums_bool_exp
  ): [eventPassOrderSums!]!

  """
  fetch aggregated fields from the table: "eventPassOrderSums"
  """
  eventPassOrderSums_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassOrderSums_order_by!]

    """filter the rows returned"""
    where: eventPassOrderSums_bool_exp
  ): eventPassOrderSums_aggregate!

  """
  fetch data from the table: "eventPassOrderSums" using primary key columns
  """
  eventPassOrderSums_by_pk(eventPassId: String!): eventPassOrderSums

  """
  fetch data from the table in a streaming manner: "eventPassOrderSums"
  """
  eventPassOrderSums_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventPassOrderSums_stream_cursor_input]!

    """filter the rows returned"""
    where: eventPassOrderSums_bool_exp
  ): [eventPassOrderSums!]!

  """
  fetch data from the table: "eventPassType"
  """
  eventPassType(
    """distinct select on columns"""
    distinct_on: [eventPassType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassType_order_by!]

    """filter the rows returned"""
    where: eventPassType_bool_exp
  ): [eventPassType!]!

  """
  fetch aggregated fields from the table: "eventPassType"
  """
  eventPassType_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassType_order_by!]

    """filter the rows returned"""
    where: eventPassType_bool_exp
  ): eventPassType_aggregate!

  """fetch data from the table: "eventPassType" using primary key columns"""
  eventPassType_by_pk(
    """Type name for event pass."""
    value: String!
  ): eventPassType

  """
  fetch data from the table in a streaming manner: "eventPassType"
  """
  eventPassType_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventPassType_stream_cursor_input]!

    """filter the rows returned"""
    where: eventPassType_bool_exp
  ): [eventPassType!]!

  """
  fetch data from the table: "eventPassValidationType"
  """
  eventPassValidationType(
    """distinct select on columns"""
    distinct_on: [eventPassValidationType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassValidationType_order_by!]

    """filter the rows returned"""
    where: eventPassValidationType_bool_exp
  ): [eventPassValidationType!]!

  """
  fetch aggregated fields from the table: "eventPassValidationType"
  """
  eventPassValidationType_aggregate(
    """distinct select on columns"""
    distinct_on: [eventPassValidationType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventPassValidationType_order_by!]

    """filter the rows returned"""
    where: eventPassValidationType_bool_exp
  ): eventPassValidationType_aggregate!

  """
  fetch data from the table: "eventPassValidationType" using primary key columns
  """
  eventPassValidationType_by_pk(
    """Type name for event pass validation."""
    value: String!
  ): eventPassValidationType

  """
  fetch data from the table in a streaming manner: "eventPassValidationType"
  """
  eventPassValidationType_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventPassValidationType_stream_cursor_input]!

    """filter the rows returned"""
    where: eventPassValidationType_bool_exp
  ): [eventPassValidationType!]!

  """
  fetch data from the table: "eventStatus"
  """
  eventStatus(
    """distinct select on columns"""
    distinct_on: [eventStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventStatus_order_by!]

    """filter the rows returned"""
    where: eventStatus_bool_exp
  ): [eventStatus!]!

  """
  fetch aggregated fields from the table: "eventStatus"
  """
  eventStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [eventStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [eventStatus_order_by!]

    """filter the rows returned"""
    where: eventStatus_bool_exp
  ): eventStatus_aggregate!

  """fetch data from the table: "eventStatus" using primary key columns"""
  eventStatus_by_pk(value: String!): eventStatus

  """
  fetch data from the table in a streaming manner: "eventStatus"
  """
  eventStatus_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [eventStatus_stream_cursor_input]!

    """filter the rows returned"""
    where: eventStatus_bool_exp
  ): [eventStatus!]!

  """
  fetch data from the table: "follow"
  """
  follow(
    """distinct select on columns"""
    distinct_on: [follow_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [follow_order_by!]

    """filter the rows returned"""
    where: follow_bool_exp
  ): [follow!]!

  """
  fetch aggregated fields from the table: "follow"
  """
  follow_aggregate(
    """distinct select on columns"""
    distinct_on: [follow_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [follow_order_by!]

    """filter the rows returned"""
    where: follow_bool_exp
  ): follow_aggregate!

  """fetch data from the table: "follow" using primary key columns"""
  follow_by_pk(
    """
    References the unique identifier of the account that is following an organizer.
    """
    accountId: uuid!

    """
    Represents the unique slug of the organizer being followed. Slugs are user-friendly identifiers that uniquely identify organizers.
    """
    organizerSlug: String!
  ): follow

  """
  fetch data from the table in a streaming manner: "follow"
  """
  follow_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [follow_stream_cursor_input]!

    """filter the rows returned"""
    where: follow_bool_exp
  ): [follow!]!

  """
  fetch data from the table: "kyc"
  """
  kyc(
    """distinct select on columns"""
    distinct_on: [kyc_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kyc_order_by!]

    """filter the rows returned"""
    where: kyc_bool_exp
  ): [kyc!]!

  """
  fetch data from the table: "kycLevelName"
  """
  kycLevelName(
    """distinct select on columns"""
    distinct_on: [kycLevelName_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycLevelName_order_by!]

    """filter the rows returned"""
    where: kycLevelName_bool_exp
  ): [kycLevelName!]!

  """
  fetch aggregated fields from the table: "kycLevelName"
  """
  kycLevelName_aggregate(
    """distinct select on columns"""
    distinct_on: [kycLevelName_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycLevelName_order_by!]

    """filter the rows returned"""
    where: kycLevelName_bool_exp
  ): kycLevelName_aggregate!

  """fetch data from the table: "kycLevelName" using primary key columns"""
  kycLevelName_by_pk(
    """
    basic_kyc_level: Basic level of KYC verification.
    advanced_kyc_level: Advanced level of KYC verification.
    """
    value: String!
  ): kycLevelName

  """
  fetch data from the table in a streaming manner: "kycLevelName"
  """
  kycLevelName_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [kycLevelName_stream_cursor_input]!

    """filter the rows returned"""
    where: kycLevelName_bool_exp
  ): [kycLevelName!]!

  """
  fetch data from the table: "kycStatus"
  """
  kycStatus(
    """distinct select on columns"""
    distinct_on: [kycStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycStatus_order_by!]

    """filter the rows returned"""
    where: kycStatus_bool_exp
  ): [kycStatus!]!

  """
  fetch aggregated fields from the table: "kycStatus"
  """
  kycStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [kycStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kycStatus_order_by!]

    """filter the rows returned"""
    where: kycStatus_bool_exp
  ): kycStatus_aggregate!

  """fetch data from the table: "kycStatus" using primary key columns"""
  kycStatus_by_pk(
    """
    init: Initial registration has started. A client is still in the process of filling out the applicant profile. Not all required documents are currently uploaded.
    pending: An applicant is ready to be processed.
    prechecked: The check is in a half way of being finished.
    queued: The checks have been started for the applicant.
    completed: The check has been completed.
    onHold: Applicant waits for a final decision from compliance officer or waits for all beneficiaries to pass KYC in case of company verification.
    """
    value: String!
  ): kycStatus

  """
  fetch data from the table in a streaming manner: "kycStatus"
  """
  kycStatus_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [kycStatus_stream_cursor_input]!

    """filter the rows returned"""
    where: kycStatus_bool_exp
  ): [kycStatus!]!

  """
  fetch aggregated fields from the table: "kyc"
  """
  kyc_aggregate(
    """distinct select on columns"""
    distinct_on: [kyc_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [kyc_order_by!]

    """filter the rows returned"""
    where: kyc_bool_exp
  ): kyc_aggregate!

  """fetch data from the table: "kyc" using primary key columns"""
  kyc_by_pk(
    """UUID referencing the user ID in the existing accounts table."""
    externalUserId: uuid!
  ): kyc

  """
  fetch data from the table in a streaming manner: "kyc"
  """
  kyc_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [kyc_stream_cursor_input]!

    """filter the rows returned"""
    where: kyc_bool_exp
  ): [kyc!]!

  """
  fetch data from the table: "lotteryParameters"
  """
  lotteryParameters(
    """distinct select on columns"""
    distinct_on: [lotteryParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryParameters_order_by!]

    """filter the rows returned"""
    where: lotteryParameters_bool_exp
  ): [lotteryParameters!]!

  """
  fetch aggregated fields from the table: "lotteryParameters"
  """
  lotteryParameters_aggregate(
    """distinct select on columns"""
    distinct_on: [lotteryParameters_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryParameters_order_by!]

    """filter the rows returned"""
    where: lotteryParameters_bool_exp
  ): lotteryParameters_aggregate!

  """
  fetch data from the table: "lotteryParameters" using primary key columns
  """
  lotteryParameters_by_pk(id: uuid!): lotteryParameters

  """
  fetch data from the table in a streaming manner: "lotteryParameters"
  """
  lotteryParameters_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [lotteryParameters_stream_cursor_input]!

    """filter the rows returned"""
    where: lotteryParameters_bool_exp
  ): [lotteryParameters!]!

  """
  fetch data from the table: "lotteryStatus"
  """
  lotteryStatus(
    """distinct select on columns"""
    distinct_on: [lotteryStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryStatus_order_by!]

    """filter the rows returned"""
    where: lotteryStatus_bool_exp
  ): [lotteryStatus!]!

  """
  fetch aggregated fields from the table: "lotteryStatus"
  """
  lotteryStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [lotteryStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [lotteryStatus_order_by!]

    """filter the rows returned"""
    where: lotteryStatus_bool_exp
  ): lotteryStatus_aggregate!

  """fetch data from the table: "lotteryStatus" using primary key columns"""
  lotteryStatus_by_pk(value: String!): lotteryStatus

  """
  fetch data from the table in a streaming manner: "lotteryStatus"
  """
  lotteryStatus_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [lotteryStatus_stream_cursor_input]!

    """filter the rows returned"""
    where: lotteryStatus_bool_exp
  ): [lotteryStatus!]!

  """
  fetch data from the table: "nftTransfer"
  """
  nftTransfer(
    """distinct select on columns"""
    distinct_on: [nftTransfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [nftTransfer_order_by!]

    """filter the rows returned"""
    where: nftTransfer_bool_exp
  ): [nftTransfer!]!

  """
  fetch aggregated fields from the table: "nftTransfer"
  """
  nftTransfer_aggregate(
    """distinct select on columns"""
    distinct_on: [nftTransfer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [nftTransfer_order_by!]

    """filter the rows returned"""
    where: nftTransfer_bool_exp
  ): nftTransfer_aggregate!

  """fetch data from the table: "nftTransfer" using primary key columns"""
  nftTransfer_by_pk(id: uuid!): nftTransfer

  """
  fetch data from the table in a streaming manner: "nftTransfer"
  """
  nftTransfer_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [nftTransfer_stream_cursor_input]!

    """filter the rows returned"""
    where: nftTransfer_bool_exp
  ): [nftTransfer!]!

  """
  fetch data from the table: "order"
  """
  order(
    """distinct select on columns"""
    distinct_on: [order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [order_order_by!]

    """filter the rows returned"""
    where: order_bool_exp
  ): [order!]!

  """
  fetch data from the table: "orderStatus"
  """
  orderStatus(
    """distinct select on columns"""
    distinct_on: [orderStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [orderStatus_order_by!]

    """filter the rows returned"""
    where: orderStatus_bool_exp
  ): [orderStatus!]!

  """
  fetch aggregated fields from the table: "orderStatus"
  """
  orderStatus_aggregate(
    """distinct select on columns"""
    distinct_on: [orderStatus_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [orderStatus_order_by!]

    """filter the rows returned"""
    where: orderStatus_bool_exp
  ): orderStatus_aggregate!

  """fetch data from the table: "orderStatus" using primary key columns"""
  orderStatus_by_pk(value: String!): orderStatus

  """
  fetch data from the table in a streaming manner: "orderStatus"
  """
  orderStatus_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [orderStatus_stream_cursor_input]!

    """filter the rows returned"""
    where: orderStatus_bool_exp
  ): [orderStatus!]!

  """
  fetch aggregated fields from the table: "order"
  """
  order_aggregate(
    """distinct select on columns"""
    distinct_on: [order_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [order_order_by!]

    """filter the rows returned"""
    where: order_bool_exp
  ): order_aggregate!

  """fetch data from the table: "order" using primary key columns"""
  order_by_pk(id: uuid!): order

  """
  fetch data from the table in a streaming manner: "order"
  """
  order_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [order_stream_cursor_input]!

    """filter the rows returned"""
    where: order_bool_exp
  ): [order!]!

  """
  fetch data from the table: "packEventPassNft"
  """
  packEventPassNft(
    """distinct select on columns"""
    distinct_on: [packEventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packEventPassNft_order_by!]

    """filter the rows returned"""
    where: packEventPassNft_bool_exp
  ): [packEventPassNft!]!

  """
  fetch aggregated fields from the table: "packEventPassNft"
  """
  packEventPassNft_aggregate(
    """distinct select on columns"""
    distinct_on: [packEventPassNft_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packEventPassNft_order_by!]

    """filter the rows returned"""
    where: packEventPassNft_bool_exp
  ): packEventPassNft_aggregate!

  """
  fetch data from the table: "packEventPassNft" using primary key columns
  """
  packEventPassNft_by_pk(
    """Identifier for the event pass NFT."""
    eventPassNftId: uuid!

    """Identifier for the pack NFT supply."""
    packNftSupplyId: uuid!
  ): packEventPassNft

  """
  fetch data from the table in a streaming manner: "packEventPassNft"
  """
  packEventPassNft_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [packEventPassNft_stream_cursor_input]!

    """filter the rows returned"""
    where: packEventPassNft_bool_exp
  ): [packEventPassNft!]!

  """
  fetch data from the table: "packNftContract"
  """
  packNftContract(
    """distinct select on columns"""
    distinct_on: [packNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContract_order_by!]

    """filter the rows returned"""
    where: packNftContract_bool_exp
  ): [packNftContract!]!

  """
  fetch data from the table: "packNftContractEventPass"
  """
  packNftContractEventPass(
    """distinct select on columns"""
    distinct_on: [packNftContractEventPass_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContractEventPass_order_by!]

    """filter the rows returned"""
    where: packNftContractEventPass_bool_exp
  ): [packNftContractEventPass!]!

  """
  fetch aggregated fields from the table: "packNftContractEventPass"
  """
  packNftContractEventPass_aggregate(
    """distinct select on columns"""
    distinct_on: [packNftContractEventPass_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContractEventPass_order_by!]

    """filter the rows returned"""
    where: packNftContractEventPass_bool_exp
  ): packNftContractEventPass_aggregate!

  """
  fetch data from the table: "packNftContractEventPass" using primary key columns
  """
  packNftContractEventPass_by_pk(
    """
    Identifier for the event pass. This field specifies which event pass is included in the pack, referring to a unique identifier within the eventPassNftContract table.
    """
    eventPassId: String!

    """
    Identifier for the pack NFT contract. This field links to the packNftContract table, establishing the connection between the pack and its contractual details.
    """
    packNftContractId: uuid!
  ): packNftContractEventPass

  """
  fetch data from the table in a streaming manner: "packNftContractEventPass"
  """
  packNftContractEventPass_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [packNftContractEventPass_stream_cursor_input]!

    """filter the rows returned"""
    where: packNftContractEventPass_bool_exp
  ): [packNftContractEventPass!]!

  """
  fetch aggregated fields from the table: "packNftContract"
  """
  packNftContract_aggregate(
    """distinct select on columns"""
    distinct_on: [packNftContract_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftContract_order_by!]

    """filter the rows returned"""
    where: packNftContract_bool_exp
  ): packNftContract_aggregate!

  """fetch data from the table: "packNftContract" using primary key columns"""
  packNftContract_by_pk(
    """Unique identifier for each pack NFT contract."""
    id: uuid!
  ): packNftContract

  """
  fetch data from the table in a streaming manner: "packNftContract"
  """
  packNftContract_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [packNftContract_stream_cursor_input]!

    """filter the rows returned"""
    where: packNftContract_bool_exp
  ): [packNftContract!]!

  """
  fetch data from the table: "packNftSupply"
  """
  packNftSupply(
    """distinct select on columns"""
    distinct_on: [packNftSupply_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftSupply_order_by!]

    """filter the rows returned"""
    where: packNftSupply_bool_exp
  ): [packNftSupply!]!

  """
  fetch aggregated fields from the table: "packNftSupply"
  """
  packNftSupply_aggregate(
    """distinct select on columns"""
    distinct_on: [packNftSupply_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packNftSupply_order_by!]

    """filter the rows returned"""
    where: packNftSupply_bool_exp
  ): packNftSupply_aggregate!

  """fetch data from the table: "packNftSupply" using primary key columns"""
  packNftSupply_by_pk(id: uuid!): packNftSupply

  """
  fetch data from the table in a streaming manner: "packNftSupply"
  """
  packNftSupply_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [packNftSupply_stream_cursor_input]!

    """filter the rows returned"""
    where: packNftSupply_bool_exp
  ): [packNftSupply!]!

  """
  fetch data from the table: "packOrderSums"
  """
  packOrderSums(
    """distinct select on columns"""
    distinct_on: [packOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packOrderSums_order_by!]

    """filter the rows returned"""
    where: packOrderSums_bool_exp
  ): [packOrderSums!]!

  """
  fetch aggregated fields from the table: "packOrderSums"
  """
  packOrderSums_aggregate(
    """distinct select on columns"""
    distinct_on: [packOrderSums_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [packOrderSums_order_by!]

    """filter the rows returned"""
    where: packOrderSums_bool_exp
  ): packOrderSums_aggregate!

  """fetch data from the table: "packOrderSums" using primary key columns"""
  packOrderSums_by_pk(packId: String!): packOrderSums

  """
  fetch data from the table in a streaming manner: "packOrderSums"
  """
  packOrderSums_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [packOrderSums_stream_cursor_input]!

    """filter the rows returned"""
    where: packOrderSums_bool_exp
  ): [packOrderSums!]!

  """
  fetch data from the table: "passAmount"
  """
  passAmount(
    """distinct select on columns"""
    distinct_on: [passAmount_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passAmount_order_by!]

    """filter the rows returned"""
    where: passAmount_bool_exp
  ): [passAmount!]!

  """
  fetch aggregated fields from the table: "passAmount"
  """
  passAmount_aggregate(
    """distinct select on columns"""
    distinct_on: [passAmount_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passAmount_order_by!]

    """filter the rows returned"""
    where: passAmount_bool_exp
  ): passAmount_aggregate!

  """fetch data from the table: "passAmount" using primary key columns"""
  passAmount_by_pk(id: uuid!): passAmount

  """
  fetch data from the table in a streaming manner: "passAmount"
  """
  passAmount_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [passAmount_stream_cursor_input]!

    """filter the rows returned"""
    where: passAmount_bool_exp
  ): [passAmount!]!

  """
  fetch data from the table: "passPricing"
  """
  passPricing(
    """distinct select on columns"""
    distinct_on: [passPricing_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passPricing_order_by!]

    """filter the rows returned"""
    where: passPricing_bool_exp
  ): [passPricing!]!

  """
  fetch aggregated fields from the table: "passPricing"
  """
  passPricing_aggregate(
    """distinct select on columns"""
    distinct_on: [passPricing_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [passPricing_order_by!]

    """filter the rows returned"""
    where: passPricing_bool_exp
  ): passPricing_aggregate!

  """fetch data from the table: "passPricing" using primary key columns"""
  passPricing_by_pk(id: uuid!): passPricing

  """
  fetch data from the table in a streaming manner: "passPricing"
  """
  passPricing_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [passPricing_stream_cursor_input]!

    """filter the rows returned"""
    where: passPricing_bool_exp
  ): [passPricing!]!

  """
  fetch data from the table: "pendingOrder"
  """
  pendingOrder(
    """distinct select on columns"""
    distinct_on: [pendingOrder_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [pendingOrder_order_by!]

    """filter the rows returned"""
    where: pendingOrder_bool_exp
  ): [pendingOrder!]!

  """
  fetch aggregated fields from the table: "pendingOrder"
  """
  pendingOrder_aggregate(
    """distinct select on columns"""
    distinct_on: [pendingOrder_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [pendingOrder_order_by!]

    """filter the rows returned"""
    where: pendingOrder_bool_exp
  ): pendingOrder_aggregate!

  """fetch data from the table: "pendingOrder" using primary key columns"""
  pendingOrder_by_pk(id: uuid!): pendingOrder

  """
  fetch data from the table in a streaming manner: "pendingOrder"
  """
  pendingOrder_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [pendingOrder_stream_cursor_input]!

    """filter the rows returned"""
    where: pendingOrder_bool_exp
  ): [pendingOrder!]!

  """
  fetch data from the table: "roleAssignment"
  """
  roleAssignment(
    """distinct select on columns"""
    distinct_on: [roleAssignment_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roleAssignment_order_by!]

    """filter the rows returned"""
    where: roleAssignment_bool_exp
  ): [roleAssignment!]!

  """
  fetch aggregated fields from the table: "roleAssignment"
  """
  roleAssignment_aggregate(
    """distinct select on columns"""
    distinct_on: [roleAssignment_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roleAssignment_order_by!]

    """filter the rows returned"""
    where: roleAssignment_bool_exp
  ): roleAssignment_aggregate!

  """
  fetch data from the table in a streaming manner: "roleAssignment"
  """
  roleAssignment_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [roleAssignment_stream_cursor_input]!

    """filter the rows returned"""
    where: roleAssignment_bool_exp
  ): [roleAssignment!]!

  """
  fetch data from the table: "roles"
  """
  roles(
    """distinct select on columns"""
    distinct_on: [roles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roles_order_by!]

    """filter the rows returned"""
    where: roles_bool_exp
  ): [roles!]!

  """
  fetch aggregated fields from the table: "roles"
  """
  roles_aggregate(
    """distinct select on columns"""
    distinct_on: [roles_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [roles_order_by!]

    """filter the rows returned"""
    where: roles_bool_exp
  ): roles_aggregate!

  """fetch data from the table: "roles" using primary key columns"""
  roles_by_pk(
    "\n    organizer_super_admin: Full Read & Write permissions on web2 and web3 components. Can assign roles and access system configurations.\n    organizer_admin: Full Read & Write permissions on web2 and web3 components.\n    organizer_operations_manager: Read & Write access to web2 components. Handles event setup, monitoring, analytics, etc.\n    organizer_finance_manager: Read & Write access to web3 components. Manages fund transfers, balance checks, and transaction approvals within limits.\n    organizer_content_manager: Read & Write access to web2 components. Manages content creation, editing, media uploads, and metadata modifications.\n    organizer_validator: Read & Write access on web2 and web3. Updates NFT traits and validates tickets and exclusive access during events.\n    organizer_auditor: Read-only access on web2 and web3. Conducts compliance checks and reviews transactions and operations.\n    organizer_guest: Limited access to web2. Can view public content without web3 permissions.\n    organizer_human_resources: Administrative permissions. Can invite new members for the organization and assign roles (except super admin and human resources).\n"
    value: String!
  ): roles

  """
  fetch data from the table in a streaming manner: "roles"
  """
  roles_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [roles_stream_cursor_input]!

    """filter the rows returned"""
    where: roles_bool_exp
  ): [roles!]!

  """
  fetch data from the table: "stripeCheckoutSession"
  """
  stripeCheckoutSession(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSession_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSession_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSession_bool_exp
  ): [stripeCheckoutSession!]!

  """
  fetch data from the table: "stripeCheckoutSessionType"
  """
  stripeCheckoutSessionType(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSessionType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSessionType_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSessionType_bool_exp
  ): [stripeCheckoutSessionType!]!

  """
  fetch aggregated fields from the table: "stripeCheckoutSessionType"
  """
  stripeCheckoutSessionType_aggregate(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSessionType_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSessionType_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSessionType_bool_exp
  ): stripeCheckoutSessionType_aggregate!

  """
  fetch data from the table: "stripeCheckoutSessionType" using primary key columns
  """
  stripeCheckoutSessionType_by_pk(
    """Type value."""
    value: String!
  ): stripeCheckoutSessionType

  """
  fetch data from the table in a streaming manner: "stripeCheckoutSessionType"
  """
  stripeCheckoutSessionType_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [stripeCheckoutSessionType_stream_cursor_input]!

    """filter the rows returned"""
    where: stripeCheckoutSessionType_bool_exp
  ): [stripeCheckoutSessionType!]!

  """
  fetch aggregated fields from the table: "stripeCheckoutSession"
  """
  stripeCheckoutSession_aggregate(
    """distinct select on columns"""
    distinct_on: [stripeCheckoutSession_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCheckoutSession_order_by!]

    """filter the rows returned"""
    where: stripeCheckoutSession_bool_exp
  ): stripeCheckoutSession_aggregate!

  """
  fetch data from the table: "stripeCheckoutSession" using primary key columns
  """
  stripeCheckoutSession_by_pk(
    """Unique identifier for the Stripe Checkout Session."""
    stripeSessionId: String!
  ): stripeCheckoutSession

  """
  fetch data from the table in a streaming manner: "stripeCheckoutSession"
  """
  stripeCheckoutSession_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [stripeCheckoutSession_stream_cursor_input]!

    """filter the rows returned"""
    where: stripeCheckoutSession_bool_exp
  ): [stripeCheckoutSession!]!

  """
  fetch data from the table: "stripeCustomer"
  """
  stripeCustomer(
    """distinct select on columns"""
    distinct_on: [stripeCustomer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCustomer_order_by!]

    """filter the rows returned"""
    where: stripeCustomer_bool_exp
  ): [stripeCustomer!]!

  """
  fetch aggregated fields from the table: "stripeCustomer"
  """
  stripeCustomer_aggregate(
    """distinct select on columns"""
    distinct_on: [stripeCustomer_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [stripeCustomer_order_by!]

    """filter the rows returned"""
    where: stripeCustomer_bool_exp
  ): stripeCustomer_aggregate!

  """fetch data from the table: "stripeCustomer" using primary key columns"""
  stripeCustomer_by_pk(
    """Unique identifier for the Stripe Customer."""
    stripeCustomerId: String!
  ): stripeCustomer

  """
  fetch data from the table in a streaming manner: "stripeCustomer"
  """
  stripeCustomer_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [stripeCustomer_stream_cursor_input]!

    """filter the rows returned"""
    where: stripeCustomer_bool_exp
  ): [stripeCustomer!]!

  """
  fetch data from the table: "timezone"
  """
  timezone(
    """distinct select on columns"""
    distinct_on: [timezone_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [timezone_order_by!]

    """filter the rows returned"""
    where: timezone_bool_exp
  ): [timezone!]!

  """
  fetch aggregated fields from the table: "timezone"
  """
  timezone_aggregate(
    """distinct select on columns"""
    distinct_on: [timezone_select_column!]

    """limit the number of rows returned"""
    limit: Int

    """skip the first n rows. Use only with order_by"""
    offset: Int

    """sort the rows by one or more columns"""
    order_by: [timezone_order_by!]

    """filter the rows returned"""
    where: timezone_bool_exp
  ): timezone_aggregate!

  """fetch data from the table: "timezone" using primary key columns"""
  timezone_by_pk(value: String!): timezone

  """
  fetch data from the table in a streaming manner: "timezone"
  """
  timezone_stream(
    """maximum number of rows returned in a single batch"""
    batch_size: Int!

    """cursor to stream the results returned by the query"""
    cursor: [timezone_stream_cursor_input]!

    """filter the rows returned"""
    where: timezone_bool_exp
  ): [timezone!]!
}

scalar timestamp

"""
Boolean expression to compare columns of type "timestamp". All fields are combined with logical 'AND'.
"""
input timestamp_comparison_exp {
  _eq: timestamp
  _gt: timestamp
  _gte: timestamp
  _in: [timestamp!]
  _is_null: Boolean
  _lt: timestamp
  _lte: timestamp
  _neq: timestamp
  _nin: [timestamp!]
}

scalar timestamptz

"""
Boolean expression to compare columns of type "timestamptz". All fields are combined with logical 'AND'.
"""
input timestamptz_comparison_exp {
  _eq: timestamptz
  _gt: timestamptz
  _gte: timestamptz
  _in: [timestamptz!]
  _is_null: Boolean
  _lt: timestamptz
  _lte: timestamptz
  _neq: timestamptz
  _nin: [timestamptz!]
}

"""IANA Time Zones fetched from pg_timezone_names in PostgreSQL"""
type timezone {
  value: String!
}

"""
aggregated selection of "timezone"
"""
type timezone_aggregate {
  aggregate: timezone_aggregate_fields
  nodes: [timezone!]!
}

"""
aggregate fields of "timezone"
"""
type timezone_aggregate_fields {
  count(columns: [timezone_select_column!], distinct: Boolean): Int!
  max: timezone_max_fields
  min: timezone_min_fields
}

"""
Boolean expression to filter rows from the table "timezone". All fields are combined with a logical 'AND'.
"""
input timezone_bool_exp {
  _and: [timezone_bool_exp!]
  _not: timezone_bool_exp
  _or: [timezone_bool_exp!]
  value: String_comparison_exp
}

"""
unique or primary key constraints on table "timezone"
"""
enum timezone_constraint {
  """
  unique or primary key constraint on columns "value"
  """
  timezone_pkey
}

"""
input type for inserting data into table "timezone"
"""
input timezone_insert_input {
  value: String
}

"""aggregate max on columns"""
type timezone_max_fields {
  value: String
}

"""aggregate min on columns"""
type timezone_min_fields {
  value: String
}

"""
response of any mutation on the table "timezone"
"""
type timezone_mutation_response {
  """number of rows affected by the mutation"""
  affected_rows: Int!

  """data from the rows affected by the mutation"""
  returning: [timezone!]!
}

"""
on_conflict condition type for table "timezone"
"""
input timezone_on_conflict {
  constraint: timezone_constraint!
  update_columns: [timezone_update_column!]! = []
  where: timezone_bool_exp
}

"""Ordering options when selecting data from "timezone"."""
input timezone_order_by {
  value: order_by
}

"""primary key columns input for table: timezone"""
input timezone_pk_columns_input {
  value: String!
}

"""
select columns of table "timezone"
"""
enum timezone_select_column {
  """column name"""
  value
}

"""
input type for updating data in table "timezone"
"""
input timezone_set_input {
  value: String
}

"""
Streaming cursor of the table "timezone"
"""
input timezone_stream_cursor_input {
  """Stream column input with initial value"""
  initial_value: timezone_stream_cursor_value_input!

  """cursor ordering"""
  ordering: cursor_ordering
}

"""Initial value of the column from where the streaming should start"""
input timezone_stream_cursor_value_input {
  value: String
}

"""
update columns of table "timezone"
"""
enum timezone_update_column {
  """column name"""
  value
}

input timezone_updates {
  """sets the columns of the filtered rows to the given values"""
  _set: timezone_set_input

  """filter the rows which have to be updated"""
  where: timezone_bool_exp!
}

scalar uuid

"""
Boolean expression to compare columns of type "uuid". All fields are combined with logical 'AND'.
"""
input uuid_comparison_exp {
  _eq: uuid
  _gt: uuid
  _gte: uuid
  _in: [uuid!]
  _is_null: Boolean
  _lt: uuid
  _lte: uuid
  _neq: uuid
  _nin: [uuid!]
}